{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from pytorch_pretrained_bert.tokenization import BertTokenizer\n",
    "from pytorch_pretrained_bert.modeling import BertForMaskedLM\n",
    "\n",
    "torch_device=torch.device('cuda')\n",
    "\n",
    "bert_model_mlm = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "bert_model_mlm.eval()\n",
    "bert_model_mlm.to(torch_device)\n",
    "\n",
    "for param in bert_model_mlm.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "bert_id2tok = dict()\n",
    "for tok, tok_id in bert_tokenizer.vocab.items():\n",
    "    bert_id2tok[tok_id] = tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters\n",
    "\n",
    "MAX_BERT_LEN=256\n",
    "MAX_COSINE_DIST=0.3\n",
    "BERT_VOCAB_QTY=30000\n",
    "\n",
    "num_threads=8\n",
    "K=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "ft_compiled_path = \"../data/jigsaw/ft_model_bert_basic_tok.npy\" # Embeddings generated from the vocabulary\n",
    "fasttext_embeds = np.load(ft_compiled_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.data.tokenizers.word_splitter import SpacyWordSplitter\n",
    "from pytorch_pretrained_bert.tokenization import BasicTokenizer\n",
    "from allennlp.data.token_indexers import WordpieceIndexer, SingleIdTokenIndexer\n",
    "import re\n",
    "\n",
    "#_spacy_tok = SpacyWordSplitter(language='en_core_web_sm', pos_tags=False).split_words\n",
    "_bert_tok = BasicTokenizer(do_lower_case=True)\n",
    "\n",
    "spacy_tokenizer = SpacyWordSplitter(language='en_core_web_sm', pos_tags=False)\n",
    "\n",
    "from allennlp.data.token_indexers import SingleIdTokenIndexer\n",
    "token_indexer = SingleIdTokenIndexer(\n",
    "    lowercase_tokens=True,\n",
    ")\n",
    "\n",
    "from itertools import groupby\n",
    "\n",
    "def remove_url(s):\n",
    "    return re.sub(r\"http\\S+\", \"\", s)\n",
    "\n",
    "def remove_extra_chars(s, max_qty=2):\n",
    "    res = [c * min(max_qty, len(list(group_iter))) for c, group_iter in groupby(s)] \n",
    "    return ''.join(res)\n",
    "\n",
    "def tokenizer(x: str):\n",
    "    return [remove_extra_chars(w) for w in _bert_tok.tokenize(remove_url(x))]\n",
    "    #return [w.text for w in _spacy_tok(x.lower())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"n't\" in bert_tokenizer.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30522"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bert_tokenizer.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[unused1]'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_id2tok[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14021"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_tokenizer.vocab['sh']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'det',\n",
       " '##ete',\n",
       " 'what',\n",
       " 'the',\n",
       " '[MASK]',\n",
       " 'are',\n",
       " 'you',\n",
       " 'doing',\n",
       " 'here',\n",
       " '?',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toks = bert_tokenizer.tokenize('[CLS] detete what the [MASK] are you doing here ? [SEP]')\n",
    "toks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[, CLS, ], what, the, [, MASK, ], are, you, do, n't, here, ?, [, SEP, ]]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toks = spacy_tokenizer.split_words(\"[CLS] what the [MASK] are you don't here ? [SEP]\")\n",
    "toks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['don',\n",
       " \"'\",\n",
       " 't',\n",
       " 'couldn',\n",
       " \"'\",\n",
       " 't',\n",
       " 'can',\n",
       " \"'\",\n",
       " 't',\n",
       " 'you',\n",
       " \"'\",\n",
       " 're',\n",
       " 'i',\n",
       " \"'\",\n",
       " 'm',\n",
       " 'sheet']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"don't  couldn't can't you're I'm sheeeet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 103, 102]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_tokenizer.convert_tokens_to_ids(bert_tokenizer.tokenize(\"[CLS] [MASK] [SEP]\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2158, 2102]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_tokenizer.convert_tokens_to_ids(bert_tokenizer.tokenize(\"mant\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "# pos_oov is OOV index with respect to the original (not BERT) tokenizer!!!\n",
    "UtterData = namedtuple('SentData', ['batch_sent_id', 'pos_oov', 'tok_ids', 'oov_token'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence list contains an arrays of arrays (without [CLS] and [SEP] tokens)\n",
    "def create_batch(bert_tokenizer, sent_list): \n",
    "    \n",
    "    batch_data = []\n",
    "    \n",
    "    batch_max_seq_qty = 0\n",
    "    \n",
    "    for one_sent_tok_arr in sent_list:\n",
    "    \n",
    "        bert_toks = [\"[CLS]\"]\n",
    "        \n",
    "        for tok in one_sent_tok_arr:\n",
    "            bert_toks.extend(bert_tokenizer.tokenize(tok))\n",
    "            \n",
    "        bert_toks.append(\"[SEP]\")\n",
    "        \n",
    "        one_sent_ids = bert_tokenizer.convert_tokens_to_ids(bert_toks)\n",
    "        batch_data.append(one_sent_ids)\n",
    "    \n",
    "        batch_max_seq_qty = max(batch_max_seq_qty, len(one_sent_ids))\n",
    "        \n",
    "    batch_qty = len(batch_data)\n",
    "    \n",
    "    tok_ids_batch = np.zeros( (batch_qty, batch_max_seq_qty), dtype=np.int64) # zero is a padding symbol\n",
    "    tok_mask_batch = np.zeros( (batch_qty, batch_max_seq_qty), dtype=np.int64)\n",
    "    for k in range(batch_qty):\n",
    "        tok_ids = batch_data[k]\n",
    "        tok_qty = len(tok_ids)\n",
    "        tok_ids_batch[k, 0:tok_qty] = tok_ids\n",
    "        tok_mask_batch[k, 0:tok_qty] = np.ones(tok_qty)\n",
    "        \n",
    "    return tok_ids_batch, tok_mask_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[101, 2023, 2003, 1037, 3231, 102], [101, 2023, 2003, 1037, 27838, 3367, 102]]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from collections import namedtuple\n",
    "import sys\n",
    "\n",
    "\n",
    "def get_bert_logits_for_words_batch(torch_device, bert_model_mlm, \n",
    "                                    tok_ids_batch, tok_mask_batch):\n",
    "    \n",
    "    print(tok_ids_batch)\n",
    "\n",
    "    batch_qty, batch_seq_len = tok_ids_batch.shape\n",
    "    \n",
    "    word_ids = list(set([e for r in tok_ids_batch for e in r]))\n",
    "    \n",
    "    word_ids_logit_id = { word_ids[i] : i for i in range(len(word_ids)) }\n",
    "    \n",
    "    tok_ids_batch = torch.from_numpy(tok_ids_batch).to(device=torch_device) \n",
    "    tok_mask_batch = torch.from_numpy(tok_mask_batch).to(device=torch_device) \n",
    "\n",
    "    seg_ids = torch.zeros_like(tok_ids_batch, device=torch_device)\n",
    "\n",
    "    # Main BERT model see modeling.py in https://github.com/huggingface/pytorch-pretrained-BERT\n",
    "    bert = bert_model_mlm.bert \n",
    "    # cls is an instance of BertOnlyMLMHead (see https://github.com/huggingface/pytorch-pretrained-BERT)\n",
    "    cls = bert_model_mlm.cls\n",
    "    # predictions are of the type BertLMPredictionHead (see https://github.com/huggingface/pytorch-pretrained-BERT)\n",
    "    predictions = cls.predictions\n",
    "    transform = predictions.transform\n",
    "   \n",
    "    # We don't use the complete decoding matrix, but only selected rows\n",
    "    word_ids = torch.from_numpy(np.array(word_ids, dtype=np.int64)).to(device=torch_device)\n",
    "                                \n",
    "    weight = predictions.decoder.weight[word_ids,:]\n",
    "    bias = predictions.bias[word_ids]\n",
    "    \n",
    "    #print(bias[:10])\n",
    "\n",
    "\n",
    "    # Transformations from the main BERT model\n",
    "    sequence_output, _= bert(tok_ids_batch, \n",
    "                             seg_ids, \n",
    "                             attention_mask=tok_mask_batch, \n",
    "                             output_all_encoded_layers=False)\n",
    "    # Transformations from the BertLMPredictionHead model with the restricted last layer\n",
    "    hidden_states = transform(sequence_output)    \n",
    "    logits_all = torch.nn.functional.linear(hidden_states, weight) + bias                            \n",
    "                                        \n",
    "    logits_all=logits_all.detach().cpu().numpy()\n",
    "    tok_ids_batch = tok_ids_batch.cpu().numpy()\n",
    "    \n",
    "    avg_logits = np.zeros(batch_qty)\n",
    "    logits_res = np.zeros( (batch_qty, batch_seq_len) )\n",
    "    \n",
    "    for r in range(batch_qty):\n",
    "\n",
    "        qty = 0.0\n",
    "        avg_logit = 0.0\n",
    "    \n",
    "        for c in range(batch_seq_len):\n",
    "            word_id = tok_ids_batch[r, c]\n",
    "            if word_id > 1:\n",
    "                logits_res[r, c] = logits_all[r, c, word_ids_logit_id[word_id]]\n",
    "                avg_logit += logits_res[r, c]\n",
    "                qty += 1\n",
    "                \n",
    "        avg_logits[r] = avg_logit / qty\n",
    "\n",
    "        \n",
    "    return avg_logits, logits_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 12)\n",
      "[[ 101 2054 1996 4429 6968 2024 2017  103 2182 1029  102    0]\n",
      " [ 101 2054 1996 4429 6968 2024 2017  103  103 2182 1029  102]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([9.0451423 , 8.01771679]),\n",
       " array([[-5.64911366, 24.19732666, 14.6283226 , -1.78911757, 11.90443134,\n",
       "         12.60415459, 16.94570732, -2.52056861, 12.2120018 , 23.39019966,\n",
       "         -6.42677879,  0.        ],\n",
       "        [-5.69555759, 23.93379021, 14.67827797, -1.88091636, 12.56620026,\n",
       "         11.75910091, 17.70313454, -3.14499998, -3.27325583, 13.65651035,\n",
       "         22.20828056, -6.29796362]]))"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_ids, batch_mask=create_batch(bert_tokenizer, [ 'What the fcuk are you [MASK] here?'.split(), 'What the fcuk are you [MASK] [MASK] here?'.split() ])\n",
    "print(batch_ids.shape)\n",
    "get_bert_logits_for_words_batch('cuda', bert_model_mlm, batch_ids, batch_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 101 2023 2003 1996 2190  102]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([8.25289265]),\n",
       " array([[-6.02657032, 18.24155807, 19.6713829 , 18.3153038 ,  5.81563187,\n",
       "         -6.49995041]]))"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_ids, batch_mask=create_batch(bert_tokenizer, [ 'this is the best'.split()])\n",
    "get_bert_logits_for_words_batch('cuda', bert_model_mlm, batch_ids, batch_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 101 2023 2003 1037 3231  102    0    0]\n",
      " [ 101 2023 2003 1037 3231 3231 3231  102]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([7.16823435, 9.61663324]),\n",
       " array([[-6.08818054,  9.49454212, 20.47395706, 17.58529854,  7.42068577,\n",
       "         -5.87689686,  0.        ,  0.        ],\n",
       "        [-6.08461523,  5.79411411, 18.26132011, 18.86880875, 18.63547707,\n",
       "         17.54462242,  9.32696629, -5.41362762]]))"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_ids, batch_mask=create_batch(bert_tokenizer, [ 'this is a test'.split(), 'this is a test test test'.split() ])\n",
    "get_bert_logits_for_words_batch('cuda', bert_model_mlm, batch_ids, batch_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{10: 0, 135: 1, -1: 2, 3: 3}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_ids = [ 10, 135, -1, 3]\n",
    "\n",
    "word_ids_logit_id = { word_ids[i] : i for i in range(len(word_ids)) }\n",
    "\n",
    "word_ids_logit_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3109, 6616, 17752, 6548, 4485, 2725, 24341, 4147, 2437, 3241, 2035, 3231]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_tokenizer.convert_tokens_to_ids(['hell', 'fuck', 'heck', 'devil', 'shit', 'doing', \n",
    "                                      'doin', 'wearing', 'making', 'thinking', 'all', 'test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sent_list = ['What the fcuk are you doingg here?',\n",
    "             'This is a *strangge* sentence']\n",
    "\n",
    "batch_data_raw, tok_ids_batch = get_batch_data(torch_device, \n",
    "                                                tokenizer, \n",
    "                                                bert_tokenizer, \n",
    "                                                sent_list,\n",
    "                                                MAX_BERT_LEN)\n",
    "\n",
    "get_bert_preds_for_words_batch(torch_device,\n",
    "                               bert_model_mlm, \n",
    "                               batch_data_raw, tok_ids_batch,\n",
    "                               bert_tokenizer.convert_tokens_to_ids(['hell', 'fuck', 'heck', 'devil', 'shit', 'doing', \n",
    "                                                                  'doin', 'wearing', 'making', 'thinking']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
