{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "dependencies"
    ]
   },
   "outputs": [],
   "source": [
    "depends_on = [\n",
    "    \"preproc_jigsaw\",\n",
    "    \"jigsaw_create_augmented_data\",\n",
    "    \"create_fasttext_matrix\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings for seamlessly running on colab\n",
    "import os\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive')\n",
    "    os.environ[\"IS_COLAB\"] = \"True\"\n",
    "except ImportError:\n",
    "    os.environ[\"IS_COLAB\"] = \"False\"    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"SLACK_TOKEN\" not in os.environ:\n",
    "    os.environ[\"SLACK_TOKEN\"] = \"\" # TODO: insert here for slack notifications\n",
    "if \"SLACK_ID\" not in os.environ:\n",
    "    os.environ[\"SLACK_ID\"] = \"\" # TODO: insert here for slack notifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "if [ \"$IS_COLAB\" = \"True\" ]; then\n",
    "    pip install git+https://github.com/facebookresearch/fastText.git\n",
    "    pip install torch\n",
    "    pip install torchvision\n",
    "    pip install --upgrade git+https://github.com/keitakurita/allennlp.git@develop\n",
    "    pip install dnspython\n",
    "    pip install jupyter_slack\n",
    "    pip install git+https://github.com/keitakurita/Better_LSTM_PyTorch.git\n",
    "    pip install nmslib\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from typing import *\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from functools import partial\n",
    "from overrides import overrides\n",
    "import warnings\n",
    "\n",
    "from allennlp.data import Instance\n",
    "from allennlp.data.token_indexers import TokenIndexer, SingleIdTokenIndexer\n",
    "from allennlp.data.tokenizers import Token\n",
    "from allennlp.nn import util as nn_util\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger(__name__)  # pylint: disable=invalid-name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from contextlib import contextmanager\n",
    "\n",
    "class Config(dict):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        for k, v in kwargs.items():\n",
    "            setattr(self, k, v)\n",
    "    \n",
    "    def set(self, key, val):\n",
    "        self[key] = val\n",
    "        setattr(self, key, val)\n",
    "\n",
    "@contextmanager\n",
    "def timer(name):\n",
    "    t0 = time.time()\n",
    "    yield\n",
    "    print(f'[{name}] done in {time.time() - t0:.0f} s')\n",
    "    \n",
    "import functools\n",
    "import traceback\n",
    "import sys\n",
    "\n",
    "def get_ref_free_exc_info():\n",
    "    \"Free traceback from references to locals/globals to avoid circular reference leading to gc.collect() unable to reclaim memory\"\n",
    "    type, val, tb = sys.exc_info()\n",
    "    traceback.clear_frames(tb)\n",
    "    return (type, val, tb)\n",
    "\n",
    "def gpu_mem_restore(func):\n",
    "    \"Reclaim GPU RAM if CUDA out of memory happened, or execution was interrupted\"\n",
    "    @functools.wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        try:\n",
    "            return func(*args, **kwargs)\n",
    "        except:\n",
    "            type, val, tb = get_ref_free_exc_info() # must!\n",
    "            raise type(val).with_traceback(tb) from None\n",
    "    return wrapper\n",
    "\n",
    "def ifnone(a: Any, alt: Any): return alt if a is None else a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = TypeVar(\"T\")\n",
    "TensorDict = Dict[str, Union[torch.Tensor, Dict[str, torch.Tensor]]]  # pylint: disable=invalid-name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# for papermill\n",
    "testing = False # set to False when running experiments\n",
    "debugging = False\n",
    "seed = 3\n",
    "use_bt = False\n",
    "computational_batch_size = 128\n",
    "batch_size = 128\n",
    "lr = 4e-3\n",
    "lr_schedule = \"slanted_triangular\"\n",
    "epochs = 4 if not testing else 1\n",
    "hidden_sz = 128\n",
    "dataset = \"jigsaw\"\n",
    "n_classes = 6\n",
    "max_seq_len = 512\n",
    "download_data = False\n",
    "ft_model_path = \"../data/jigsaw/ft_basic_toks.txt\"\n",
    "max_vocab_size = 400000\n",
    "dropouti = 0.2\n",
    "dropoutw = 0.0\n",
    "dropoute = 0.1\n",
    "dropoute_max = None\n",
    "dropoutr = 0.3 # TODO: Implement\n",
    "val_ratio = 0.0\n",
    "use_mbern_loss = False\n",
    "use_focal_loss = True\n",
    "label_smoothing_eps = 1.\n",
    "focal_loss_alpha = 1.\n",
    "focal_loss_gamma = 2.\n",
    "use_augmented = False\n",
    "freeze_embeddings = True\n",
    "mixup_ratio = 0.0\n",
    "discrete_mixup_ratio = 0.0\n",
    "attention_bias = True\n",
    "use_attention_aux = False\n",
    "weight_decay = 0.\n",
    "bias_init = True\n",
    "neg_splits = 1\n",
    "num_layers = 2\n",
    "rnn_type = \"lstm\"\n",
    "rnn_residual = False\n",
    "pooling_type = \"augmented_multipool\" # attention or multipool or augmented_multipool\n",
    "model_type = \"standard\"\n",
    "cache_elmo_embeddings = True\n",
    "use_word_level_features = False\n",
    "use_sentence_level_features = False\n",
    "bucket = True\n",
    "compute_thres_on_test = False\n",
    "find_lr = False\n",
    "permute_sentences = False\n",
    "run_id = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Can we make this play better with papermill?\n",
    "config = Config(\n",
    "    testing=testing,\n",
    "    debugging=debugging,\n",
    "    seed=seed,\n",
    "    use_bt=use_bt,\n",
    "    computational_batch_size=computational_batch_size,\n",
    "    batch_size=batch_size,\n",
    "    lr=lr,\n",
    "    lr_schedule=lr_schedule,\n",
    "    epochs=epochs,\n",
    "    hidden_sz=hidden_sz,\n",
    "    dataset=dataset,\n",
    "    n_classes=n_classes,\n",
    "    max_seq_len=max_seq_len, # necessary to limit memory usage\n",
    "    ft_model_path=ft_model_path,\n",
    "    max_vocab_size=max_vocab_size,\n",
    "    dropouti=dropouti,\n",
    "    dropoutw=dropoutw,\n",
    "    dropoute=dropoute,\n",
    "    dropoute_max=dropoute_max,\n",
    "    dropoutr=dropoutr,\n",
    "    val_ratio=val_ratio,\n",
    "    use_mbern_loss=use_mbern_loss,\n",
    "    use_focal_loss=use_focal_loss,\n",
    "    label_smoothing_eps=label_smoothing_eps,\n",
    "    focal_loss_alpha=focal_loss_alpha,\n",
    "    focal_loss_gamma=focal_loss_gamma,\n",
    "    use_augmented=use_augmented,\n",
    "    freeze_embeddings=freeze_embeddings,\n",
    "    attention_bias=attention_bias,\n",
    "    use_attention_aux=use_attention_aux,\n",
    "    weight_decay=weight_decay,\n",
    "    bias_init=bias_init,\n",
    "    neg_splits=neg_splits,\n",
    "    num_layers=num_layers,\n",
    "    rnn_type=rnn_type,\n",
    "    rnn_residual=rnn_residual,\n",
    "    pooling_type=pooling_type,\n",
    "    model_type=model_type,\n",
    "    cache_elmo_embeddings=cache_elmo_embeddings,\n",
    "    use_word_level_features=use_word_level_features,\n",
    "    use_sentence_level_features=use_sentence_level_features,\n",
    "    mixup_ratio=mixup_ratio,\n",
    "    discrete_mixup_ratio=discrete_mixup_ratio,\n",
    "    bucket=bucket,\n",
    "    compute_thres_on_test=compute_thres_on_test,\n",
    "    permute_sentences=permute_sentences,\n",
    "    find_lr=find_lr,\n",
    "    run_id=run_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.common.checks import ConfigurationError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.model_type != \"standard\" and \"bert\" not in config.model_type and \"elmo\" not in config.model_type:\n",
    "    raise ConfigurationError(f\"Invalid model type {config.model_type} given\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.mixup_ratio > 0. and config.bucket:\n",
    "    raise ConfigurationError(f\"Mixup should be combined with complete random shuffling of the input\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"bert\" in config.model_type and config.computational_batch_size > 16:\n",
    "    raise ConfigurationError(\"Batch size too large for BERT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.permute_sentences and (config.use_word_level_features or config.use_sentence_level_features):\n",
    "    raise ConfigurationError(\"Token shuffling does not yet transfer to wlf and slf.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "now = datetime.datetime.now()\n",
    "RUN_ID = config.run_id if config.run_id is not None else now.strftime(\"%m_%d_%H:%M:%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_GPU = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.environ[\"IS_COLAB\"] != \"True\":\n",
    "    DATA_ROOT = Path(\"../data\") / config.dataset\n",
    "else:\n",
    "    DATA_ROOT = Path(\"./gdrive/My Drive/Colab_Workspace/Colab Notebooks/data\") / config.dataset\n",
    "    config.ft_model_path = str(DATA_ROOT / \"ft_basic_toks.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p {DATA_ROOT}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "if download_data:\n",
    "    \n",
    "    if config.val_ratio > 0.0:\n",
    "        #fnames = [\"train_wo_val.csv\", \"test_proced.csv\", \"val.csv\", \"ft_model.txt\"]\n",
    "        raise ConfigurationError(f\"Validation dataset not processed.\")\n",
    "    \n",
    "    else:\n",
    "        fnames = [\"train_basic.jsonl\", \"test_basic.jsonl\", \"ft_basic_toks.txt\"]\n",
    "    \n",
    "    if config.use_augmented or config.discrete_mixup_ratio > 0.0: \n",
    "        #fnames.append(\"train_extra.csv\")\n",
    "        raise ConfigurationError(f\"Augmented datasets not processed.\")\n",
    "    \n",
    "    for fname in fnames:\n",
    "        if not (DATA_ROOT / fname).exists():\n",
    "            print(subprocess.Popen([f\"aws s3 cp s3://nnfornlp/raw_data/jigsaw/{fname} {str(DATA_ROOT)}\"],\n",
    "                                   shell=True, stdout=subprocess.PIPE).stdout.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert_oov_map.bin\r\n",
      "ckpts\r\n",
      "data_ft_vocab\r\n",
      "data_vocab.bin\r\n",
      "debug.csv\r\n",
      "distractor_targets.txt\r\n",
      "example.csv\r\n",
      "ft_basic_toks.npy\r\n",
      "ft_basic_toks.txt\r\n",
      "ft_basic_toks.txt.md\r\n",
      "ft_basic_toks.txt.sm\r\n",
      "ft_compiled.npy\r\n",
      "ft_model_bert_basic_tok.npy\r\n",
      "ft_model_bert_basic_tok.txt\r\n",
      "ft_model_no_bt.txt\r\n",
      "ft_model.txt\r\n",
      "sample_pred_bert_oov.csv\r\n",
      "sample_pred_bert_oov_fasttext_knn.csv\r\n",
      "sample_pred.csv\r\n",
      "sample_submission.csv\r\n",
      "test_augmented.jsonl\r\n",
      "test_basic.jsonl\r\n",
      "test_basic_preds_5cbdf665145ce50cdcb3443d.npy\r\n",
      "test_basic_preds_5cbdffb3145ce5102902573c.npy\r\n",
      "test_basic_preds_5cbe0b0c145ce513a4e33ed4.npy\r\n",
      "test_basic_preds_5cbe1ef3145ce5191af228f6.npy\r\n",
      "test_basic_preds_5cbe29f7145ce51b590c4682.npy\r\n",
      "test_basic_preds_5cbe3713145ce51e29b0eb03.npy\r\n",
      "test_basic_preds_5cbe547f145ce524b08751f3.npy\r\n",
      "test_basic_preds_5cbe5ef2145ce5293a0453b3.npy\r\n",
      "test_basic_preds_5cbe6846145ce52c349ca45b.npy\r\n",
      "test_basic_preds_5cbe709b145ce52d8f1e42a0.npy\r\n",
      "test_basic_preds.npy\r\n",
      "test.csv\r\n",
      "test_ds.bin\r\n",
      "test_labels.csv\r\n",
      "test_proced.csv\r\n",
      "test_proced_no_oov1.csv\r\n",
      "thres_basic_preds_5cbdf665145ce50cdcb3443d.npy\r\n",
      "thres_basic_preds_5cbdffb3145ce5102902573c.npy\r\n",
      "thres_basic_preds_5cbe0b0c145ce513a4e33ed4.npy\r\n",
      "thres_basic_preds_5cbe1ef3145ce5191af228f6.npy\r\n",
      "thres_basic_preds_5cbe29f7145ce51b590c4682.npy\r\n",
      "thres_basic_preds_5cbe3713145ce51e29b0eb03.npy\r\n",
      "thres_basic_preds_5cbe547f145ce524b08751f3.npy\r\n",
      "thres_basic_preds_5cbe5ef2145ce5293a0453b3.npy\r\n",
      "thres_basic_preds_5cbe6846145ce52c349ca45b.npy\r\n",
      "thres_basic_preds_5cbe709b145ce52d8f1e42a0.npy\r\n",
      "thres_basic_preds.npy\r\n",
      "tmp_model.pth\r\n",
      "toxic_targets.txt\r\n",
      "train_augmented.jsonl\r\n",
      "train_basic.jsonl\r\n",
      "train.csv\r\n",
      "train_ds.bin\r\n",
      "train_extra.csv\r\n",
      "train_extra_interpolated.csv\r\n",
      "train_new_jigsaw.csv\r\n",
      "train_new_jigsaw_raw.csv\r\n",
      "train_no_oov1.csv\r\n",
      "train_with_bt.csv\r\n",
      "train_wo_val.csv\r\n",
      "val.csv\r\n",
      "val_ds.bin\r\n",
      "val_no_oov1.csv\r\n",
      "voc_basic_toks\r\n",
      "wiki.en.bin\r\n"
     ]
    }
   ],
   "source": [
    "!ls {DATA_ROOT}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set random seed manually to replicate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7ff744e73bf0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(config.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.data.dataset_readers import DatasetReader, StanfordSentimentTreeBankDatasetReader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.environ[\"IS_COLAB\"] != \"True\":\n",
    "    import sys\n",
    "    sys.path.append(\"../preprocessing\")\n",
    "    from data_loader import *\n",
    "    from basic_processing import word_level_features, sentence_level_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_cols = JIGSAW_LABEL_NAMES\n",
    "\n",
    "from enum import IntEnum\n",
    "ColIdx = IntEnum('ColIdx', [(x.upper(), i) for i, x in enumerate(label_cols)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare token handlers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from functools import wraps\n",
    "\n",
    "def maybeshuffle(_tokenize):\n",
    "    def func(*args, **kwargs):\n",
    "        arr = _tokenize(*args, **kwargs)\n",
    "        if config.permute_sentences:\n",
    "            random.shuffle(arr)\n",
    "        return arr\n",
    "    return func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.data.tokenizers.word_splitter import SpacyWordSplitter\n",
    "from allennlp.data.token_indexers import WordpieceIndexer, SingleIdTokenIndexer\n",
    "\n",
    "\n",
    "if config.model_type == \"standard\" or (\"elmo\" in config.model_type and config.cache_elmo_embeddings):\n",
    "    \n",
    "    from allennlp.data.token_indexers import SingleIdTokenIndexer\n",
    "    token_indexer = SingleIdTokenIndexer(\n",
    "        lowercase_tokens=True,\n",
    "    )\n",
    "    \n",
    "    @maybeshuffle\n",
    "    def token_extender(x: List[str]):\n",
    "        return x\n",
    "    \n",
    "elif \"elmo\" in config.model_type:\n",
    "    \n",
    "    from allennlp.data.token_indexers.elmo_indexer import ELMoTokenCharactersIndexer\n",
    "    token_indexer = ELMoTokenCharactersIndexer()\n",
    "    \n",
    "    @maybeshuffle\n",
    "    def token_extender(x: List[str]):\n",
    "        # add start and end of sentence tokens\n",
    "        return [\"<S>\"] + x + [\"</S>\"]\n",
    "    \n",
    "    if config.use_word_level_features or config.use_sentence_level_features:\n",
    "        raise ConfigurationError(\"Elmo tokenization does not yet transfer to wlf and slf.\")\n",
    "    \n",
    "elif \"bert\" in config.model_type:\n",
    "    \n",
    "    #def flatten(x: List[List[T]]) -> List[T]:\n",
    "    #    return [item for sublist in x for item in sublist]\n",
    "\n",
    "    from allennlp.data.token_indexers import PretrainedBertIndexer\n",
    "    token_indexer = PretrainedBertIndexer(\n",
    "        pretrained_model=config.model_type,\n",
    "        max_pieces=config.max_seq_len,\n",
    "        do_lowercase=True,\n",
    "     )\n",
    "    \n",
    "    @maybeshuffle\n",
    "    def token_extender(x: List[str]):\n",
    "        return x\n",
    "\n",
    "    if config.use_word_level_features or config.use_sentence_level_features:\n",
    "        raise ConfigurationError(\"BERT tokenization does not yet transfer to wlf and slf.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<allennlp.data.token_indexers.single_id_token_indexer.SingleIdTokenIndexer at 0x7ff6e40b3ac8>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_indexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = JigsawDatasetJSONLReader(\n",
    "    token_extender = token_extender,\n",
    "    oov_token_swapper = OOVTokenSwapper(),\n",
    "    token_indexers={\"tokens\": token_indexer},\n",
    "    testing = config.testing\n",
    "   )"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "159571it [00:24, 6633.87it/s]\n",
      "63978it [00:10, 6268.28it/s]\n"
     ]
    }
   ],
   "source": [
    "if config.val_ratio > 0.0:\n",
    "    \n",
    "    raise ConfigurationError(\"Validation dataset not processed.\")\n",
    "    \n",
    "    #train_ds, val_ds, test_ds = (reader.read(DATA_ROOT / fname) for fname in [\"train_wo_val.csv\",\n",
    "    #                                                                          \"val.csv\",\n",
    "    #                                                                          \"test_proced.csv\"])\n",
    "else:\n",
    "    \n",
    "    if config.use_bt:\n",
    "        raise ConfigurationError(\"BT dataset not processed.\")\n",
    "    \n",
    "    else:\n",
    "        train_ds, test_ds = (reader.read(DATA_ROOT / fname) for fname in [\"train_basic.jsonl\",\"test_basic.jsonl\"])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.use_augmented or config.discrete_mixup_ratio > 0.0:\n",
    "    raise ConfigurationError(\"Augmented dataset not processed.\")\n",
    "    # TODO: Handle data leak for validation!\n",
    "    #train_aug_ds = reader.read(DATA_ROOT / \"train_extra.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "159571"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': ['Explanation',\n",
       "  'Why',\n",
       "  'the',\n",
       "  'edits',\n",
       "  'made',\n",
       "  'under',\n",
       "  'my',\n",
       "  'username',\n",
       "  'Hardcore',\n",
       "  'Metallica',\n",
       "  'Fan',\n",
       "  'were',\n",
       "  'reverted',\n",
       "  '?',\n",
       "  'They',\n",
       "  'weren',\n",
       "  \"'\",\n",
       "  't',\n",
       "  'vandalisms',\n",
       "  ',',\n",
       "  'just',\n",
       "  'closure',\n",
       "  'on',\n",
       "  'some',\n",
       "  'Gas',\n",
       "  'after',\n",
       "  'I',\n",
       "  'voted',\n",
       "  'at',\n",
       "  'New',\n",
       "  'York',\n",
       "  'Dolls',\n",
       "  'FAC',\n",
       "  '.',\n",
       "  'And',\n",
       "  'please',\n",
       "  'don',\n",
       "  \"'\",\n",
       "  't',\n",
       "  'remove',\n",
       "  'the',\n",
       "  'template',\n",
       "  'from',\n",
       "  'the',\n",
       "  'talk',\n",
       "  'page',\n",
       "  'since',\n",
       "  'I',\n",
       "  \"'\",\n",
       "  'm',\n",
       "  'retired',\n",
       "  'now.89.205.38.27'],\n",
       " '_token_indexers': {'tokens': <allennlp.data.token_indexers.single_id_token_indexer.SingleIdTokenIndexer at 0x7ff6e40b3ac8>},\n",
       " '_indexed_tokens': None,\n",
       " '_indexer_name_to_indexed_token': None}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(train_ds[0].fields[\"tokens\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.val_ratio > 0.0:\n",
    "    \n",
    "    train_labels = pd.read_csv(DATA_ROOT / \"train_wo_val.csv\")[label_cols].values\n",
    "    \n",
    "else:\n",
    "    train_labels = pd.read_csv(DATA_ROOT / \"train.csv\")[label_cols].values\n",
    "    \n",
    "if config.testing:\n",
    "    \n",
    "    train_labels = train_labels[:len(train_ds), :]\n",
    "    \n",
    "if config.use_augmented:\n",
    "    \n",
    "    train_aux_labels = pd.read_csv(DATA_ROOT / \"train_extra.csv\")[label_cols].values\n",
    "    \n",
    "    if config.testing: train_aux_labels = train_aux_labels[:len(train_ds), :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 223549/223549 [00:18<00:00, 12310.30it/s]\n"
     ]
    }
   ],
   "source": [
    "from allennlp.data.vocabulary import Vocabulary\n",
    "\n",
    "if \"bert\" in config.model_type:\n",
    "    vocab = Vocabulary()\n",
    "    \n",
    "elif config.model_type == \"standard\" or config.cache_elmo_embeddings:\n",
    "    \n",
    "    full_ds = train_ds + test_ds\n",
    "    \n",
    "    if config.val_ratio > 0.0: \n",
    "        full_ds = full_ds + val_ds\n",
    "        \n",
    "    vocab = Vocabulary.from_instances(full_ds, max_vocab_size=config.max_vocab_size)\n",
    "    \n",
    "else:\n",
    "    vocab = Vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.data.iterators import BucketIterator, DataIterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "class Sampler:\n",
    "    def sample(self, ds: List[Instance]) -> List[Instance]:\n",
    "        return ds\n",
    "    def sample_size(self, ds: List[Instance]) -> int:\n",
    "        return len(ds)\n",
    "\n",
    "class BiasedSampler(Sampler):\n",
    "    def __init__(self, mask: np.ndarray, n_splits: int):\n",
    "        self.mask = mask\n",
    "        self.n_splits = n_splits\n",
    "        self.pos = np.where(self.mask)[0]\n",
    "        self.neg = np.where(~self.mask)[0]\n",
    "        self._n_splits_iterated = 0\n",
    "        \n",
    "    def sample(self, ds: List[Instance]) -> List[Instance]:\n",
    "        if self._n_splits_iterated % self.n_splits == 0:\n",
    "            self.folds = KFold(n_splits=self.n_splits).split(self.neg)\n",
    "        _, neg_idxs = next(self.folds)\n",
    "        \n",
    "        p = np.random.permutation(len(self.pos) + len(neg_idxs))\n",
    "        smpl = np.r_[self.pos, self.neg[neg_idxs]][p]\n",
    "        \n",
    "        self._n_splits_iterated += 1\n",
    "        return [ds[i] for i in smpl]\n",
    "    \n",
    "    def sample_size(self, ds: List[Instance]) -> int:\n",
    "        \"\"\"Returns number of samples that would be returned upon a call to sample\"\"\"\n",
    "        # there might be a slight difference depending on the epoch, but it's okay\n",
    "        return len(self.pos) + len(self.neg) // self.n_splits "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScoredSampler:\n",
    "    def __init__(self, mask: np.ndarray, ratio: float):\n",
    "        self.mask = mask\n",
    "        self.ratio = ratio\n",
    "        self.n_samples = int(len(self.tgt) * self.ratio)\n",
    "        self.score = mask.astype(\"int\")\n",
    "    \n",
    "    def set_score(self, score: np.ndarray):\n",
    "        assert len(score) == len(self.tgt)\n",
    "        self.score = score\n",
    "    \n",
    "    def sample(self, ds: List[Instance]):\n",
    "        \"\"\"Sample top n targets sorted by score descending\"\"\"\n",
    "        smpl = np.arange(len(self.mask))[np.argsort(-self.score)][:self.n_samples]\n",
    "        return [ds[i] for i in smpl]\n",
    "    \n",
    "    def sample_size(self, ds: List[Instance]) -> int:\n",
    "        \"\"\"Returns number of samples that would be returned upon a call to sample\"\"\"\n",
    "        return self.n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import deque\n",
    "from overrides import overrides\n",
    "\n",
    "from allennlp.common.checks import ConfigurationError\n",
    "from allennlp.common.util import lazy_groups_of, add_noise_to_dict_values\n",
    "from allennlp.data.dataset import Batch\n",
    "from allennlp.data.instance import Instance\n",
    "from allennlp.data.iterators import DataIterator, BucketIterator, BasicIterator\n",
    "from allennlp.data.vocabulary import Vocabulary\n",
    "\n",
    "class SamplingIteratorMixin:\n",
    "    \"\"\"Uses Python's MRO to add sampling.\n",
    "    DANGER: This is pushing the limits of OOP and might lead to bugs\n",
    "    \"\"\"\n",
    "    def __init__(self, *args, sampler: Sampler=None, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.sampler = ifnone(sampler, Sampler())\n",
    "        \n",
    "    def get_num_batches(self, instances: List[Instance]):\n",
    "        return math.ceil(self.sampler.sample_size(instances) / self._batch_size)\n",
    "\n",
    "    def _create_batches(self, instances: Iterable[Instance], shuffle: bool) -> Iterable[Batch]:\n",
    "        yield from super()._create_batches(self.sampler.sample(instances), shuffle)\n",
    "\n",
    "# Caution: Inheritance must be in order: SamplingIteratorMixin, BucketIterator\n",
    "class CustomBucketIterator(SamplingIteratorMixin, BucketIterator): pass\n",
    "class CustomBasicIterator(SamplingIteratorMixin, BasicIterator): pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Allow for customization\n",
    "if config.neg_splits > 1:\n",
    "    if config.use_augmented:\n",
    "        full_trn_labels = np.concatenate([train_labels, train_aux_labels], axis=0)\n",
    "    else:\n",
    "        full_trn_labels = train_labels\n",
    "    sampler = BiasedSampler(full_trn_labels.sum(1) >= 1,\n",
    "                            config.neg_splits)\n",
    "else:\n",
    "    sampler = Sampler()\n",
    "if config.bucket:\n",
    "    iterator = CustomBucketIterator(\n",
    "        batch_size=config.batch_size, \n",
    "        biggest_batch_first=config.testing,\n",
    "        sorting_keys=[(\"tokens\", \"num_tokens\")],\n",
    "        max_instances_in_memory=config.batch_size * 2,\n",
    "        sampler=sampler,\n",
    "    )\n",
    "else:\n",
    "    # CAUTION: BasicIterator shuffles the dataset internally\n",
    "    # TODO: Either fix this bug or ensure evalutation can handle shuffle\n",
    "    # in the dataset order\n",
    "    iterator = CustomBasicIterator(\n",
    "        batch_size=config.batch_size, \n",
    "        max_instances_in_memory=config.batch_size * 2,\n",
    "        sampler=sampler,\n",
    "    )\n",
    "iterator.index_with(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(iterator(train_ds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': {'tokens': tensor([[    5,  1391,   490,  ...,     0,     0,     0],\n",
       "          [    5,   225,     3,  ...,     0,     0,     0],\n",
       "          [   52,   263,    25,  ...,     0,     0,     0],\n",
       "          ...,\n",
       "          [    5,    31,    31,  ...,     0,     0,     0],\n",
       "          [    5, 39905,  2774,  ...,     0,     0,     0],\n",
       "          [ 6453,     4,   338,  ...,    73,  6262,     2]])},\n",
       " 'word_level_features': tensor([[[0.0000, 1.0000, 1.0000],\n",
       "          [0.1667, 0.0000, 1.0000],\n",
       "          [0.0000, 1.0000, 1.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000]],\n",
       " \n",
       "         [[0.0000, 1.0000, 1.0000],\n",
       "          [0.2000, 0.0000, 1.0000],\n",
       "          [0.0000, 0.0000, 1.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000]],\n",
       " \n",
       "         [[0.2000, 0.0000, 1.0000],\n",
       "          [0.0000, 0.0000, 1.0000],\n",
       "          [0.0000, 0.0000, 1.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[0.0000, 1.0000, 1.0000],\n",
       "          [0.0000, 1.0000, 1.0000],\n",
       "          [0.0000, 1.0000, 1.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000]],\n",
       " \n",
       "         [[0.0000, 1.0000, 1.0000],\n",
       "          [0.1111, 0.0000, 1.0000],\n",
       "          [0.1429, 0.0000, 1.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000]],\n",
       " \n",
       "         [[0.3333, 0.0000, 1.0000],\n",
       "          [0.0000, 1.0000, 1.0000],\n",
       "          [0.2000, 0.0000, 1.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 1.0000],\n",
       "          [0.0000, 0.0000, 1.0000],\n",
       "          [0.0000, 1.0000, 1.0000]]]),\n",
       " 'sentence_level_features': tensor([], size=(128, 0)),\n",
       " 'label': tensor([[0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [1., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [1., 0., 1., 0., 1., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [1., 0., 1., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [1., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [1., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [1., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [1., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [1., 0., 1., 0., 1., 1.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 1., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [1., 0., 1., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.]]),\n",
       " 'id': ['002a13f2896596fa',\n",
       "  '002d6c9d9f85e81f',\n",
       "  '0029541a38c523a0',\n",
       "  '004f981460421bdf',\n",
       "  '00070ef96486d6f9',\n",
       "  '00733f0a4a58cf42',\n",
       "  '00537730daf8c5f1',\n",
       "  '0060c5c9030b2d14',\n",
       "  '0061b075244dd234',\n",
       "  '007bbfa4da2bc32d',\n",
       "  '0029b87aa9c7dc4a',\n",
       "  '00173958f46763a2',\n",
       "  '000113f07ec002fd',\n",
       "  '004f6dbe69f3545d',\n",
       "  '00349c6325526c11',\n",
       "  '004f5608984d99f1',\n",
       "  '007bc29766a43e3c',\n",
       "  '0000997932d777bf',\n",
       "  '002f0e29c60807b1',\n",
       "  '0057b7710cb5ebb2',\n",
       "  '008f22e7b58e559b',\n",
       "  '00744c2f77391702',\n",
       "  '007571394afafcb5',\n",
       "  '0005c987bdfc9d4b',\n",
       "  '0030614cfd96d9d1',\n",
       "  '000f35deef84dc4a',\n",
       "  '009b3b15f1ada72f',\n",
       "  '00a20f187531df59',\n",
       "  '0052a7e684beeb1a',\n",
       "  '006120d209a4a46c',\n",
       "  '0082b4b42b3f07a1',\n",
       "  '006f2c1459f3b6b1',\n",
       "  '003dbd1b9b354c1f',\n",
       "  '000ffab30195c5e1',\n",
       "  '0053bab79133c0fc',\n",
       "  '0063dd8f202a698a',\n",
       "  '002a6beca33307b3',\n",
       "  '004de318396bbf8b',\n",
       "  '0022cf8467ebc9fd',\n",
       "  '0015f4aa35ebe9b5',\n",
       "  '0028d62e8a5629aa',\n",
       "  '008198c5a9d85a8e',\n",
       "  '005b214511a69b4b',\n",
       "  '007f1839ada915e6',\n",
       "  '0013a8b1a5f26bcb',\n",
       "  '001956c382006abd',\n",
       "  '0087c131ccffe160',\n",
       "  '0037e59caead9dab',\n",
       "  '008f93320e3661b8',\n",
       "  '00961bcaadd6a278',\n",
       "  '008344a80c43b8c9',\n",
       "  '00585c1da10b448b',\n",
       "  '008f1b06428778fe',\n",
       "  '0074b307c2d9a100',\n",
       "  '0069e6d57a3beb51',\n",
       "  '008e2acf5bcf4be4',\n",
       "  '0069bf2d6881ad29',\n",
       "  '0038f191ffc93d75',\n",
       "  '005306a4c109dab3',\n",
       "  '002264ea4d5f2887',\n",
       "  '003a19c04c079bf7',\n",
       "  '004176f28a17bf45',\n",
       "  '005eb6d31a8d821e',\n",
       "  '0009eaea3325de8c',\n",
       "  '00037261f536c51d',\n",
       "  '005e6b1369cbe377',\n",
       "  '00905910dcbcc8aa',\n",
       "  '006a4cdda4960588',\n",
       "  '0060062dd4db5195',\n",
       "  '000c0dfd995809fa',\n",
       "  '004b103182fb1eab',\n",
       "  '009565ee1bc64e68',\n",
       "  '0034065c7b12a7a2',\n",
       "  '007e1e47cd0e2fec',\n",
       "  '009820fe28cd24dc',\n",
       "  '003b9f448ee4a29d',\n",
       "  '00a317acddff8a62',\n",
       "  '0082b5a7b4a67da2',\n",
       "  '004d07d94cb92e35',\n",
       "  '005f59485fcddeb0',\n",
       "  '001ee16c46a99262',\n",
       "  '0048de0c9422f64f',\n",
       "  '00822d0d01752c7e',\n",
       "  '004b073d5b456b15',\n",
       "  '0001b41b1c6bb37e',\n",
       "  '001363e1dbe91225',\n",
       "  '00548f16a392d8ed',\n",
       "  '005ed4dfecd86188',\n",
       "  '001810bf8c45bf5f',\n",
       "  '005cec874506e9d9',\n",
       "  '006774d59329b7bd',\n",
       "  '007f127033d66db5',\n",
       "  '007db1f1477ea977',\n",
       "  '00078f8ce7eb276d',\n",
       "  '00148d055a169b93',\n",
       "  '001cadfd324f8087',\n",
       "  '001b2dd65d9d925c',\n",
       "  '00218d74784ce50b',\n",
       "  '006854d70298693e',\n",
       "  '006d11791d76b9f3',\n",
       "  '00480b6e1f19601b',\n",
       "  '0038d1dc2ad29469',\n",
       "  '009e3f1c0c757e43',\n",
       "  '0006f16e4e9f292e',\n",
       "  '00584d887401f47b',\n",
       "  '0091aec11b57d12e',\n",
       "  '00957fadc476d7d9',\n",
       "  '00328eadb85b3010',\n",
       "  '002746baedcdff10',\n",
       "  '00882ab8cfa42274',\n",
       "  '0092f0871cc66dbc',\n",
       "  '002c9cccf2f1d05b',\n",
       "  '006de7a80921e04b',\n",
       "  '0016e01b742b8da3',\n",
       "  '000cfee90f50d471',\n",
       "  '004b975fabbbffa9',\n",
       "  '001d874a4d3e8813',\n",
       "  '00151a9f93c6b059',\n",
       "  '0098257c6952c9e3',\n",
       "  '00587c559177dcf2',\n",
       "  '0021fe88bc4da3e6',\n",
       "  '0063f66706c20dfa',\n",
       "  '0033b9d5ccd499fb',\n",
       "  '001d8e7be417776a',\n",
       "  '0005300084f90edc',\n",
       "  '0063a8786a7034fc',\n",
       "  '008a1e9c45de8138',\n",
       "  '00510c3d06745849']}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    5,  1391,   490,  ...,     0,     0,     0],\n",
       "        [    5,   225,     3,  ...,     0,     0,     0],\n",
       "        [   52,   263,    25,  ...,     0,     0,     0],\n",
       "        ...,\n",
       "        [    5,    31,    31,  ...,     0,     0,     0],\n",
       "        [    5, 39905,  2774,  ...,     0,     0,     0],\n",
       "        [ 6453,     4,   338,  ...,    73,  6262,     2]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"tokens\"][\"tokens\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 1130])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"tokens\"][\"tokens\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.modules.token_embedders import Embedding\n",
    "from allennlp.modules.token_embedders.bert_token_embedder import BertEmbedder, PretrainedBertEmbedder\n",
    "from allennlp.modules.seq2vec_encoders import Seq2VecEncoder, PytorchSeq2VecWrapper\n",
    "from allennlp.modules.stacked_bidirectional_lstm import StackedBidirectionalLstm\n",
    "from allennlp.nn.util import get_text_field_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(Seq2VecEncoder):\n",
    "    def __init__(self, inp_sz, aug_sz=None,\n",
    "                 hidden_sz=None, out_sz=None, dim=1, eps=1e-9,\n",
    "                 return_attention=False, use_bias=True):\n",
    "        super().__init__()\n",
    "        self.inp_sz, self.dim, self.eps = inp_sz, dim, eps\n",
    "        self.out_sz = ifnone(out_sz, self.inp_sz)\n",
    "        self.return_attention = return_attention\n",
    "        self.l1 = nn.Linear(inp_sz, ifnone(inp_sz * 2, hidden_sz))\n",
    "        nn.init.xavier_uniform_(self.l1.weight.data)\n",
    "        nn.init.zeros_(self.l1.bias.data)\n",
    "        \n",
    "        vw = torch.zeros(ifnone(inp_sz * 2, hidden_sz), 1)\n",
    "        nn.init.xavier_uniform_(vw)        \n",
    "        self.vw = nn.Parameter(vw)\n",
    "        self.use_bias = use_bias\n",
    "        if self.use_bias: self.b = nn.Parameter(torch.zeros(1))\n",
    "    \n",
    "    @overrides\n",
    "    def get_input_dim(self) -> int:\n",
    "        return self.inp_sz\n",
    "    \n",
    "    @overrides\n",
    "    def get_output_dim(self) -> int:\n",
    "        return self.out_sz\n",
    "        \n",
    "    def forward(self, x, aug=None, mask=None):\n",
    "        e = torch.tanh(self.l1(x))\n",
    "        e = torch.einsum(\"bij,jk->bi\", [e, self.vw]) \n",
    "        if self.use_bias: e = e + self.b\n",
    "        a = torch.exp(e)\n",
    "        \n",
    "        if mask is not None: a = a.masked_fill(mask == 0, 0)\n",
    "\n",
    "        a = a / (torch.sum(a, dim=self.dim, keepdim=True) + self.eps)\n",
    "\n",
    "        weighted_input = x * a.unsqueeze(-1)\n",
    "        if self.return_attention:\n",
    "            return torch.sum(weighted_input, dim=1), a\n",
    "        else:\n",
    "            return torch.sum(weighted_input, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.modules.seq2seq_encoders import PytorchSeq2SeqWrapper, Seq2SeqEncoder\n",
    "from better_lstm import LSTM, VariationalDropout\n",
    "\n",
    "class MultiPooling(Seq2VecEncoder):\n",
    "    \"\"\"Does max and mean pooling over the temporal dimension\"\"\"\n",
    "    def __init__(self, input_sz: int):\n",
    "        super().__init__()\n",
    "        self.input_sz = input_sz\n",
    "        \n",
    "    @overrides\n",
    "    def get_input_dim(self) -> int:\n",
    "        return self.input_sz\n",
    "    \n",
    "    @overrides\n",
    "    def get_output_dim(self) -> int:\n",
    "        return self.input_sz * 2\n",
    "        \n",
    "    def forward(self, x, mask=None, aug=None):\n",
    "        max_, _ = torch.max(x, dim=1)\n",
    "        mean_ = torch.mean(x, dim=1)\n",
    "        return torch.cat([max_, mean_], dim=-1)\n",
    "\n",
    "class AugmentedMultiPool(MultiPooling):\n",
    "    def __init__(self, input_sz, aug_sz):\n",
    "        super().__init__(input_sz)\n",
    "        self.attn = Attention(input_sz, hidden_sz=input_sz, \n",
    "                              out_sz=input_sz)\n",
    "    @overrides\n",
    "    def get_output_dim(self) -> int:\n",
    "        return self.input_sz * 3\n",
    "    \n",
    "    def forward(self, x, mask=None, aug=None):\n",
    "        pooled = super().forward(x, mask=mask, aug=aug)\n",
    "        attn = self.attn(x, mask=mask, aug=None)\n",
    "        return torch.cat([pooled, attn], dim=-1)\n",
    "    \n",
    "class BiRNN(Seq2SeqEncoder):\n",
    "    def __init__(self, rnn_type, n_layers, embed_sz, hidden_sz, dropoutw=0.):\n",
    "        super().__init__()\n",
    "        self.rnn_type = rnn_type\n",
    "        self.n_layers = n_layers\n",
    "        self.embed_sz = embed_sz\n",
    "        self.hidden_sz = hidden_sz\n",
    "        in_szs = [embed_sz] + [hidden_sz * 2] * (n_layers - 1)\n",
    "        if rnn_type == \"lstm\":\n",
    "            rnns = [LSTM(embed_sz, hidden_sz, batch_first=True, num_layers=n_layers,\n",
    "                         bidirectional=True, dropoutw=dropoutw)]\n",
    "        else:\n",
    "            if dropoutw > 0.0:\n",
    "                warnings.warn(\"Weight dropout not currently supported with GRUs\")\n",
    "            rnns = [nn.GRU(embed_sz, hidden_sz, batch_first=True, num_layers=n_layers, \n",
    "                           bidirectional=True)]\n",
    "            for gru in rnns:\n",
    "                for name, param in gru.named_parameters():\n",
    "                    if \"weight_hh\" in name:\n",
    "                        nn.init.orthogonal_(param.data)\n",
    "                    elif \"weight_ih\" in name:\n",
    "                        nn.init.xavier_uniform_(param.data)\n",
    "                    elif \"bias\" in name:\n",
    "                        nn.init.zeros_(param.data)\n",
    "        self.rnns = nn.ModuleList([PytorchSeq2SeqWrapper(rnn) for rnn in rnns])\n",
    "        if config.use_attention_aux:\n",
    "            self.ln = nn.Linear(embed_sz, 64) # handle attention auxillary input here\n",
    "\n",
    "    @overrides\n",
    "    def get_input_dim(self) -> int:\n",
    "        return self.embed_sz\n",
    "    \n",
    "    @overrides\n",
    "    def get_output_dim(self) -> int:\n",
    "        if config.rnn_residual:\n",
    "            out_sz = self.hidden_sz * 2 * self.n_layers\n",
    "        else:\n",
    "            out_sz = self.hidden_sz * 2\n",
    "        if config.use_attention_aux: out_sz += 64\n",
    "        return out_sz\n",
    "    \n",
    "    def forward(self, embeds, mask=None):\n",
    "        x = embeds\n",
    "        outputs = []\n",
    "        for rnn in self.rnns:\n",
    "            x = rnn(x, mask=mask)\n",
    "            if config.rnn_residual:\n",
    "                outputs.append(x)\n",
    "        if config.rnn_residual:\n",
    "            x = torch.cat(outputs, dim=-1)\n",
    "\n",
    "        if config.use_attention_aux:\n",
    "            x = torch.cat([torch.tanh(self.ln(embeds)), x], dim=-1)\n",
    "        return x\n",
    "    \n",
    "class BiRNNEncoder(Seq2VecEncoder):\n",
    "    def __init__(self, rnn: Seq2SeqEncoder,\n",
    "                 pooler: Seq2VecEncoder,\n",
    "                 dropouti=0.0, dropoutr=0.0):\n",
    "        super().__init__()\n",
    "        self.dropouti = VariationalDropout(dropouti, batch_first=True)\n",
    "        self.rnn = rnn\n",
    "        self.dropouto = VariationalDropout(dropoutr, batch_first=True)\n",
    "        self.pool = pooler\n",
    "        \n",
    "    @overrides\n",
    "    def get_input_dim(self) -> int:\n",
    "        return self.rnn.get_input_dim()\n",
    "    \n",
    "    @overrides\n",
    "    def get_output_dim(self) -> int:\n",
    "        out_dim = self.pool.get_output_dim()\n",
    "        if config.use_sentence_level_features:\n",
    "            out_dim += len(sentence_level_features)\n",
    "        return out_dim\n",
    "    \n",
    "    def _init_hidden_state(self, bs:int):\n",
    "        if self.rnn.rnn_type == \"lstm\":\n",
    "            return torch.zeros(bs, self.hidden_sz), torch.zeros(bs, self.hidden_sz)\n",
    "        else:\n",
    "            return torch.zeros(bs, self.hidden_sz)\n",
    "    \n",
    "    @overrides\n",
    "    def forward(self, x: torch.Tensor, sentence_feats: torch.Tensor,\n",
    "                mask: Optional[torch.Tensor]=None) -> torch.Tensor:\n",
    "        seq = self.rnn(x, mask)\n",
    "        seq = self.dropouto(seq)\n",
    "        vec = self.pool(seq, aug=x, mask=mask)\n",
    "        if config.use_sentence_level_features:\n",
    "            return torch.cat([sentence_feats, vec], dim=-1)\n",
    "        else:\n",
    "            return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.training.metrics import CategoricalAccuracy, BooleanAccuracy, Metric\n",
    "\n",
    "def prod(x: Iterable):\n",
    "    acc = 1\n",
    "    for v in x: acc *= v\n",
    "    return acc\n",
    "\n",
    "class MultilabelAccuracy(Metric):\n",
    "    def __init__(self, thres=0.5):\n",
    "        \n",
    "        self.thres = 0.5\n",
    "        self.correct_count = 0\n",
    "        self.total_count = 0\n",
    "    \n",
    "    def __call__(self, logits: torch.FloatTensor, \n",
    "                 t: torch.LongTensor) -> float:\n",
    "        \n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        t = t.detach().cpu().numpy() >= config.label_smoothing_eps\n",
    "        cc = ((logits >= self.thres) == t).sum()\n",
    "        tc = prod(logits.shape)\n",
    "        self.correct_count += cc\n",
    "        self.total_count += tc\n",
    "        return cc / tc\n",
    "    \n",
    "    def get_metric(self, reset: bool=False):\n",
    "        acc = self.correct_count / self.total_count\n",
    "        if reset:\n",
    "            self.reset()\n",
    "        return acc\n",
    "    \n",
    "    @overrides\n",
    "    def reset(self):\n",
    "        self.correct_count = 0\n",
    "        self.total_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.nn.util import move_to_device, has_tensor\n",
    "\n",
    "def permute(obj, p: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Given a structure (possibly) containing Tensors on the CPU,\n",
    "    permute all the Tensors\n",
    "    \"\"\"\n",
    "    if not has_tensor(obj):\n",
    "        return obj\n",
    "    elif isinstance(obj, torch.Tensor):\n",
    "        return obj[p]\n",
    "    elif isinstance(obj, dict):\n",
    "        return {key: permute(value, p) for key, value in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [permute(item, p) for item in obj]\n",
    "    elif isinstance(obj, tuple):\n",
    "        return tuple([permute(item, p) for item in obj])\n",
    "    else:\n",
    "        return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.functional as F\n",
    "\n",
    "class FocalLossWithLogits(nn.Module):\n",
    "    \"\"\"Borrowed from https://www.kaggle.com/c/tgs-salt-identification-challenge/discussion/65938\"\"\"\n",
    "    def __init__(self, alpha=1., gamma=2.):\n",
    "        super().__init__()\n",
    "        self.alpha,self.gamma = alpha,gamma\n",
    "        self._loss = nn.BCEWithLogitsLoss(reduction=\"none\")\n",
    "        \n",
    "    def forward(self, y, t):\n",
    "        bce_loss = self._loss(y, t)\n",
    "        pt = torch.exp(-bce_loss)\n",
    "        return (self.alpha * (1-pt) ** self.gamma * bce_loss).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "#Reference https://stackoverflow.com/a/5898031\n",
    "from itertools import chain, combinations\n",
    "def all_subsets(ss):\n",
    "    return chain(*map(lambda x: combinations(ss, x), range(0, len(ss)+1)))\n",
    "\n",
    "\n",
    "class MBernLossWithLogits(nn.Module):\n",
    "    \"\"\" Referece https://arxiv.org/pdf/1206.1874.pdf\n",
    "        We perform a simple trick of assigning each label\n",
    "        value combination to a separate category\n",
    "        Total number of categories is 2**config.n_classes\n",
    "    \"\"\"\n",
    "    def __init__(self, n_classes = config.n_classes):\n",
    "        super().__init__()\n",
    "        self.n_classes = n_classes\n",
    "        self.loss = nn.CrossEntropyLoss(reduction=\"mean\")\n",
    "        self._create_dicts()\n",
    "        self.n_cat = len(self.dict_cat)\n",
    "        \n",
    "    def _create_dicts(self):\n",
    "        pos = list(range(self.n_classes))\n",
    "        self.dict_cat = dict()\n",
    "        self.dict_cat_inv = dict()\n",
    "        for i, subset in enumerate(all_subsets(stuff)):\n",
    "            self.dict_cat[i] = subset\n",
    "            self.dict_cat_inv[subset] = i\n",
    "        \n",
    "    def _map_target(self,t_old):\n",
    "        \n",
    "        t_new = torch.zeros(t_old.size()[0],device=t_old.get_device(),dtype=torch.long)\n",
    "        \n",
    "        for i in range(t_old.size(0)):\n",
    "            p = (t_old[i,:]==1).nonzero().reshape(-1,).tolist()\n",
    "            t_new[i] = self.dict_cat_inv[tuple(p)]\n",
    "        \n",
    "        return t_new\n",
    "        \n",
    "    def get_marginal_probs(self,y):\n",
    "\n",
    "        probs = F.softmax(y,dim=1)\n",
    "        marginal_probs = torch.zeros(probs.size()[0],self.n_classes,device=probs.get_device())\n",
    "        \n",
    "        for i in range(probs.size(0)):\n",
    "            for j in range(probs.size(1)):\n",
    "                for p in self.dict_cat[j]:\n",
    "                    marginal_probs[i,p] += probs[i,j]\n",
    "                \n",
    "        return marginal_probs\n",
    "            \n",
    "    def forward(self, y, t):\n",
    "        t_new = self._map_target(t)\n",
    "        loss = self.loss(y, t_new)\n",
    "        return loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.models import Model\n",
    "from allennlp.modules.text_field_embedders import TextFieldEmbedder, BasicTextFieldEmbedder\n",
    "from torch.distributions.beta import Beta\n",
    "\n",
    "class BaselineModel(Model):\n",
    "    def __init__(self, word_embeddings: TextFieldEmbedder,\n",
    "                 encoder: Seq2VecEncoder,\n",
    "                 loss: nn.Module,\n",
    "                 out_sz: int=config.n_classes,\n",
    "                 multilabel: bool=True, \n",
    "                 dropouto=0.1,\n",
    "                 mixup_alpha: int=0.2):\n",
    "        super().__init__(vocab)\n",
    "        \n",
    "        self.word_embeddings = word_embeddings\n",
    "        self.encoder = encoder\n",
    "        feature_sz = self.encoder.get_output_dim()\n",
    "        \n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(feature_sz, 50),\n",
    "            nn.ELU(),\n",
    "            nn.Dropout(dropouto),\n",
    "            nn.Linear(50, out_sz),\n",
    "        )        \n",
    "        \n",
    "        self.loss = loss\n",
    "\n",
    "        self.multilabel = multilabel\n",
    "        self.lambda_sampler = Beta(torch.tensor([mixup_alpha]), torch.tensor([mixup_alpha]))\n",
    "        \n",
    "        if self.multilabel:\n",
    "            self.accuracy = MultilabelAccuracy()\n",
    "            self.per_label_acc = {c: MultilabelAccuracy() for c in label_cols}\n",
    "        \n",
    "        elif self.loss._get_name()==\"MBernLossWithLogits\":\n",
    "            self.accuracy = CategoricalAccuracy(thres=0.6224593312)\n",
    "        \n",
    "        else:\n",
    "            self.accuracy = CategoricalAccuracy()\n",
    "        \n",
    "        self.is_test_mode = False\n",
    "            \n",
    "    def test_mode(self, val=True):\n",
    "        self.is_test_mode = val\n",
    "        \n",
    "    def get_embeddings(self, toks: Dict[str, torch.Tensor],\n",
    "                       word_feats: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Encapsulates addition of word level features\"\"\"\n",
    "        embeddings = self.word_embeddings(toks)\n",
    "        if config.use_word_level_features:\n",
    "            embeddings = torch.cat([word_feats, embeddings], dim=-1)\n",
    "        return embeddings\n",
    "\n",
    "    def forward(self, tokens: Dict[str, torch.Tensor],\n",
    "                label: torch.Tensor,\n",
    "                word_level_features: torch.Tensor,\n",
    "                sentence_level_features: torch.Tensor,\n",
    "                **meta) -> torch.Tensor:\n",
    "        \n",
    "        if self.is_test_mode: tokens[\"tokens\"] *= 0\n",
    "        \n",
    "        mask = get_text_field_mask(tokens)\n",
    "        embeddings = self.get_embeddings(tokens, word_level_features)\n",
    "        \n",
    "        state = self.encoder(embeddings, \n",
    "                             sentence_feats=sentence_level_features, \n",
    "                             mask=mask)\n",
    "        \n",
    "        class_logits = self.projection(state)\n",
    "        \n",
    "        output = {\"class_logits\": class_logits}\n",
    "\n",
    "        if self.loss._get_name()==\"MBernLossWithLogits\":\n",
    "            output[\"marginal_probs\"] = self.loss.get_marginal_probs(class_logits)\n",
    "\n",
    "        if self.loss._get_name()==\"MBernLossWithLogits\":\n",
    "            output[\"accuracy\"] = self.accuracy(self.loss.get_marginal_probs(class_logits), label)\n",
    "        else:\n",
    "            output[\"accuracy\"] = self.accuracy(class_logits, label)\n",
    "        \n",
    "        output[\"loss\"] = self.loss(class_logits, label)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def mixup(self, tokens: Dict[str, torch.Tensor],\n",
    "              label: torch.Tensor,\n",
    "              word_level_features: torch.Tensor,\n",
    "              sentence_level_features: torch.Tensor,\n",
    "              **meta) -> TensorDict:\n",
    "        # generate new tokens and labels\n",
    "        bs = label.size(0)\n",
    "        shuf = torch.randperm(bs).to(label.device)\n",
    "        tokens2 = permute(tokens, shuf)\n",
    "        labels1, labels2 = label, permute(label, shuf)\n",
    "        # TODO: Think of how to handle this masking intelligently\n",
    "        mask1, mask2 = (get_text_field_mask(t) for t in (tokens, tokens2))\n",
    "        embs1, embs2 = (self.get_embeddings(t, word_level_features) for t in (tokens, tokens2))\n",
    "        # interpolate\n",
    "        ratios = self.lambda_sampler.sample((bs, 1)).to(label.device)\n",
    "        embs = ratios * embs1 + (1-ratios) * embs2\n",
    "        label = ratios.squeeze(2) * labels1 + (1-ratios.squeeze(2)) * labels2\n",
    "        \n",
    "        # remaining process is the same\n",
    "        # TODO: Handle stat feats\n",
    "        state = self.encoder(embs, sentence_level_features, mask1 * mask2) # TODO: Handle masking\n",
    "        class_logits = self.projection(state)\n",
    "        \n",
    "        output = {\"loss\": self.loss(class_logits, label)}\n",
    "        return output\n",
    "    \n",
    "    def get_metrics(self, reset: bool = False) -> Dict[str, float]:\n",
    "        return {\"accuracy\": self.accuracy.get_metric(reset)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.set(\"vocab_size\", min(vocab.get_vocab_size(), config.max_vocab_size))\n",
    "if config.model_type == \"standard\":\n",
    "    config.set(\"embedding_dim\", 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import warnings\n",
    "\n",
    "def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "def get_fasttext_embeddings(model_path: str, vocab: Vocabulary):\n",
    "    prog_bar = tqdm(open(model_path, encoding=\"utf8\", errors='ignore'))\n",
    "    prog_bar.set_description(\"Loading embeddings\")\n",
    "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in prog_bar\n",
    "                             if len(o)>100)\n",
    "    all_embs = np.stack(embeddings_index.values())\n",
    "\n",
    "    embeddings = np.zeros((config.vocab_size + 5, 300))\n",
    "    n_missing_tokens = 0\n",
    "    prog_bar = tqdm(vocab.get_index_to_token_vocabulary().items())\n",
    "    prog_bar.set_description(\"Creating matrix\")\n",
    "    for idx, token in prog_bar:\n",
    "        if idx == 0: continue # keep padding as all zeros\n",
    "        if idx == 1: continue # Treat unknown words as dropped words\n",
    "        if token == \"[MASK]\":\n",
    "            embeddings[idx, :] = np.random.randn(300) * 0.5\n",
    "        if token not in embeddings_index:\n",
    "            n_missing_tokens += 1\n",
    "            if n_missing_tokens < 10:\n",
    "                warnings.warn(f\"Token {token} not in embeddings: did you change preprocessing?\")\n",
    "            if n_missing_tokens == 10:\n",
    "                warnings.warn(f\"More than {n_missing_tokens} missing, supressing warnings\")\n",
    "        else:\n",
    "            embeddings[idx, :] = embeddings_index[token]\n",
    "    \n",
    "    print(\"MISSING tokens: \", n_missing_tokens)\n",
    "    if n_missing_tokens > 0:\n",
    "        warnings.warn(f\"{n_missing_tokens} in total are missing from embedding text file\")\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading embeddings: : 320356it [00:30, 10672.38it/s]\n",
      "/home/anna/neuralnlp/lib/python3.7/site-packages/ipykernel_launcher.py:11: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "Creating matrix: 100%|██████████| 306326/306326 [00:01<00:00, 240566.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MISSING tokens:  0\n",
      "[Loading embeddings] done in 32 s\n"
     ]
    }
   ],
   "source": [
    "with timer(\"Loading embeddings\"):\n",
    "    if config.model_type == \"standard\":\n",
    "        embedding_weights = get_fasttext_embeddings(config.ft_model_path, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomEmbedding(Embedding):\n",
    "    # TODO: Fix (make this decently efficient: currently allocating two embeddings)\n",
    "    def __init__(self, num_embeddings, embedding_dim,\n",
    "                 padding_index=None, max_norm=None, trainable=True,\n",
    "                 weight=None, dropout=0., scale=None):\n",
    "        super().__init__(num_embeddings, embedding_dim, weight=weight,\n",
    "                         padding_index=padding_index, max_norm=max_norm,\n",
    "                         trainable=trainable)\n",
    "        self.dropout = dropout\n",
    "        self.scale = scale\n",
    "        self.padding_idx = padding_index\n",
    "\n",
    "    def forward(self, words):\n",
    "        weight = self.weight\n",
    "        if self.dropout > 0.0 and self.training:\n",
    "            mask = weight.data.new().resize_((weight.size(0), 1)).bernoulli_(1 - self.dropout).expand_as(weight) / (1 - self.dropout)\n",
    "            masked_embed_weight = mask * weight\n",
    "        else:\n",
    "            masked_embed_weight = weight\n",
    "        if self.scale:\n",
    "            masked_embed_weight = scale.expand_as(masked_embed_weight) * masked_embed_weight\n",
    "\n",
    "        padding_idx = self.padding_idx\n",
    "        if padding_idx is None:\n",
    "            padding_idx = -1\n",
    "\n",
    "        X = torch.nn.functional.embedding(words, masked_embed_weight,\n",
    "            padding_idx, self.max_norm, self.norm_type,\n",
    "            self.scale_grad_by_freq, self.sparse\n",
    "          )\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.modules.text_field_embedders.text_field_embedder import TextFieldEmbedder\n",
    "from allennlp.modules.time_distributed import TimeDistributed\n",
    "\n",
    "# TODO: Implement\n",
    "class ElmoTextFieldEmbedder(TextFieldEmbedder):\n",
    "    # AllenNLP support for caching sucks by default\n",
    "    # so we have to write our own embedder to bypass this problem\n",
    "    def __init__(self,\n",
    "                 token_embedders: Dict[str, Any],\n",
    "                 embedder_to_indexer_map: Dict[str, List[str]] = None,\n",
    "                 allow_unmatched_keys: bool = False) -> None:\n",
    "        super().__init__()\n",
    "        self._token_embedders = token_embedders\n",
    "        self._embedder_to_indexer_map = embedder_to_indexer_map\n",
    "        for key, embedder in token_embedders.items():\n",
    "            name = 'token_embedder_%s' % key\n",
    "            self.add_module(name, embedder)\n",
    "        self._allow_unmatched_keys = allow_unmatched_keys\n",
    "    \n",
    "    @overrides\n",
    "    def get_output_dim(self) -> int:\n",
    "        output_dim = 0\n",
    "        for embedder in self._token_embedders.values():\n",
    "            output_dim += embedder.get_output_dim()\n",
    "        return output_dim\n",
    "    \n",
    "    def forward(self, text_field_input: Dict[str, torch.Tensor],\n",
    "                num_wrapping_dims: int = 0) -> torch.Tensor:\n",
    "        if self._token_embedders.keys() != text_field_input.keys():\n",
    "            if not self._allow_unmatched_keys:\n",
    "                message = \"Mismatched token keys: %s and %s\" % (str(self._token_embedders.keys()),\n",
    "                                                                str(text_field_input.keys()))\n",
    "                raise ConfigurationError(message)\n",
    "        embedded_representations = []\n",
    "        keys = sorted(self._token_embedders.keys())\n",
    "        for key in keys:\n",
    "            # If we pre-specified a mapping explictly, use that.\n",
    "            if self._embedder_to_indexer_map is not None:\n",
    "                tensors = [text_field_input[indexer_key] for\n",
    "                           indexer_key in self._embedder_to_indexer_map[key]]\n",
    "            else:\n",
    "                # otherwise, we assume the mapping between indexers and embedders\n",
    "                # is bijective and just use the key directly.\n",
    "                tensors = [text_field_input[key]]\n",
    "            # Note: need to use getattr here so that the pytorch voodoo\n",
    "            # with submodules works with multiple GPUs.\n",
    "            embedder = getattr(self, 'token_embedder_{}'.format(key))\n",
    "            for _ in range(num_wrapping_dims):\n",
    "                embedder = TimeDistributed(embedder)\n",
    "            # Force embedder to use word inputs\n",
    "            if key == \"tokens\":\n",
    "                token_vectors = embedder(tensors[0], \n",
    "                                         word_inputs=tensors[0] if config.cache_elmo_embeddings else None)\n",
    "            else:\n",
    "                token_vectors = embedder(*tensors)\n",
    "            embedded_representations.append(token_vectors)\n",
    "        return torch.cat(embedded_representations, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_pretrained_bert.modeling import BertModel\n",
    "from allennlp.modules.token_embedders.token_embedder import TokenEmbedder\n",
    "\n",
    "class CustomBertEmbedder(TokenEmbedder):\n",
    "    \"\"\"\n",
    "    A ``TokenEmbedder`` that produces BERT embeddings for your tokens.\n",
    "    Should be paired with a ``BertIndexer``, which produces wordpiece ids.\n",
    "    Sums last 4 hidden layers for now (might use scalar mix in the future)\n",
    "    \"\"\"\n",
    "    def __init__(self, pretrained_model: str,\n",
    "                 use_scalar_mix: bool = False,\n",
    "                 fine_tune: bool = False,\n",
    "                 n_hidden_layers: int = 4) -> None:\n",
    "        super().__init__()\n",
    "        if use_scalar_mix and fine_tune:\n",
    "            raise ConfigurationError(\"Choose mix or fine tuning\")\n",
    "        \n",
    "        self.bert_model = BertModel.from_pretrained(pretrained_model)\n",
    "        for param in self.bert_model.parameters():\n",
    "            param.requires_grad = fine_tune\n",
    "        self.output_dim = self.bert_model.config.hidden_size\n",
    "        self.n_hidden_layers = n_hidden_layers\n",
    "        if use_scalar_mix:\n",
    "            self._scalar_mix = ScalarMix(n_hidden_layers,\n",
    "                                         do_layer_norm=False)\n",
    "        else:\n",
    "            self._scalar_mix = None\n",
    "\n",
    "    def get_output_dim(self) -> int:\n",
    "        return self.output_dim\n",
    "\n",
    "    def forward(self,\n",
    "                input_ids: torch.LongTensor,\n",
    "                offsets: torch.LongTensor = None,\n",
    "                token_type_ids: torch.LongTensor = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_ids : ``torch.LongTensor``\n",
    "            The (batch_size, ..., max_sequence_length) tensor of wordpiece ids.\n",
    "        offsets : ``torch.LongTensor``, optional\n",
    "            The BERT embeddings are one per wordpiece. However it's possible/likely\n",
    "            you might want one per original token. In that case, ``offsets``\n",
    "            represents the indices of the desired wordpiece for each original token.\n",
    "            Depending on how your token indexer is configured, this could be the\n",
    "            position of the last wordpiece for each token, or it could be the position\n",
    "            of the first wordpiece for each token.\n",
    "\n",
    "            For example, if you had the sentence \"Definitely not\", and if the corresponding\n",
    "            wordpieces were [\"Def\", \"##in\", \"##ite\", \"##ly\", \"not\"], then the input_ids\n",
    "            would be 5 wordpiece ids, and the \"last wordpiece\" offsets would be [3, 4].\n",
    "            If offsets are provided, the returned tensor will contain only the wordpiece\n",
    "            embeddings at those positions, and (in particular) will contain one embedding\n",
    "            per token. If offsets are not provided, the entire tensor of wordpiece embeddings\n",
    "            will be returned.\n",
    "        token_type_ids : ``torch.LongTensor``, optional\n",
    "            If an input consists of two sentences (as in the BERT paper),\n",
    "            tokens from the first sentence should have type 0 and tokens from\n",
    "            the second sentence should have type 1.  If you don't provide this\n",
    "            (the default BertIndexer doesn't) then it's assumed to be all 0s.\n",
    "        \"\"\"\n",
    "        # pylint: disable=arguments-differ\n",
    "        if token_type_ids is None:\n",
    "            token_type_ids = torch.zeros_like(input_ids)\n",
    "\n",
    "        input_mask = (input_ids != 0).long()\n",
    "\n",
    "        # input_ids may have extra dimensions, so we reshape down to 2-d\n",
    "        # before calling the BERT model and then reshape back at the end.\n",
    "        all_encoder_layers, _ = self.bert_model(input_ids=nn_util.combine_initial_dims(input_ids),\n",
    "                                                token_type_ids=nn_util.combine_initial_dims(token_type_ids),\n",
    "                                                attention_mask=nn_util.combine_initial_dims(input_mask))\n",
    "        if self._scalar_mix is not None:\n",
    "            mix = self._scalar_mix(all_encoder_layers[-self.n_hidden_layers:], input_mask)\n",
    "        else:\n",
    "            mix = torch.stack(all_encoder_layers[-self.n_hidden_layers:]).mean(dim=0)\n",
    "\n",
    "        # At this point, mix is (batch_size * d1 * ... * dn, sequence_length, embedding_dim)\n",
    "\n",
    "        if offsets is None:\n",
    "            # Resize to (batch_size, d1, ..., dn, sequence_length, embedding_dim)\n",
    "            return nn_util.uncombine_initial_dims(mix, input_ids.size())\n",
    "        else:\n",
    "            # offsets is (batch_size, d1, ..., dn, orig_sequence_length)\n",
    "            offsets2d = nn_util.combine_initial_dims(offsets)\n",
    "            # now offsets is (batch_size * d1 * ... * dn, orig_sequence_length)\n",
    "            range_vector = nn_util.get_range_vector(offsets2d.size(0),\n",
    "                                                 device=nn_util.get_device_of(mix)).unsqueeze(1)\n",
    "            # selected embeddings is also (batch_size * d1 * ... * dn, orig_sequence_length)\n",
    "            selected_embeddings = mix[range_vector, offsets2d]\n",
    "\n",
    "            return util.uncombine_initial_dims(selected_embeddings, offsets.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.modules.text_field_embedders import BasicTextFieldEmbedder\n",
    "\n",
    "if config.model_type == \"standard\":\n",
    "    token_embedding = CustomEmbedding(num_embeddings=config.vocab_size + 5,\n",
    "                                      embedding_dim=config.embedding_dim,\n",
    "                                      trainable=not config.freeze_embeddings,\n",
    "                                      weight=torch.tensor(embedding_weights, dtype=torch.float),\n",
    "                                      dropout=config.dropoute, padding_index=0)\n",
    "    word_embeddings = BasicTextFieldEmbedder({\"tokens\": token_embedding})\n",
    "    \n",
    "elif \"elmo\" in config.model_type:\n",
    "    from allennlp.modules.token_embedders import ElmoTokenEmbedder\n",
    "    from allennlp.modules.elmo import Elmo\n",
    "\n",
    "    options_file = 'https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x1024_128_2048cnn_1xhighway/elmo_2x1024_128_2048cnn_1xhighway_options.json'\n",
    "    weight_file = 'https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x1024_128_2048cnn_1xhighway/elmo_2x1024_128_2048cnn_1xhighway_weights.hdf5'\n",
    "    \n",
    "    all_words_ordered = [w for i, w in sorted([(i, w) for w, i in vocab.get_token_to_index_vocabulary().items()])]\n",
    "    elmo_embedder = ElmoTokenEmbedder(\n",
    "        options_file, weight_file, dropout=0.5, # recommended value\n",
    "        vocab_to_cache=all_words_ordered if config.cache_elmo_embeddings else None,\n",
    "        do_layer_norm=False,\n",
    "        requires_grad=False,\n",
    "    )\n",
    "    # TODO: Find a way to skip character encodings\n",
    "    word_embeddings = ElmoTextFieldEmbedder({\"tokens\": elmo_embedder})\n",
    "    \n",
    "elif \"bert\" in config.model_type:\n",
    "    from allennlp.modules.token_embedders.bert_token_embedder import PretrainedBertEmbedder\n",
    "    bert_embedder = CustomBertEmbedder(\n",
    "            pretrained_model=config.model_type,\n",
    "            fine_tune=False, use_scalar_mix=False,\n",
    "    )\n",
    "    word_embeddings: TextFieldEmbedder = BasicTextFieldEmbedder({\"tokens\": bert_embedder},\n",
    "                                                                 # we'll be ignoring masks so we'll need to set this to True\n",
    "                                                                allow_unmatched_keys = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.pooling_type != \"bert_pool\":\n",
    "    embed_sz = word_embeddings.get_output_dim()\n",
    "    if config.use_word_level_features: \n",
    "        embed_sz += len(word_level_features)\n",
    "    rnn = BiRNN(rnn_type=rnn_type, n_layers=config.num_layers, \n",
    "                embed_sz=embed_sz, hidden_sz=config.hidden_sz, \n",
    "                dropoutw=config.dropoutw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.pooling_type != \"bert_pool\":\n",
    "    if config.pooling_type == \"attention\":\n",
    "        pooler = Attention(rnn.get_output_dim(), hidden_sz=rnn.get_output_dim(),\n",
    "                           out_sz=rnn.get_output_dim(), dim=1, \n",
    "                           use_bias=config.attention_bias)\n",
    "    elif config.pooling_type == \"multipool\":\n",
    "        pooler = MultiPooling(rnn.get_output_dim())\n",
    "    elif config.pooling_type == \"augmented_multipool\":\n",
    "        pooler = AugmentedMultiPool(rnn.get_output_dim(), \n",
    "                                    aug_sz=embed_sz)\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid pooling type {config.pooling_type}\")\n",
    "\n",
    "    encoder = BiRNNEncoder(\n",
    "            rnn,\n",
    "            pooler,\n",
    "            dropouti=config.dropouti,\n",
    "            dropoutr=config.dropoutr,\n",
    "        )\n",
    "else:\n",
    "    BERT_DIM = word_embeddings.get_output_dim()\n",
    "\n",
    "    class BertSentencePooler(Seq2VecEncoder):\n",
    "        def forward(self, embs: torch.tensor, \n",
    "                    mask: torch.tensor=None,\n",
    "                    **kwargs,\n",
    "                   ) -> torch.tensor:\n",
    "            # extract first token tensor\n",
    "            return embs[:, 0]\n",
    "\n",
    "        @overrides\n",
    "        def get_output_dim(self) -> int:\n",
    "            return BERT_DIM\n",
    "\n",
    "    encoder = BertSentencePooler(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.n_classes > 2:\n",
    "    if config.use_focal_loss:\n",
    "        loss = FocalLossWithLogits(config.focal_loss_alpha,\n",
    "                                   config.focal_loss_gamma)\n",
    "    elif config.use_mbern_loss:\n",
    "        loss = MBernLossWithLogits()\n",
    "        \n",
    "    else:\n",
    "        loss = nn.BCEWithLogitsLoss()\n",
    "else:\n",
    "    loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BaselineModel(\n",
    "    word_embeddings, \n",
    "    encoder, \n",
    "    loss,\n",
    "    out_sz=2**config.n_classes if use_mbern_loss else config.n_classes\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss._get_name()==\"MBernLossWithLogits\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaselineModel(\n",
       "  (word_embeddings): BasicTextFieldEmbedder(\n",
       "    (token_embedder_tokens): CustomEmbedding()\n",
       "  )\n",
       "  (encoder): BiRNNEncoder(\n",
       "    (dropouti): VariationalDropout()\n",
       "    (rnn): BiRNN(\n",
       "      (rnns): ModuleList(\n",
       "        (0): PytorchSeq2SeqWrapper(\n",
       "          (_module): LSTM(\n",
       "            300, 128, num_layers=2, batch_first=True, bidirectional=True\n",
       "            (input_drop): VariationalDropout()\n",
       "            (output_drop): VariationalDropout()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (dropouto): VariationalDropout()\n",
       "    (pool): AugmentedMultiPool(\n",
       "      (attn): Attention(\n",
       "        (l1): Linear(in_features=256, out_features=512, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (projection): Sequential(\n",
       "    (0): Linear(in_features=768, out_features=50, bias=True)\n",
       "    (1): ELU(alpha=1.0)\n",
       "    (2): Dropout(p=0.1)\n",
       "    (3): Linear(in_features=50, out_features=6, bias=True)\n",
       "  )\n",
       "  (loss): FocalLossWithLogits(\n",
       "    (_loss): BCEWithLogitsLoss()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize bias according to prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.bias_init:\n",
    "    \n",
    "    class_bias = torch.zeros(2**config.n_classes if use_mbern_loss else config.n_classes)\n",
    "    \n",
    "    for i, _ in enumerate(label_cols):\n",
    "        p = train_labels[:, i].mean()\n",
    "        class_bias[i] = np.log(p / (1-p))\n",
    "\n",
    "    model.projection[-1].bias.data = class_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_GPU: model.cuda()\n",
    "else: model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), DATA_ROOT / \"tmp_model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic sanity checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = nn_util.move_to_device(batch, 0 if USE_GPU else -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = batch[\"tokens\"]\n",
    "labels = batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': tensor([[    5,  1391,   490,  ...,     0,     0,     0],\n",
       "         [    5,   225,     3,  ...,     0,     0,     0],\n",
       "         [   52,   263,    25,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [    5,    31,    31,  ...,     0,     0,     0],\n",
       "         [    5, 39905,  2774,  ...,     0,     0,     0],\n",
       "         [ 6453,     4,   338,  ...,    73,  6262,     2]], device='cuda:0')}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = get_text_field_mask(tokens)\n",
    "wlfs = batch[\"word_level_features\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = model.word_embeddings(tokens)\n",
    "if config.use_word_level_features:\n",
    "    embeddings = torch.cat([wlfs, embeddings], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.use_word_level_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 1130, 300])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 1130])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.pooling_type != \"bert_pool\":\n",
    "    encoded = model.encoder.rnn(embeddings, mask=mask)\n",
    "else:\n",
    "    encoded = model.encoder(embeddings, mask=mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 1130, 256])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'class_logits': tensor([[-2.2144, -4.3826, -2.9863, -5.8451, -3.0174, -4.7472],\n",
       "         [-2.2868, -4.6116, -2.8953, -5.7901, -3.0686, -4.7133],\n",
       "         [-2.3129, -4.4941, -2.9188, -5.8395, -2.9499, -4.7012],\n",
       "         [-2.2655, -4.5313, -3.0153, -5.8318, -2.9557, -4.7310],\n",
       "         [-2.2385, -4.5033, -2.8185, -5.7382, -2.9352, -4.6949],\n",
       "         [-2.3782, -4.5380, -2.8418, -5.7320, -2.9605, -4.7240],\n",
       "         [-2.2698, -4.5407, -2.9082, -5.7936, -3.0371, -4.6980],\n",
       "         [-2.3528, -4.6029, -2.8685, -5.7746, -2.9798, -4.6912],\n",
       "         [-2.2144, -4.5016, -2.8562, -5.7118, -2.9536, -4.7481],\n",
       "         [-2.3756, -4.5480, -2.7871, -5.8904, -2.9693, -4.7097],\n",
       "         [-2.3032, -4.5681, -2.8706, -5.7675, -3.0434, -4.6767],\n",
       "         [-2.3629, -4.5017, -2.9427, -5.8644, -2.9962, -4.7281],\n",
       "         [-2.2017, -4.4897, -2.9278, -5.7714, -2.9777, -4.6823],\n",
       "         [-2.4082, -4.5874, -2.8418, -5.8327, -2.9790, -4.6644],\n",
       "         [-2.2974, -4.4995, -2.9415, -5.8085, -3.0006, -4.7328],\n",
       "         [-2.2846, -4.5671, -3.0004, -5.7115, -2.9969, -4.6953],\n",
       "         [-2.3772, -4.5656, -2.9464, -5.7505, -2.9862, -4.6746],\n",
       "         [-2.4221, -4.5101, -2.8846, -5.8672, -2.8899, -4.6776],\n",
       "         [-2.3472, -4.5522, -2.8620, -5.9017, -3.0122, -4.7132],\n",
       "         [-2.3792, -4.4652, -2.9447, -5.9300, -2.9679, -4.6405],\n",
       "         [-2.2843, -4.5878, -2.9207, -5.7365, -2.9939, -4.7602],\n",
       "         [-2.2443, -4.5017, -2.9847, -5.8272, -2.9905, -4.7248],\n",
       "         [-2.3764, -4.4997, -2.9099, -5.7805, -3.0230, -4.8301],\n",
       "         [-2.3568, -4.5726, -2.8503, -5.7536, -3.0624, -4.7091],\n",
       "         [-2.3418, -4.5456, -2.9077, -5.8144, -3.1069, -4.6538],\n",
       "         [-2.2948, -4.5956, -2.9101, -5.9338, -2.9870, -4.6985],\n",
       "         [-2.2742, -4.6139, -2.8543, -5.8162, -3.0982, -4.6773],\n",
       "         [-2.2237, -4.5815, -2.9637, -5.7466, -2.9742, -4.7092],\n",
       "         [-2.3325, -4.6105, -2.8888, -5.7912, -2.9941, -4.7331],\n",
       "         [-2.2633, -4.5166, -2.8954, -5.8234, -2.9577, -4.7323],\n",
       "         [-2.4048, -4.5634, -3.0020, -5.7639, -2.9950, -4.6571],\n",
       "         [-2.2751, -4.6121, -2.9406, -5.7569, -2.9895, -4.7585],\n",
       "         [-2.3054, -4.5152, -2.9460, -5.8881, -2.9417, -4.7273],\n",
       "         [-2.3380, -4.6197, -2.9151, -5.8106, -3.0482, -4.7646],\n",
       "         [-2.3121, -4.5199, -2.9473, -5.8849, -3.0091, -4.7323],\n",
       "         [-2.3839, -4.6865, -2.7541, -5.6724, -3.0303, -4.6990],\n",
       "         [-2.2628, -4.5087, -2.9374, -5.8932, -2.9890, -4.6658],\n",
       "         [-2.3370, -4.6559, -2.8959, -5.8006, -2.9679, -4.7239],\n",
       "         [-2.3302, -4.5753, -2.9559, -5.7308, -3.0262, -4.6216],\n",
       "         [-2.2441, -4.5429, -2.9834, -5.7523, -3.0970, -4.7076],\n",
       "         [-2.3156, -4.4778, -2.8924, -5.8582, -2.9539, -4.6766],\n",
       "         [-2.1969, -4.5099, -2.8566, -5.7766, -3.0197, -4.7084],\n",
       "         [-2.2422, -4.5872, -2.9566, -5.7289, -2.9439, -4.7588],\n",
       "         [-2.3067, -4.4686, -2.8522, -5.8891, -2.9568, -4.5980],\n",
       "         [-2.3712, -4.5165, -2.9319, -5.8047, -2.9423, -4.7449],\n",
       "         [-2.2651, -4.5033, -2.9450, -5.8538, -2.9195, -4.7013],\n",
       "         [-2.4298, -4.5409, -2.9251, -5.8676, -2.9014, -4.7010],\n",
       "         [-2.2269, -4.4810, -2.8857, -5.8281, -3.0247, -4.7203],\n",
       "         [-2.3116, -4.6989, -2.9292, -5.8263, -2.9813, -4.7795],\n",
       "         [-2.3151, -4.4916, -2.9990, -5.8245, -2.8869, -4.6942],\n",
       "         [-2.4702, -4.5699, -2.9448, -5.7395, -2.9853, -4.7377],\n",
       "         [-2.2891, -4.5022, -2.9637, -5.7633, -3.1085, -4.6408],\n",
       "         [-2.2372, -4.4553, -2.8582, -5.7895, -2.9782, -4.7960],\n",
       "         [-2.3532, -4.5425, -2.9051, -5.8291, -3.0221, -4.6619],\n",
       "         [-2.3745, -4.6133, -2.7698, -5.8638, -2.9631, -4.6845],\n",
       "         [-2.2687, -4.5060, -2.9007, -5.7415, -2.9926, -4.8127],\n",
       "         [-2.3548, -4.6025, -2.9281, -5.8265, -3.0468, -4.7152],\n",
       "         [-2.3092, -4.5144, -2.9872, -5.8536, -2.9635, -4.6703],\n",
       "         [-2.4533, -4.5624, -2.8115, -5.8409, -2.9309, -4.6707],\n",
       "         [-2.3872, -4.5162, -2.8894, -5.8317, -2.9060, -4.6883],\n",
       "         [-2.2378, -4.6118, -2.9185, -5.7989, -3.1177, -4.6664],\n",
       "         [-2.2979, -4.4793, -2.8606, -5.7848, -2.9724, -4.6519],\n",
       "         [-2.2476, -4.6200, -2.8903, -5.7426, -2.9276, -4.7132],\n",
       "         [-2.3519, -4.6795, -2.8916, -5.8165, -2.9980, -4.6448],\n",
       "         [-2.3063, -4.5734, -2.9832, -5.8398, -3.0153, -4.7087],\n",
       "         [-2.3876, -4.5064, -2.9483, -5.8107, -2.9631, -4.6422],\n",
       "         [-2.2990, -4.6069, -2.8582, -5.7554, -3.0292, -4.7755],\n",
       "         [-2.3498, -4.5295, -2.9000, -5.7631, -3.0572, -4.7600],\n",
       "         [-2.3867, -4.5914, -2.8831, -5.7903, -2.9293, -4.6694],\n",
       "         [-2.2434, -4.5530, -2.9308, -5.7968, -3.0460, -4.6896],\n",
       "         [-2.3423, -4.5654, -2.8867, -5.8149, -2.8842, -4.7745],\n",
       "         [-2.2999, -4.4890, -2.8428, -5.7957, -2.9421, -4.7865],\n",
       "         [-2.2746, -4.4786, -2.9921, -5.8475, -2.8890, -4.6801],\n",
       "         [-2.3501, -4.5259, -2.9804, -5.8474, -3.0113, -4.7025],\n",
       "         [-2.2699, -4.4998, -2.9546, -5.7817, -2.9912, -4.6666],\n",
       "         [-2.1898, -4.4834, -2.8037, -5.7837, -3.0763, -4.7269],\n",
       "         [-2.2670, -4.4230, -2.9267, -5.7999, -3.0038, -4.7005],\n",
       "         [-2.3851, -4.5598, -2.8695, -5.7915, -2.8907, -4.7083],\n",
       "         [-2.2901, -4.5542, -2.9401, -5.8263, -3.0244, -4.6802],\n",
       "         [-2.4191, -4.5982, -2.8073, -5.7503, -2.9892, -4.7203],\n",
       "         [-2.3234, -4.3969, -2.9665, -5.8481, -2.9814, -4.7725],\n",
       "         [-2.3310, -4.6451, -2.9045, -5.7840, -3.0099, -4.7213],\n",
       "         [-2.4267, -4.5251, -2.9173, -5.8932, -3.0177, -4.7211],\n",
       "         [-2.4037, -4.7407, -2.8696, -5.7453, -2.9897, -4.6902],\n",
       "         [-2.3380, -4.4886, -2.9373, -5.8449, -3.0606, -4.6959],\n",
       "         [-2.2929, -4.4762, -2.9283, -5.8659, -2.9781, -4.7604],\n",
       "         [-2.3257, -4.4992, -2.9251, -5.8531, -3.0032, -4.6445],\n",
       "         [-2.3202, -4.6563, -2.8179, -5.7867, -2.9936, -4.7568],\n",
       "         [-2.3440, -4.6552, -2.8397, -5.6894, -3.1165, -4.6921],\n",
       "         [-2.1641, -4.5185, -2.8707, -5.6905, -2.9673, -4.7354],\n",
       "         [-2.2402, -4.6212, -2.8936, -5.7834, -3.0038, -4.7541],\n",
       "         [-2.2844, -4.5839, -2.8830, -5.7142, -2.9626, -4.7984],\n",
       "         [-2.3454, -4.6021, -2.6927, -5.7791, -2.9889, -4.5912],\n",
       "         [-2.1957, -4.4199, -2.9399, -5.7314, -2.9430, -4.7553],\n",
       "         [-2.3326, -4.6294, -2.9073, -5.7855, -3.1015, -4.6438],\n",
       "         [-2.2970, -4.6315, -2.8857, -5.7648, -3.0020, -4.7138],\n",
       "         [-2.3634, -4.5558, -2.9688, -5.8399, -3.0996, -4.6637],\n",
       "         [-2.3392, -4.4358, -2.8874, -5.7324, -3.0957, -4.7398],\n",
       "         [-2.2661, -4.5904, -2.8564, -5.7276, -3.0272, -4.6966],\n",
       "         [-2.2766, -4.6051, -2.9200, -5.7534, -2.9754, -4.8613],\n",
       "         [-2.2832, -4.4422, -2.9721, -5.7514, -3.0041, -4.7275],\n",
       "         [-2.3577, -4.5260, -2.9402, -5.9189, -3.0286, -4.6892],\n",
       "         [-2.3094, -4.4540, -2.8887, -5.7474, -2.9785, -4.6278],\n",
       "         [-2.2993, -4.5364, -2.8712, -5.7616, -2.9126, -4.6518],\n",
       "         [-2.2899, -4.5420, -2.8669, -5.8233, -3.0088, -4.6868],\n",
       "         [-2.2661, -4.5499, -2.8523, -5.7034, -3.0428, -4.7761],\n",
       "         [-2.3067, -4.5897, -2.8648, -5.7819, -3.0347, -4.7087],\n",
       "         [-2.3491, -4.5578, -2.8930, -5.8392, -2.9116, -4.7329],\n",
       "         [-2.2867, -4.5324, -2.9306, -5.8708, -3.1082, -4.6963],\n",
       "         [-2.3228, -4.5211, -2.9231, -5.7610, -3.0172, -4.7519],\n",
       "         [-2.3318, -4.5908, -2.8763, -5.8498, -2.9586, -4.7343],\n",
       "         [-2.1805, -4.5572, -2.8798, -5.8365, -2.9834, -4.6304],\n",
       "         [-2.1858, -4.4430, -2.9406, -5.8099, -2.8604, -4.7477],\n",
       "         [-2.3721, -4.5262, -2.8605, -5.7842, -3.0374, -4.7722],\n",
       "         [-2.2272, -4.4406, -2.9915, -5.8001, -3.0146, -4.6763],\n",
       "         [-2.3526, -4.6267, -2.9133, -5.7573, -3.0780, -4.5964],\n",
       "         [-2.3615, -4.4770, -2.9179, -5.8462, -2.9842, -4.6878],\n",
       "         [-2.3850, -4.5782, -2.9205, -5.7277, -3.1661, -4.6419],\n",
       "         [-2.3288, -4.5805, -2.9538, -5.7627, -3.0794, -4.7048],\n",
       "         [-2.2309, -4.5503, -2.8943, -5.7291, -3.0172, -4.6916],\n",
       "         [-2.2899, -4.5188, -2.9849, -5.8255, -3.0879, -4.7390],\n",
       "         [-2.3031, -4.4768, -2.9171, -5.6780, -3.0166, -4.7101],\n",
       "         [-2.1921, -4.5090, -3.0090, -5.7610, -3.0793, -4.7330],\n",
       "         [-2.2550, -4.5924, -3.0291, -5.7164, -3.1336, -4.6211],\n",
       "         [-2.3145, -4.6155, -2.9291, -5.6910, -3.0338, -4.7483],\n",
       "         [-2.3544, -4.5574, -2.8854, -5.8427, -2.9794, -4.7816],\n",
       "         [-2.3486, -4.5795, -2.8337, -5.8221, -3.0593, -4.7378],\n",
       "         [-2.2915, -4.4704, -2.9175, -5.7973, -3.0213, -4.7195]],\n",
       "        device='cuda:0', grad_fn=<AddmmBackward>),\n",
       " 'accuracy': 0.9778645833333334,\n",
       " 'loss': tensor(0.0545, device='cuda:0', grad_fn=<MeanBackward1>)}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(**batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = model(**batch)[\"loss\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0545, device='cuda:0', grad_fn=<MeanBackward1>)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"label\"].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"tokens\"][\"tokens\"].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.mixup(**batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.training import Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StopTraining(Exception): pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NanWeightMonitor(Callback):\n",
    "    def on_backward_end(self, data):\n",
    "        for name, param in self.trainer.model.named_parameters():\n",
    "            if torch.isnan(param.data).any() or torch.isinf(param.data).any():\n",
    "                raise StopTraining(f\"Nan/Inf weights in param {name}: \\n {param}\")\n",
    "try:\n",
    "    import jupyter_slack\n",
    "    can_notify = True\n",
    "except:\n",
    "    jupyter_slack = None\n",
    "    can_notify = False\n",
    "\n",
    "class SlackNotification(Callback):\n",
    "    def __init__(self, silent):\n",
    "        self.silent = silent\n",
    "    def on_train_end(self, data):\n",
    "        if not self.silent and can_notify:\n",
    "            try:\n",
    "                jupyter_slack.notify_self(f\"Finished training with state {self.trainer._state}\")\n",
    "            except: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TensorboardCallback(Callback):\n",
    "    \"\"\"For now, delegate all processing to the trainer's own methods\"\"\"\n",
    "    def on_batch_begin(self):\n",
    "        self._log_histograms_this_batch = \\\n",
    "        self.trainer._histogram_interval is not None and (\n",
    "            self.trainer._batch_num_total % self.trainer._histogram_interval == 0)\n",
    "    \n",
    "    def on_backward_end(self, loss):\n",
    "        if self._log_histograms_this_batch:\n",
    "            # get the magnitude of parameter updates for logging\n",
    "            # We need a copy of current parameters to compute magnitude of updates,\n",
    "            # and copy them to CPU so large models won't go OOM on the GPU.\n",
    "            self.param_updates = {\n",
    "                name: param.detach().cpu().clone()\n",
    "                for name, param in self.trainer.model.named_parameters()\n",
    "            }\n",
    "    \n",
    "    def on_step_end(self, loss):\n",
    "        if self._log_histograms_this_batch:\n",
    "            for name, param in self.trainer.model.named_parameters():\n",
    "                self.param_updates[name].sub_(param.detach().cpu())\n",
    "                update_norm = torch.norm(self.param_updates[name].view(-1, ))\n",
    "                param_norm = torch.norm(param.view(-1, )).cpu()\n",
    "                self.trainer._tensorboard.add_train_scalar(\n",
    "                    \"gradient_update/\" + name,\n",
    "                     update_norm / (param_norm + 1e-7),\n",
    "                     batch_num_total\n",
    "                )\n",
    "            self.param_updates = {} # release memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mixup(Callback):\n",
    "    \"\"\"Does mixup in embedding space\n",
    "    TODO: Figure out how to best handle masking...\n",
    "    \"\"\"\n",
    "    def __init__(self, weight: float, \n",
    "                 batch_iterator: Optional[Iterable[TensorDict]]=None):\n",
    "        self.weight = weight\n",
    "        self.batch_iterator = batch_iterator\n",
    "        \n",
    "    def on_batch_loss(self, batch: TensorDict, for_training=True):\n",
    "        if for_training and self.weight > 0.:\n",
    "            # use mixup iterator if exists\n",
    "            if self.batch_iterator is not None: batch = next(self.batch_iterator)\n",
    "            mixup_output_dict = self.model.mixup(**batch)\n",
    "            return mixup_output_dict[\"loss\"] * self.weight\n",
    "        else:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscreteMixup(Callback):\n",
    "    \"\"\"Mixes up and concatenates sentences within a batch\"\"\"\n",
    "    def __init__(self, weight: float, \n",
    "                 batch_iterator: Optional[Iterable[TensorDict]]=None):\n",
    "        self.weight = weight\n",
    "        self.batch_iterator = batch_iterator\n",
    "    \n",
    "    def on_batch_loss(self, batch: TensorDict, for_training=True):\n",
    "        if for_training and self.weight > 0.:\n",
    "            # use mixup iterator if exists\n",
    "            if self.batch_iterator is not None: \n",
    "                batch = next(self.batch_iterator)\n",
    "                \n",
    "            # create permutation\n",
    "            tokens, label = batch[\"tokens\"], batch[\"label\"]\n",
    "            bs = label.size(0)\n",
    "            shuf = torch.randperm(bs).to(label.device)\n",
    "            tokens2 = permute(tokens, shuf)\n",
    "            labels2 = permute(label, shuf)\n",
    "            \n",
    "            # join the sentences\n",
    "            n_tokens1 = get_text_field_mask(tokens).sum(1)\n",
    "            n_tokens2 = get_text_field_mask(tokens2).sum(1)\n",
    "            maxlen = min(config.max_seq_len, (n_tokens1 + n_tokens2).sum())\n",
    "            # TODO: Is there a faster way?\n",
    "            new_tokens = torch.zeros(bs, maxlen, \n",
    "                                     dtype=torch.long).to(label.device)\n",
    "            for i, (t1, t2) in enumerate(zip(tokens[\"tokens\"], tokens2[\"tokens\"])):\n",
    "                l1, l2 = n_tokens1[i].item(), n_tokens2[i].item()\n",
    "                new_tokens[i, :l1] = t1 # TODO: Fairly divide the capacity\n",
    "                new_tokens[i, l1:min(maxlen, l1+l2)] = \\\n",
    "                    t2[:min(maxlen-l1, l2)]\n",
    "            \n",
    "            # compute loss on new batch\n",
    "            new_batch = {k: v for k, v in batch.items()}\n",
    "            new_batch[\"tokens\"] = {\"tokens\": new_tokens}\n",
    "            new_batch[\"label\"] = new_label\n",
    "            return model(**new_batch)[\"loss\"] * self.weight\n",
    "        else:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearEpochDropoutSchedule:\n",
    "    def __init__(self, n_epochs, start_do, end_do):\n",
    "        self.n_epochs = n_epochs\n",
    "        self.start_do = start_do\n",
    "        self.end_do = end_do\n",
    "        if n_epochs > 1:\n",
    "            self.delta_do = (end_do - start_do) / (n_epochs - 1)\n",
    "        else:\n",
    "            self.delta_do = 0 # cannot change dropout if only one epoch\n",
    "\n",
    "    def __call__(self, epochs, batches_this_epoch, batches_total):\n",
    "        return self.start_do + self.delta_do * epochs\n",
    "\n",
    "class DropoutScheduler(Callback):\n",
    "    def __init__(self, \n",
    "                 module: nn.Module,\n",
    "                 schedule: Callable[[int, int, int], float]):\n",
    "        self._module = module\n",
    "        self._schedule = schedule\n",
    "        self.epoch = 0\n",
    "    \n",
    "    def on_batch_end(self, data):\n",
    "        self._module.dropout = self._schedule(self.epoch, \n",
    "                                              data[\"batches_this_epoch\"],\n",
    "                                              data[\"batch_num_total\"],\n",
    "                                             )\n",
    "    def on_epoch_end(self, data):\n",
    "        # handle per-epoch schedules\n",
    "        self.epoch += 1\n",
    "        self._module.dropout = self._schedule(self.epoch, -1, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test performance when input is all 0s\n",
    "- If our initialization works decently, the loss should barely/not move and accuracy should stay constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.training import TrainerWithCallbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.debugging:\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config.lr)\n",
    "    model.test_mode()\n",
    "    trainer = TrainerWithCallbacks(\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        iterator=iterator,\n",
    "        train_dataset=train_ds[:256],\n",
    "        cuda_device=0 if USE_GPU else -1,\n",
    "        num_epochs=5,\n",
    "    )\n",
    "    metrics = trainer.train()\n",
    "    model.load_state_dict(torch.load(DATA_ROOT / \"tmp_model.pth\"))\n",
    "    model.test_mode(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test performance on a small batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.debugging:\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config.lr)\n",
    "    state_dict = deepcopy(model.state_dict())\n",
    "    trainer = TrainerWithCallbacks(\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        iterator=iterator,\n",
    "        train_dataset=train_ds[:256],\n",
    "        cuda_device=0 if USE_GPU else -1,\n",
    "        num_epochs=50,\n",
    "    )\n",
    "    metrics = trainer.train()\n",
    "    model.load_state_dict(torch.load(DATA_ROOT / \"tmp_model.pth\"))\n",
    "    metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import math\n",
    "\n",
    "class LRFinder(Callback):\n",
    "    def __init__(self):\n",
    "        self.losses = []\n",
    "        self.lrs = []\n",
    "        self.best_loss = 1e9\n",
    "\n",
    "    def on_step_end(self, loss, **kwargs):\n",
    "        # Log the learning rate\n",
    "        self.losses.append(loss.item())\n",
    "        self.lrs.append(self.trainer.optimizer.state_dict()['param_groups'][0][\"lr\"])\n",
    "\n",
    "    def plot_loss(self, n_skip_beginning=10, n_skip_end=5):\n",
    "        plt.ylabel(\"loss\")\n",
    "        plt.xlabel(\"learning rate (log scale)\")\n",
    "        plt.plot(self.lrs[n_skip_beginning:-n_skip_end], self.losses[n_skip_beginning:-n_skip_end])\n",
    "        plt.xscale('log')\n",
    "\n",
    "    def plot_loss_change(self, sma=1, n_skip_beginning=10, n_skip_end=5, y_lim=(-0.01, 0.01)):\n",
    "        assert sma >= 1\n",
    "        derivatives = [0] * sma\n",
    "        for i in range(sma, len(self.lrs)):\n",
    "            derivative = (self.losses[i] - self.losses[i - sma]) / sma\n",
    "            derivatives.append(derivative)\n",
    "\n",
    "        plt.ylabel(\"rate of loss change\")\n",
    "        plt.xlabel(\"learning rate (log scale)\")\n",
    "        plt.plot(self.lrs[n_skip_beginning:-n_skip_end], derivatives[n_skip_beginning:-n_skip_end])\n",
    "        plt.xscale('log')\n",
    "        plt.ylim(y_lim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.find_lr:\n",
    "    from copy import deepcopy\n",
    "    \n",
    "    class ExponentialIncrease(torch.optim.lr_scheduler._LRScheduler):\n",
    "        def __init__(self, optimizer: torch.optim.Optimizer, n_iters: int,\n",
    "                     lr_start=1e-6, lr_end=2.0) -> None:\n",
    "            self.n_iters = n_iters\n",
    "            self.steps = 0\n",
    "            self.lr_start = lr_start\n",
    "            self.gamma = (lr_end / lr_start) ** (1 / n_iters)\n",
    "            super().__init__(optimizer)\n",
    "        def step(self, epoch=None): pass\n",
    "        def step_batch(self, epoch=None):\n",
    "            self.steps += 1\n",
    "            if epoch is None: epoch = self.last_epoch + 1\n",
    "            self.last_epoch = epoch\n",
    "            for param_group, learning_rate in zip(self.optimizer.param_groups, self.get_lr()):\n",
    "                param_group['lr'] = learning_rate\n",
    "        def get_lr(self):\n",
    "            return [self.lr_start * (self.gamma ** self.steps) for _ in self.base_lrs]\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-6)\n",
    "    lr_finder = LRFinder()\n",
    "    trainer = CustomTrainer(\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        iterator=iterator,\n",
    "        train_dataset=train_ds,\n",
    "        learning_rate_scheduler=ExponentialIncrease(optimizer, \n",
    "                                                    iterator.get_num_batches(train_ds)),\n",
    "        cuda_device=0 if USE_GPU else -1,\n",
    "        num_epochs=1,\n",
    "        callbacks=[lr_finder],\n",
    "    )\n",
    "    trainer.train()\n",
    "    model.load_state_dict(torch.load(DATA_ROOT / \"tmp_model.pth\"))\n",
    "    del model_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.find_lr:\n",
    "    lr_finder.plot_loss(n_skip_beginning=0, n_skip_end=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actual Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(DATA_ROOT / \"tmp_model.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"elmo\" in config.model_type:\n",
    "    # apply a small amount of l2 regularization to scalar params as recommended\n",
    "    # here: https://github.com/allenai/allennlp/blob/master/tutorials/how_to/elmo.md\n",
    "    pgroup_1 = [p for nm, p in model.named_parameters() if \"scalar_parameters\" not in nm and p.requires_grad]\n",
    "    pgroup_2 = [p for nm, p in model.named_parameters() if \"scalar_parameters\" in nm]\n",
    "    optimizer = optim.Adam([{\"params\": pgroup_1}, {\"params\": pgroup_2, \"weight_decay\": 0.001}], lr=config.lr, weight_decay=config.weight_decay)\n",
    "else:\n",
    "    optimizer = optim.Adam(model.parameters(), \n",
    "                           lr=config.lr, weight_decay=config.weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1006437"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def _prod(args):\n",
    "    acc = 1\n",
    "    for a in args: acc *= a\n",
    "    return acc\n",
    "num_trainable_params = sum([_prod(p.shape) for p in model.parameters() if p.requires_grad])\n",
    "num_trainable_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.training.learning_rate_schedulers import SlantedTriangular, CosineWithRestarts\n",
    "if config.lr_schedule == \"slanted_triangular\":\n",
    "    lr_sched = SlantedTriangular(optimizer, \n",
    "                                 num_epochs=config.epochs, \n",
    "                                 num_steps_per_epoch=iterator.get_num_batches(train_ds))\n",
    "elif config.lr_scheduler == \"cosine_annealing\":\n",
    "    lr_sched = optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer, T_max=iterator.get_num_batches(train_ds) * config.epochs,\n",
    "    )\n",
    "elif config.lr_scheduler is None:\n",
    "    lr_sched = None\n",
    "else:\n",
    "    raise ConfigurationError(f\"Invalid lr schedule {config.lr_scheduler} passed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_options = {\n",
    "    # TODO: Add appropriate learning rate scheduler\n",
    "    \"should_log_parameter_statistics\": True,\n",
    "    \"should_log_learning_rate\": True,\n",
    "    \"num_epochs\": config.epochs,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [NanWeightMonitor(), \n",
    "             SlackNotification(silent=config.testing)]\n",
    "if config.dropoute_max is not None:\n",
    "    callbacks.append(DropoutScheduler(\n",
    "        model.word_embeddings.token_embedder_tokens,\n",
    "        LinearEpochDropoutSchedule(config.epochs, config.dropoute, config.dropoute_max)\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (os.environ[\"IS_COLAB\"] != \"True\" and not config.testing):\n",
    "    SER_DIR = DATA_ROOT / \"ckpts\" / RUN_ID\n",
    "else:\n",
    "    SER_DIR = None\n",
    "\n",
    "trainer = TrainerWithCallbacks(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    iterator=iterator,\n",
    "    train_dataset=train_ds + train_aug_ds if config.use_augmented else train_ds,\n",
    "    validation_dataset=val_ds if config.val_ratio > 0.0 else None,\n",
    "    serialization_dir=SER_DIR,\n",
    "    cuda_device=0 if USE_GPU else -1,\n",
    "    gradient_accumulation_steps=config.batch_size // config.computational_batch_size,\n",
    "    callbacks=callbacks,\n",
    "    learning_rate_scheduler=lr_sched,\n",
    "    **training_options,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9750, loss: 0.0191 ||: 100%|██████████| 1247/1247 [05:33<00:00,  3.60it/s]\n",
      "accuracy: 0.9787, loss: 0.0133 ||: 100%|██████████| 1247/1247 [05:13<00:00,  3.85it/s]\n",
      "accuracy: 0.9800, loss: 0.0120 ||: 100%|██████████| 1247/1247 [05:13<00:00,  4.40it/s]\n",
      "accuracy: 0.9809, loss: 0.0113 ||: 100%|██████████| 1247/1247 [05:14<00:00,  3.95it/s]\n"
     ]
    }
   ],
   "source": [
    "metrics = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0545, device='cuda:0', grad_fn=<MeanBackward1>)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'best_epoch': 3,\n",
       " 'peak_cpu_memory_MB': 5465.764,\n",
       " 'peak_gpu_0_memory_MB': 10538,\n",
       " 'training_duration': '00:21:24',\n",
       " 'training_start_epoch': 0,\n",
       " 'training_epochs': 3,\n",
       " 'epoch': 3,\n",
       " 'training_accuracy': 0.9809478748227017,\n",
       " 'training_loss': 0.011252634498569293,\n",
       " 'training_cpu_memory_MB': 5465.764,\n",
       " 'training_gpu_0_memory_MB': 10538}"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('word_embeddings.token_embedder_tokens.weight', Parameter containing:\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1111, -0.0014, -0.1778,  ...,  0.0634, -0.1216,  0.0393],\n",
      "        ...,\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0'))\n",
      "('encoder.rnn.rnns.0._module.weight_ih_l0', Parameter containing:\n",
      "tensor([[-0.0402, -0.0047, -0.0755,  ..., -0.1781, -0.0917, -0.1025],\n",
      "        [-0.0172,  0.5338,  0.1886,  ..., -0.0750, -0.3072,  0.0497],\n",
      "        [-0.1100,  0.2496,  0.0423,  ..., -0.0338, -0.4616,  0.1071],\n",
      "        ...,\n",
      "        [-0.2888, -0.0898,  0.2549,  ..., -0.0920,  0.1364,  0.1141],\n",
      "        [ 0.0205,  0.0225,  0.0851,  ..., -0.0401,  0.0954,  0.2105],\n",
      "        [-0.1580,  0.2131,  0.2435,  ..., -0.2104,  0.2198, -0.2380]],\n",
      "       device='cuda:0', requires_grad=True))\n",
      "('encoder.rnn.rnns.0._module.weight_hh_l0', Parameter containing:\n",
      "tensor([[-5.1443e-02, -4.9291e-02,  4.6337e-02,  ..., -1.1228e-01,\n",
      "         -1.0581e-01, -1.3070e-01],\n",
      "        [-5.6189e-02, -7.6692e-02,  1.6517e-01,  ..., -9.7178e-02,\n",
      "         -6.7963e-03, -4.2751e-01],\n",
      "        [-1.9916e-03,  6.9586e-02, -1.8715e-05,  ..., -2.7521e-01,\n",
      "         -8.8813e-03,  5.2465e-02],\n",
      "        ...,\n",
      "        [ 2.0437e-02,  2.5876e-01,  5.1803e-02,  ..., -3.1119e-02,\n",
      "         -1.2019e-01,  6.0384e-02],\n",
      "        [ 3.1050e-02,  8.6527e-02,  1.3124e-01,  ...,  1.1873e-02,\n",
      "         -4.6288e-02, -1.9836e-02],\n",
      "        [-3.6670e-03, -1.7961e-02,  4.6184e-02,  ...,  1.5136e-01,\n",
      "          3.7067e-02,  2.2712e-02]], device='cuda:0', requires_grad=True))\n",
      "('encoder.rnn.rnns.0._module.bias_ih_l0', Parameter containing:\n",
      "tensor([-1.0775e-01, -1.3437e-01, -2.8567e-02, -2.5765e-02, -7.1188e-02,\n",
      "         5.7263e-03, -3.1217e-04, -1.5193e-02, -4.7889e-02, -5.4343e-02,\n",
      "        -7.0347e-02, -2.3630e-01, -2.0059e-01, -4.1305e-02, -1.8068e-01,\n",
      "        -1.7601e-01, -1.4062e-02, -1.0480e-01, -9.5575e-03, -1.8192e-01,\n",
      "         3.0971e-02, -8.0700e-02, -1.4840e-02, -1.2630e-01, -9.9954e-03,\n",
      "        -1.5080e-01, -6.1938e-02,  2.1855e-02, -1.8375e-03, -8.8703e-02,\n",
      "        -1.4511e-02, -1.0086e-01,  8.4702e-02, -3.6451e-02, -5.8224e-02,\n",
      "        -1.3944e-01, -1.7176e-01, -7.2940e-02, -2.4222e-02, -6.4831e-02,\n",
      "        -1.3597e-01, -1.0410e-01, -1.8614e-01, -7.7514e-02, -6.6750e-02,\n",
      "        -8.1674e-02, -5.5239e-02, -5.1553e-02, -1.5639e-02, -7.2652e-02,\n",
      "         1.2941e-02, -1.1565e-01, -3.4790e-02, -3.0606e-02,  2.4753e-02,\n",
      "        -1.4949e-01, -1.8712e-02, -1.9903e-02, -2.0673e-01, -1.8521e-01,\n",
      "        -2.1712e-01, -1.3878e-01, -1.1612e-01, -1.3541e-01, -1.5531e-01,\n",
      "        -1.6418e-01, -2.0989e-02, -1.4358e-01, -6.6187e-02, -4.7120e-02,\n",
      "        -3.2452e-01, -3.5899e-02, -9.8371e-03, -2.4718e-02, -2.8093e-01,\n",
      "        -1.3618e-01, -9.5027e-02, -1.6624e-02, -2.1895e-01, -2.5000e-01,\n",
      "        -1.0553e-02, -1.2722e-01, -3.8289e-02, -2.1011e-01, -4.4811e-03,\n",
      "        -8.1178e-02, -9.7910e-02, -9.8762e-02, -3.1868e-02,  1.8897e-02,\n",
      "        -4.2482e-02, -1.5014e-02, -2.2721e-02, -1.6228e-01, -1.2269e-01,\n",
      "        -2.8197e-02, -1.4773e-02,  1.5644e-01, -3.1498e-01, -6.4285e-02,\n",
      "        -2.4699e-02, -8.2603e-02, -2.5953e-02, -1.0880e-01, -1.1090e-01,\n",
      "        -7.7882e-02, -5.8024e-04,  1.9757e-01, -2.1236e-01, -1.0754e-01,\n",
      "         1.4255e-01, -3.8281e-02, -8.5444e-02, -8.9195e-02, -9.6181e-02,\n",
      "         1.1608e-01, -1.7409e-02, -2.6171e-01, -6.6498e-02, -1.2118e-02,\n",
      "        -9.7160e-02, -2.5115e-02, -4.8242e-02,  3.1723e-02, -1.1277e-01,\n",
      "        -1.5331e-01, -6.4787e-02, -7.7324e-02,  9.1564e-01,  9.0135e-01,\n",
      "         9.0143e-01,  1.0093e+00,  9.5110e-01,  9.6587e-01,  9.1397e-01,\n",
      "         9.5832e-01,  9.3933e-01,  9.4018e-01,  9.7263e-01,  1.2946e+00,\n",
      "         9.6975e-01,  9.3930e-01,  1.0793e+00,  1.2280e+00,  1.0637e+00,\n",
      "         9.7397e-01,  9.4709e-01,  1.0447e+00,  1.0612e+00,  9.7035e-01,\n",
      "         1.0686e+00,  8.9682e-01,  9.2830e-01,  1.1626e+00,  1.0724e+00,\n",
      "         1.0734e+00,  1.1700e+00,  1.0159e+00,  9.1972e-01,  9.6757e-01,\n",
      "         1.0576e+00,  9.8711e-01,  9.4896e-01,  9.6296e-01,  1.0947e+00,\n",
      "         8.6651e-01,  9.5719e-01,  9.3095e-01,  9.7674e-01,  9.3655e-01,\n",
      "         1.0462e+00,  1.0425e+00,  9.3801e-01,  8.8172e-01,  8.6071e-01,\n",
      "         9.4580e-01,  9.5951e-01,  1.2186e+00,  7.2980e-01,  1.1324e+00,\n",
      "         9.0466e-01,  9.3883e-01,  9.2426e-01,  1.0158e+00,  9.5406e-01,\n",
      "         9.8025e-01,  1.0083e+00,  1.0439e+00,  9.9105e-01,  9.5325e-01,\n",
      "         1.0285e+00,  9.6291e-01,  1.1691e+00,  8.7222e-01,  9.8543e-01,\n",
      "         9.6692e-01,  9.5351e-01,  8.9112e-01,  1.1720e+00,  9.0991e-01,\n",
      "         9.0641e-01,  8.5767e-01,  1.1417e+00,  1.1014e+00,  9.7207e-01,\n",
      "         9.2697e-01,  9.5077e-01,  1.0091e+00,  8.8312e-01,  9.4108e-01,\n",
      "         9.4475e-01,  1.1566e+00,  9.0670e-01,  9.1596e-01,  1.1024e+00,\n",
      "         9.4049e-01,  9.1262e-01,  9.6449e-01,  8.4872e-01,  8.7576e-01,\n",
      "         1.0881e+00,  9.9114e-01,  9.1124e-01,  9.1658e-01,  8.9974e-01,\n",
      "         1.1337e+00,  1.1378e+00,  9.2566e-01,  8.7374e-01,  8.7055e-01,\n",
      "         9.9601e-01,  9.7694e-01,  8.7561e-01,  9.1203e-01,  9.6869e-01,\n",
      "         1.0740e+00,  1.1366e+00,  8.7273e-01,  1.1423e+00,  8.8201e-01,\n",
      "         1.0209e+00,  9.2764e-01,  9.0219e-01,  9.9993e-01,  9.1010e-01,\n",
      "         1.1862e+00,  9.5237e-01,  9.6957e-01,  1.0489e+00,  9.3018e-01,\n",
      "         9.5261e-01,  9.7863e-01,  8.8699e-01,  8.8955e-01,  1.0826e+00,\n",
      "         8.6017e-01,  1.5099e-02,  8.3769e-03, -2.8260e-02, -6.8756e-02,\n",
      "        -2.2902e-02, -6.8285e-02,  4.0585e-02,  1.4890e-02,  1.8236e-02,\n",
      "        -3.5850e-02, -1.0553e-02,  2.6926e-02, -4.1611e-03,  3.8108e-02,\n",
      "        -8.7793e-03,  3.9050e-02, -4.7394e-02,  3.4433e-02, -9.5722e-02,\n",
      "         1.9071e-02, -7.5645e-02, -2.3139e-02, -5.9125e-02, -8.6750e-03,\n",
      "         6.5809e-02, -2.3581e-03, -4.0293e-02, -8.2602e-02, -9.1064e-02,\n",
      "        -4.8782e-02, -6.9114e-02,  3.7289e-03, -1.3681e-01,  1.2780e-03,\n",
      "         6.4875e-02, -5.6065e-02,  1.8560e-02,  1.8919e-02,  1.5327e-02,\n",
      "        -6.1724e-02,  2.2234e-02, -3.3969e-02, -3.1172e-02,  1.2079e-02,\n",
      "         1.7438e-03,  1.3804e-02, -4.4288e-02, -3.0676e-03, -8.0779e-02,\n",
      "        -5.7079e-02, -8.1635e-02,  1.1961e-02,  4.1408e-02, -5.3523e-02,\n",
      "        -1.0344e-02, -3.4019e-02,  6.2505e-03,  8.8661e-02,  6.8099e-03,\n",
      "        -1.7594e-02,  2.4513e-02,  1.0903e-02, -2.9489e-02,  5.7120e-02,\n",
      "        -1.2238e-02,  9.4850e-02,  1.0958e-01,  1.6864e-02, -1.5254e-03,\n",
      "        -4.3007e-02,  1.1885e-02,  3.5800e-02,  1.7350e-02, -1.5759e-02,\n",
      "         1.7403e-02, -9.3920e-03, -8.6207e-03,  2.8336e-02, -1.5866e-02,\n",
      "         1.3208e-02, -5.6595e-02,  2.7308e-02, -5.0754e-02,  1.9638e-02,\n",
      "         7.2320e-03,  3.3757e-03, -6.0146e-03,  1.5878e-02, -4.9674e-03,\n",
      "        -2.9365e-02, -6.8775e-03, -4.6098e-02,  9.4638e-02, -5.2232e-03,\n",
      "         3.5730e-02,  3.2103e-02, -7.8379e-02, -1.4931e-01, -2.0986e-03,\n",
      "         1.8803e-02,  9.4113e-03, -1.7686e-02,  8.8743e-02, -4.2485e-02,\n",
      "         5.0353e-02,  3.3062e-02,  3.5124e-02, -1.9439e-01, -1.4454e-02,\n",
      "         2.6651e-02,  7.4332e-02, -4.6857e-03,  4.9811e-02, -3.9126e-02,\n",
      "        -1.5253e-02, -1.1098e-01, -1.5721e-02,  1.9013e-03,  3.0853e-02,\n",
      "        -1.6143e-02,  6.4417e-02,  3.0575e-03, -9.3377e-02, -1.9028e-02,\n",
      "        -1.1737e-02,  1.1631e-02,  6.5732e-02, -2.7923e-02, -8.8499e-02,\n",
      "         2.9856e-02,  5.7477e-02, -1.3666e-01, -7.1888e-02, -4.7716e-02,\n",
      "         1.2169e-02, -1.4585e-01,  1.1506e-02,  1.6997e-02, -2.3914e-02,\n",
      "         3.3775e-02, -7.2303e-02, -4.1331e-02,  5.8641e-02,  1.8162e-02,\n",
      "        -1.0032e-01, -5.6720e-02, -1.1666e-01, -2.5097e-03, -1.1599e-01,\n",
      "        -7.1027e-02, -5.7559e-02, -5.6187e-02,  2.8095e-02, -7.4092e-02,\n",
      "        -1.3677e-01, -1.2289e-01, -6.1360e-02,  1.3054e-01, -4.2742e-02,\n",
      "        -6.7510e-02, -1.3753e-01, -8.0331e-03, -3.9962e-02, -1.3641e-03,\n",
      "        -8.6367e-02, -6.4759e-02, -6.5499e-02, -3.6868e-02, -8.2482e-02,\n",
      "        -3.3891e-02,  2.1246e-03, -1.1679e-01,  3.1650e-03,  1.2474e-01,\n",
      "        -1.9254e-02, -5.1184e-02, -5.5204e-02, -4.3774e-02, -1.0250e-01,\n",
      "        -8.0714e-02,  2.9246e-03,  2.5935e-02, -6.4056e-02, -1.0154e-01,\n",
      "        -1.7393e-02, -1.2362e-01, -7.7101e-02,  3.9978e-02, -3.7054e-02,\n",
      "        -1.1847e-01, -1.1833e-01,  2.5737e-02, -1.0936e-01, -8.8341e-02,\n",
      "        -1.0554e-01, -3.1177e-02, -5.1639e-02, -3.6349e-02, -2.4645e-02,\n",
      "        -2.8166e-02, -1.8011e-03, -1.4113e-02, -4.9252e-03, -9.7892e-02,\n",
      "        -2.3604e-02, -2.5926e-02, -9.7952e-02, -1.8406e-03, -8.1670e-02,\n",
      "        -9.6277e-02, -2.2997e-02, -6.8178e-02,  2.4370e-02, -5.3143e-02,\n",
      "        -9.0717e-02, -7.0742e-02, -1.0267e-02, -1.7552e-01,  4.5258e-02,\n",
      "        -7.9227e-02, -1.2580e-01, -1.7826e-02, -8.5590e-02, -1.0450e-01,\n",
      "        -1.2891e-01, -7.9875e-02,  5.5190e-02, -3.4760e-02, -6.2576e-02,\n",
      "        -4.8237e-02, -1.0377e-01, -6.3066e-02, -5.1792e-02, -6.9795e-02,\n",
      "         2.3920e-03, -8.4273e-02, -1.8970e-03, -1.9258e-01, -4.0399e-02,\n",
      "        -1.5045e-01, -1.0371e-01, -6.2508e-02, -2.0652e-02, -1.0346e-01,\n",
      "        -4.1533e-02, -8.1068e-02, -6.2532e-02, -3.6026e-02, -6.3810e-02,\n",
      "         6.0436e-02, -3.4755e-02, -2.0883e-01, -2.3570e-02, -1.9142e-01,\n",
      "        -8.6957e-02,  1.6435e-02], device='cuda:0', requires_grad=True))\n",
      "('encoder.rnn.rnns.0._module.bias_hh_l0', Parameter containing:\n",
      "tensor([-1.0775e-01, -1.3437e-01, -2.8567e-02, -2.5765e-02, -7.1188e-02,\n",
      "         5.7263e-03, -3.1217e-04, -1.5193e-02, -4.7889e-02, -5.4343e-02,\n",
      "        -7.0347e-02, -2.3630e-01, -2.0059e-01, -4.1305e-02, -1.8068e-01,\n",
      "        -1.7601e-01, -1.4062e-02, -1.0480e-01, -9.5575e-03, -1.8192e-01,\n",
      "         3.0971e-02, -8.0700e-02, -1.4840e-02, -1.2630e-01, -9.9954e-03,\n",
      "        -1.5080e-01, -6.1938e-02,  2.1855e-02, -1.8375e-03, -8.8703e-02,\n",
      "        -1.4511e-02, -1.0086e-01,  8.4702e-02, -3.6451e-02, -5.8224e-02,\n",
      "        -1.3944e-01, -1.7176e-01, -7.2940e-02, -2.4222e-02, -6.4831e-02,\n",
      "        -1.3597e-01, -1.0410e-01, -1.8614e-01, -7.7514e-02, -6.6750e-02,\n",
      "        -8.1674e-02, -5.5239e-02, -5.1553e-02, -1.5639e-02, -7.2652e-02,\n",
      "         1.2941e-02, -1.1565e-01, -3.4790e-02, -3.0606e-02,  2.4753e-02,\n",
      "        -1.4949e-01, -1.8712e-02, -1.9903e-02, -2.0673e-01, -1.8521e-01,\n",
      "        -2.1712e-01, -1.3878e-01, -1.1612e-01, -1.3541e-01, -1.5531e-01,\n",
      "        -1.6418e-01, -2.0989e-02, -1.4358e-01, -6.6187e-02, -4.7120e-02,\n",
      "        -3.2452e-01, -3.5899e-02, -9.8371e-03, -2.4718e-02, -2.8093e-01,\n",
      "        -1.3618e-01, -9.5027e-02, -1.6624e-02, -2.1895e-01, -2.5000e-01,\n",
      "        -1.0553e-02, -1.2722e-01, -3.8289e-02, -2.1011e-01, -4.4811e-03,\n",
      "        -8.1178e-02, -9.7910e-02, -9.8762e-02, -3.1868e-02,  1.8897e-02,\n",
      "        -4.2482e-02, -1.5014e-02, -2.2721e-02, -1.6228e-01, -1.2269e-01,\n",
      "        -2.8197e-02, -1.4773e-02,  1.5644e-01, -3.1498e-01, -6.4285e-02,\n",
      "        -2.4699e-02, -8.2603e-02, -2.5953e-02, -1.0880e-01, -1.1090e-01,\n",
      "        -7.7882e-02, -5.8024e-04,  1.9757e-01, -2.1236e-01, -1.0754e-01,\n",
      "         1.4255e-01, -3.8281e-02, -8.5444e-02, -8.9195e-02, -9.6181e-02,\n",
      "         1.1608e-01, -1.7409e-02, -2.6171e-01, -6.6498e-02, -1.2118e-02,\n",
      "        -9.7160e-02, -2.5115e-02, -4.8242e-02,  3.1723e-02, -1.1277e-01,\n",
      "        -1.5331e-01, -6.4787e-02, -7.7324e-02,  9.1564e-01,  9.0135e-01,\n",
      "         9.0143e-01,  1.0093e+00,  9.5110e-01,  9.6587e-01,  9.1397e-01,\n",
      "         9.5832e-01,  9.3933e-01,  9.4018e-01,  9.7263e-01,  1.2946e+00,\n",
      "         9.6975e-01,  9.3930e-01,  1.0793e+00,  1.2280e+00,  1.0637e+00,\n",
      "         9.7397e-01,  9.4709e-01,  1.0447e+00,  1.0612e+00,  9.7035e-01,\n",
      "         1.0686e+00,  8.9682e-01,  9.2830e-01,  1.1626e+00,  1.0724e+00,\n",
      "         1.0734e+00,  1.1700e+00,  1.0159e+00,  9.1972e-01,  9.6757e-01,\n",
      "         1.0576e+00,  9.8711e-01,  9.4896e-01,  9.6296e-01,  1.0947e+00,\n",
      "         8.6651e-01,  9.5719e-01,  9.3095e-01,  9.7674e-01,  9.3655e-01,\n",
      "         1.0462e+00,  1.0425e+00,  9.3801e-01,  8.8172e-01,  8.6071e-01,\n",
      "         9.4580e-01,  9.5951e-01,  1.2186e+00,  7.2980e-01,  1.1324e+00,\n",
      "         9.0466e-01,  9.3883e-01,  9.2426e-01,  1.0158e+00,  9.5406e-01,\n",
      "         9.8025e-01,  1.0083e+00,  1.0439e+00,  9.9105e-01,  9.5325e-01,\n",
      "         1.0285e+00,  9.6291e-01,  1.1691e+00,  8.7222e-01,  9.8543e-01,\n",
      "         9.6692e-01,  9.5351e-01,  8.9112e-01,  1.1720e+00,  9.0991e-01,\n",
      "         9.0641e-01,  8.5767e-01,  1.1417e+00,  1.1014e+00,  9.7207e-01,\n",
      "         9.2697e-01,  9.5077e-01,  1.0091e+00,  8.8312e-01,  9.4108e-01,\n",
      "         9.4475e-01,  1.1566e+00,  9.0670e-01,  9.1596e-01,  1.1024e+00,\n",
      "         9.4049e-01,  9.1262e-01,  9.6449e-01,  8.4872e-01,  8.7576e-01,\n",
      "         1.0881e+00,  9.9114e-01,  9.1124e-01,  9.1658e-01,  8.9974e-01,\n",
      "         1.1337e+00,  1.1378e+00,  9.2566e-01,  8.7374e-01,  8.7055e-01,\n",
      "         9.9601e-01,  9.7694e-01,  8.7561e-01,  9.1203e-01,  9.6869e-01,\n",
      "         1.0740e+00,  1.1366e+00,  8.7273e-01,  1.1423e+00,  8.8201e-01,\n",
      "         1.0209e+00,  9.2764e-01,  9.0219e-01,  9.9993e-01,  9.1010e-01,\n",
      "         1.1862e+00,  9.5237e-01,  9.6957e-01,  1.0489e+00,  9.3018e-01,\n",
      "         9.5261e-01,  9.7863e-01,  8.8699e-01,  8.8955e-01,  1.0826e+00,\n",
      "         8.6017e-01,  1.5099e-02,  8.3769e-03, -2.8260e-02, -6.8756e-02,\n",
      "        -2.2902e-02, -6.8285e-02,  4.0585e-02,  1.4890e-02,  1.8236e-02,\n",
      "        -3.5850e-02, -1.0553e-02,  2.6926e-02, -4.1611e-03,  3.8108e-02,\n",
      "        -8.7793e-03,  3.9050e-02, -4.7394e-02,  3.4433e-02, -9.5722e-02,\n",
      "         1.9071e-02, -7.5645e-02, -2.3139e-02, -5.9125e-02, -8.6750e-03,\n",
      "         6.5809e-02, -2.3581e-03, -4.0293e-02, -8.2602e-02, -9.1064e-02,\n",
      "        -4.8782e-02, -6.9114e-02,  3.7289e-03, -1.3681e-01,  1.2780e-03,\n",
      "         6.4875e-02, -5.6065e-02,  1.8560e-02,  1.8919e-02,  1.5327e-02,\n",
      "        -6.1724e-02,  2.2234e-02, -3.3969e-02, -3.1172e-02,  1.2079e-02,\n",
      "         1.7438e-03,  1.3804e-02, -4.4288e-02, -3.0676e-03, -8.0779e-02,\n",
      "        -5.7079e-02, -8.1635e-02,  1.1961e-02,  4.1408e-02, -5.3523e-02,\n",
      "        -1.0344e-02, -3.4019e-02,  6.2505e-03,  8.8661e-02,  6.8099e-03,\n",
      "        -1.7594e-02,  2.4513e-02,  1.0903e-02, -2.9489e-02,  5.7120e-02,\n",
      "        -1.2238e-02,  9.4850e-02,  1.0958e-01,  1.6864e-02, -1.5254e-03,\n",
      "        -4.3007e-02,  1.1885e-02,  3.5800e-02,  1.7350e-02, -1.5759e-02,\n",
      "         1.7403e-02, -9.3920e-03, -8.6207e-03,  2.8336e-02, -1.5866e-02,\n",
      "         1.3208e-02, -5.6595e-02,  2.7308e-02, -5.0754e-02,  1.9638e-02,\n",
      "         7.2320e-03,  3.3757e-03, -6.0146e-03,  1.5878e-02, -4.9674e-03,\n",
      "        -2.9365e-02, -6.8775e-03, -4.6098e-02,  9.4638e-02, -5.2232e-03,\n",
      "         3.5730e-02,  3.2103e-02, -7.8379e-02, -1.4931e-01, -2.0986e-03,\n",
      "         1.8803e-02,  9.4113e-03, -1.7686e-02,  8.8743e-02, -4.2485e-02,\n",
      "         5.0353e-02,  3.3062e-02,  3.5124e-02, -1.9439e-01, -1.4454e-02,\n",
      "         2.6651e-02,  7.4332e-02, -4.6857e-03,  4.9811e-02, -3.9126e-02,\n",
      "        -1.5253e-02, -1.1098e-01, -1.5721e-02,  1.9013e-03,  3.0853e-02,\n",
      "        -1.6143e-02,  6.4417e-02,  3.0575e-03, -9.3377e-02, -1.9028e-02,\n",
      "        -1.1737e-02,  1.1631e-02,  6.5732e-02, -2.7923e-02, -8.8499e-02,\n",
      "         2.9856e-02,  5.7477e-02, -1.3666e-01, -7.1888e-02, -4.7716e-02,\n",
      "         1.2169e-02, -1.4585e-01,  1.1506e-02,  1.6997e-02, -2.3914e-02,\n",
      "         3.3775e-02, -7.2303e-02, -4.1331e-02,  5.8641e-02,  1.8162e-02,\n",
      "        -1.0032e-01, -5.6720e-02, -1.1666e-01, -2.5097e-03, -1.1599e-01,\n",
      "        -7.1027e-02, -5.7559e-02, -5.6187e-02,  2.8095e-02, -7.4092e-02,\n",
      "        -1.3677e-01, -1.2289e-01, -6.1360e-02,  1.3054e-01, -4.2742e-02,\n",
      "        -6.7510e-02, -1.3753e-01, -8.0331e-03, -3.9962e-02, -1.3641e-03,\n",
      "        -8.6367e-02, -6.4759e-02, -6.5499e-02, -3.6868e-02, -8.2482e-02,\n",
      "        -3.3891e-02,  2.1246e-03, -1.1679e-01,  3.1650e-03,  1.2474e-01,\n",
      "        -1.9254e-02, -5.1184e-02, -5.5204e-02, -4.3774e-02, -1.0250e-01,\n",
      "        -8.0714e-02,  2.9246e-03,  2.5935e-02, -6.4056e-02, -1.0154e-01,\n",
      "        -1.7393e-02, -1.2362e-01, -7.7101e-02,  3.9978e-02, -3.7054e-02,\n",
      "        -1.1847e-01, -1.1833e-01,  2.5737e-02, -1.0936e-01, -8.8341e-02,\n",
      "        -1.0554e-01, -3.1177e-02, -5.1639e-02, -3.6349e-02, -2.4645e-02,\n",
      "        -2.8166e-02, -1.8011e-03, -1.4113e-02, -4.9252e-03, -9.7892e-02,\n",
      "        -2.3604e-02, -2.5926e-02, -9.7952e-02, -1.8406e-03, -8.1670e-02,\n",
      "        -9.6277e-02, -2.2997e-02, -6.8178e-02,  2.4370e-02, -5.3143e-02,\n",
      "        -9.0717e-02, -7.0742e-02, -1.0267e-02, -1.7552e-01,  4.5258e-02,\n",
      "        -7.9227e-02, -1.2580e-01, -1.7826e-02, -8.5590e-02, -1.0450e-01,\n",
      "        -1.2891e-01, -7.9875e-02,  5.5190e-02, -3.4760e-02, -6.2576e-02,\n",
      "        -4.8237e-02, -1.0377e-01, -6.3066e-02, -5.1792e-02, -6.9795e-02,\n",
      "         2.3920e-03, -8.4273e-02, -1.8970e-03, -1.9258e-01, -4.0399e-02,\n",
      "        -1.5045e-01, -1.0371e-01, -6.2508e-02, -2.0652e-02, -1.0346e-01,\n",
      "        -4.1533e-02, -8.1068e-02, -6.2532e-02, -3.6026e-02, -6.3810e-02,\n",
      "         6.0436e-02, -3.4755e-02, -2.0883e-01, -2.3570e-02, -1.9142e-01,\n",
      "        -8.6957e-02,  1.6435e-02], device='cuda:0', requires_grad=True))\n",
      "('encoder.rnn.rnns.0._module.weight_ih_l0_reverse', Parameter containing:\n",
      "tensor([[ 0.0853, -0.2642, -0.1696,  ..., -0.0153,  0.0282,  0.0950],\n",
      "        [ 0.3373, -0.2626,  0.0505,  ..., -0.0804, -0.1637,  0.1877],\n",
      "        [ 0.2711, -0.0545,  0.1385,  ..., -0.1070, -0.1286,  0.1024],\n",
      "        ...,\n",
      "        [ 0.0072,  0.1228, -0.0403,  ..., -0.0296, -0.0256,  0.0458],\n",
      "        [ 0.0330,  0.0926,  0.0529,  ..., -0.0719, -0.1765, -0.0798],\n",
      "        [ 0.0297,  0.2739,  0.0660,  ..., -0.0173, -0.1733, -0.1556]],\n",
      "       device='cuda:0', requires_grad=True))\n",
      "('encoder.rnn.rnns.0._module.weight_hh_l0_reverse', Parameter containing:\n",
      "tensor([[ 0.0939, -0.2475,  0.0237,  ..., -0.1359,  0.0211,  0.0722],\n",
      "        [ 0.1275,  0.0238,  0.0692,  ...,  0.0120, -0.1283, -0.0907],\n",
      "        [ 0.0094,  0.0237, -0.0229,  ...,  0.0813,  0.0371,  0.0006],\n",
      "        ...,\n",
      "        [ 0.0346,  0.1025,  0.0570,  ..., -0.0439, -0.0189, -0.0197],\n",
      "        [ 0.0224,  0.0190,  0.0018,  ...,  0.0305, -0.0902,  0.0371],\n",
      "        [-0.0718,  0.2371, -0.0426,  ..., -0.0581, -0.0114, -0.0247]],\n",
      "       device='cuda:0', requires_grad=True))\n",
      "('encoder.rnn.rnns.0._module.bias_ih_l0_reverse', Parameter containing:\n",
      "tensor([-1.3028e-01, -1.6404e-01, -5.6950e-02, -1.0869e-01, -2.2082e-02,\n",
      "        -1.6031e-01, -8.9388e-02, -7.3298e-02, -1.2090e-01, -1.8996e-01,\n",
      "        -5.0236e-02, -8.1361e-02, -1.6543e-02, -2.3656e-01, -1.4677e-01,\n",
      "         5.1226e-02, -7.8361e-02, -1.1917e-01, -2.5441e-01, -1.0708e-01,\n",
      "        -1.2182e-01, -3.7788e-02, -9.9441e-02, -1.4253e-01, -1.1939e-01,\n",
      "         1.0664e-02, -1.1275e-01, -8.7501e-02, -4.4054e-02, -1.0756e-01,\n",
      "        -1.1915e-01, -6.0038e-02, -1.5395e-01, -1.1764e-01, -1.0802e-01,\n",
      "        -1.8587e-01, -1.5843e-01, -1.3456e-01, -5.9070e-02, -7.1774e-02,\n",
      "        -1.5893e-01, -5.3130e-02, -1.8645e-01, -2.0214e-02, -1.4593e-01,\n",
      "        -1.1617e-01, -1.1535e-01, -1.1727e-01, -1.3620e-01, -1.3952e-01,\n",
      "        -1.8460e-02, -5.7073e-02, -2.0945e-01, -3.2803e-03, -1.0466e-01,\n",
      "        -5.3009e-02, -1.8032e-01, -1.1541e-01, -8.1539e-02, -1.1065e-01,\n",
      "        -1.3011e-01, -1.2008e-01, -1.2026e-01,  2.4505e-02, -2.0183e-01,\n",
      "        -9.8191e-02, -2.4222e-01, -8.3016e-02, -7.5582e-02, -6.0896e-02,\n",
      "        -7.3686e-02, -9.1753e-02, -8.3844e-02, -3.1377e-02, -1.0676e-01,\n",
      "        -1.0254e-01, -7.7500e-02, -4.9962e-02, -9.8186e-02, -5.6321e-02,\n",
      "        -7.1003e-02, -1.1115e-01, -6.7481e-02, -1.4786e-01, -1.5709e-01,\n",
      "        -7.6829e-02, -1.4763e-01, -9.7179e-02, -2.3381e-01, -3.3488e-03,\n",
      "        -5.7859e-02, -1.4649e-01, -1.2071e-01, -1.4537e-01, -2.3232e-01,\n",
      "        -5.2536e-02, -3.7092e-02, -6.6101e-02, -7.2639e-02, -1.2249e-01,\n",
      "        -4.3050e-02, -1.0981e-01, -5.1804e-02, -5.1236e-02, -9.1955e-02,\n",
      "        -1.4313e-01, -1.1575e-01, -7.7623e-02, -1.3224e-01, -1.2126e-01,\n",
      "         4.8737e-02, -9.1753e-02, -1.2907e-01, -7.2020e-02, -2.8678e-01,\n",
      "        -1.0969e-01, -7.3245e-02, -9.6228e-02, -6.9231e-02, -1.7823e-01,\n",
      "        -1.0272e-01, -1.0191e-01, -1.7671e-01, -7.1449e-02, -1.3001e-01,\n",
      "        -1.6756e-01, -1.5365e-01, -2.5555e-01,  1.0307e+00,  1.0097e+00,\n",
      "         9.7960e-01,  9.3437e-01,  9.9644e-01,  9.9735e-01,  9.2627e-01,\n",
      "         9.5446e-01,  9.0603e-01,  9.5524e-01,  8.6566e-01,  9.0445e-01,\n",
      "         9.1388e-01,  1.0156e+00,  9.8502e-01,  9.2971e-01,  1.0108e+00,\n",
      "         9.3561e-01,  1.1727e+00,  9.8044e-01,  9.5580e-01,  9.7707e-01,\n",
      "         9.4246e-01,  9.3341e-01,  9.8534e-01,  8.8791e-01,  9.6259e-01,\n",
      "         1.0003e+00,  9.6536e-01,  8.8664e-01,  9.4500e-01,  9.0937e-01,\n",
      "         1.0734e+00,  9.7960e-01,  9.2357e-01,  1.0822e+00,  9.8082e-01,\n",
      "         9.7615e-01,  9.8269e-01,  9.3300e-01,  1.0619e+00,  8.2430e-01,\n",
      "         1.0142e+00,  9.1345e-01,  9.9243e-01,  9.3895e-01,  9.2954e-01,\n",
      "         9.4406e-01,  9.8452e-01,  9.7645e-01,  1.1321e+00,  9.0149e-01,\n",
      "         9.7080e-01,  9.2900e-01,  1.0148e+00,  9.4871e-01,  1.0706e+00,\n",
      "         8.8983e-01,  9.0273e-01,  9.3823e-01,  9.7287e-01,  9.2474e-01,\n",
      "         1.0176e+00,  1.0770e+00,  8.9427e-01,  1.0330e+00,  1.1506e+00,\n",
      "         9.3894e-01,  8.6824e-01,  8.9765e-01,  1.1267e+00,  1.0316e+00,\n",
      "         9.2037e-01,  9.2834e-01,  9.6822e-01,  9.4348e-01,  9.4259e-01,\n",
      "         9.7745e-01,  8.8862e-01,  9.6086e-01,  9.5250e-01,  1.0394e+00,\n",
      "         9.6807e-01,  9.7362e-01,  9.1692e-01,  9.5339e-01,  1.0646e+00,\n",
      "         9.2926e-01,  1.2362e+00,  8.9090e-01,  9.4003e-01,  8.9816e-01,\n",
      "         9.3749e-01,  9.8976e-01,  9.4942e-01,  9.4500e-01,  1.0514e+00,\n",
      "         9.1954e-01,  9.5975e-01,  1.0341e+00,  9.0761e-01,  9.5349e-01,\n",
      "         9.4937e-01,  9.1209e-01,  9.7346e-01,  1.0735e+00,  9.8282e-01,\n",
      "         9.7773e-01,  8.9224e-01,  9.2077e-01,  1.0357e+00,  9.4738e-01,\n",
      "         9.0138e-01,  9.1119e-01,  9.6732e-01,  8.9769e-01,  9.4924e-01,\n",
      "         9.4429e-01,  9.5377e-01,  1.0086e+00,  9.2795e-01,  9.1398e-01,\n",
      "         9.8837e-01,  8.9559e-01,  9.9901e-01,  1.1355e+00,  1.0551e+00,\n",
      "         1.0502e+00,  2.5557e-02, -3.1866e-02,  2.9763e-02, -4.0597e-03,\n",
      "         6.5740e-03, -1.7482e-02,  6.1152e-02,  6.2009e-02,  1.8224e-02,\n",
      "         5.1042e-02, -7.8829e-03, -1.6065e-03,  5.8496e-02,  2.3771e-02,\n",
      "         7.6414e-02,  6.0277e-02, -1.9457e-02,  4.9397e-03,  1.5280e-02,\n",
      "        -3.7175e-02, -8.6488e-02,  1.2823e-03,  2.1069e-02, -1.1067e-02,\n",
      "        -2.4581e-02, -7.9506e-02, -9.9483e-03,  4.9619e-03, -1.1314e-01,\n",
      "         2.0110e-02, -9.4625e-03, -5.2735e-02, -1.7621e-02, -3.4546e-02,\n",
      "        -4.0121e-02, -2.7151e-02, -7.6504e-03,  4.2734e-03, -1.8634e-02,\n",
      "         4.6851e-02, -1.1887e-02, -2.8773e-02, -2.5681e-02, -6.2235e-02,\n",
      "        -1.4244e-02, -2.7810e-02,  2.1152e-02, -1.2581e-02, -4.5406e-02,\n",
      "         4.8112e-02, -3.9672e-02, -6.0065e-02,  4.2947e-02,  1.2937e-02,\n",
      "        -1.7068e-02,  4.8072e-02, -2.4341e-02, -2.3690e-02, -9.0318e-03,\n",
      "         1.7759e-02,  3.6102e-02,  2.7796e-02, -2.2810e-02,  1.0479e-01,\n",
      "         8.2776e-02,  4.7589e-02, -2.5331e-03,  3.6392e-02, -7.0061e-02,\n",
      "         2.5889e-02, -2.8233e-03, -2.3754e-02, -3.2170e-02,  8.7912e-04,\n",
      "        -5.5741e-02,  1.9169e-02,  1.7852e-02, -6.1280e-02,  2.2004e-02,\n",
      "         3.2185e-02, -2.5907e-03,  2.9626e-02,  1.4335e-02,  1.2065e-02,\n",
      "         2.6490e-02,  1.8484e-02, -2.3345e-02, -3.1525e-02,  1.5690e-02,\n",
      "         5.0509e-02,  3.2686e-02,  2.7005e-03, -8.3580e-02,  4.1314e-03,\n",
      "         3.5838e-02, -3.7084e-03,  3.5450e-02, -7.1217e-03, -2.3569e-02,\n",
      "         1.8196e-02, -3.2813e-03, -3.8223e-02,  2.5790e-02, -1.2062e-01,\n",
      "        -4.4047e-02,  6.9442e-02,  3.7536e-02,  1.2976e-02, -1.1883e-02,\n",
      "         1.3338e-02, -7.7000e-02, -6.9047e-02,  5.4427e-02,  4.3183e-02,\n",
      "        -3.3870e-02, -2.0889e-02, -7.5473e-02, -1.7393e-02, -3.8119e-02,\n",
      "         2.2810e-02,  6.9389e-02,  3.5893e-02, -3.5938e-02,  5.5486e-03,\n",
      "        -4.6391e-03, -2.9358e-02,  1.1712e-02,  1.8585e-02, -1.3219e-01,\n",
      "        -6.5523e-02, -4.6751e-02, -3.3938e-02, -1.3547e-01, -3.4730e-02,\n",
      "        -4.6434e-02, -4.6042e-02, -4.0815e-02, -1.1214e-01, -2.2622e-02,\n",
      "         3.5646e-02, -1.1394e-02, -2.4341e-02, -5.0188e-02,  1.3036e-02,\n",
      "        -1.0952e-03, -6.0577e-02, -6.3912e-02, -3.8344e-02, -4.3805e-02,\n",
      "        -7.8589e-02, -5.1659e-02, -4.3497e-02, -1.2581e-01,  1.3100e-02,\n",
      "        -4.9897e-02, -4.7072e-02, -3.4516e-02, -7.0376e-02, -9.0956e-02,\n",
      "         9.4300e-03, -5.6398e-02, -4.2262e-02, -9.1958e-02, -3.2661e-02,\n",
      "        -9.6438e-02, -4.7134e-02, -2.5675e-02,  2.0910e-02, -6.7422e-02,\n",
      "        -1.6923e-02, -2.6818e-02,  1.2182e-03,  1.4488e-02, -3.0451e-02,\n",
      "        -7.1607e-02, -4.9744e-02,  3.8017e-03, -4.7116e-03, -6.1419e-02,\n",
      "        -2.5526e-02, -7.5104e-02, -9.2306e-03, -1.4832e-01, -6.1647e-02,\n",
      "         1.2744e-02,  4.2097e-02, -6.5980e-03, -2.7495e-02, -4.9473e-04,\n",
      "        -1.0641e-01, -5.1827e-02, -7.9768e-02, -3.9207e-02, -1.0940e-01,\n",
      "        -6.3215e-02, -1.6789e-02, -4.5338e-02, -2.5494e-02, -7.0577e-02,\n",
      "        -5.0708e-02, -5.9634e-02,  8.5346e-03, -4.5691e-02, -9.1357e-02,\n",
      "        -5.5483e-02, -3.2910e-02, -4.3004e-02, -3.8516e-02, -4.4911e-02,\n",
      "        -6.3647e-02, -4.6708e-02, -1.5311e-01, -5.6764e-02,  1.1581e-03,\n",
      "        -1.0773e-01, -3.8051e-02, -4.2264e-03,  3.1141e-02, -1.0627e-02,\n",
      "        -8.0646e-02,  6.2712e-03, -6.2201e-02, -4.7627e-02,  6.2752e-03,\n",
      "        -5.7101e-02, -2.8487e-02, -2.2245e-02, -6.5351e-02, -9.0739e-03,\n",
      "        -4.9154e-02,  1.2206e-02,  2.1756e-02, -8.2818e-02, -3.0882e-02,\n",
      "        -4.6510e-02, -8.8099e-02, -1.1316e-01, -7.8393e-02, -5.8894e-02,\n",
      "        -4.3134e-02, -1.3824e-01, -9.5935e-02, -2.7176e-02, -6.7410e-02,\n",
      "        -8.7178e-02, -5.4250e-02, -3.8502e-02, -5.6318e-02, -2.2855e-02,\n",
      "        -3.7587e-02,  3.7075e-03, -4.7792e-02, -6.2914e-02, -3.0066e-02,\n",
      "        -6.4896e-02, -3.4268e-02], device='cuda:0', requires_grad=True))\n",
      "('encoder.rnn.rnns.0._module.bias_hh_l0_reverse', Parameter containing:\n",
      "tensor([-1.3028e-01, -1.6404e-01, -5.6950e-02, -1.0869e-01, -2.2082e-02,\n",
      "        -1.6031e-01, -8.9388e-02, -7.3298e-02, -1.2090e-01, -1.8996e-01,\n",
      "        -5.0236e-02, -8.1361e-02, -1.6543e-02, -2.3656e-01, -1.4677e-01,\n",
      "         5.1226e-02, -7.8361e-02, -1.1917e-01, -2.5441e-01, -1.0708e-01,\n",
      "        -1.2182e-01, -3.7788e-02, -9.9441e-02, -1.4253e-01, -1.1939e-01,\n",
      "         1.0664e-02, -1.1275e-01, -8.7501e-02, -4.4054e-02, -1.0756e-01,\n",
      "        -1.1915e-01, -6.0038e-02, -1.5395e-01, -1.1764e-01, -1.0802e-01,\n",
      "        -1.8587e-01, -1.5843e-01, -1.3456e-01, -5.9070e-02, -7.1774e-02,\n",
      "        -1.5893e-01, -5.3130e-02, -1.8645e-01, -2.0214e-02, -1.4593e-01,\n",
      "        -1.1617e-01, -1.1535e-01, -1.1727e-01, -1.3620e-01, -1.3952e-01,\n",
      "        -1.8460e-02, -5.7073e-02, -2.0945e-01, -3.2803e-03, -1.0466e-01,\n",
      "        -5.3009e-02, -1.8032e-01, -1.1541e-01, -8.1539e-02, -1.1065e-01,\n",
      "        -1.3011e-01, -1.2008e-01, -1.2026e-01,  2.4505e-02, -2.0183e-01,\n",
      "        -9.8191e-02, -2.4222e-01, -8.3016e-02, -7.5582e-02, -6.0896e-02,\n",
      "        -7.3686e-02, -9.1753e-02, -8.3844e-02, -3.1377e-02, -1.0676e-01,\n",
      "        -1.0254e-01, -7.7500e-02, -4.9962e-02, -9.8186e-02, -5.6321e-02,\n",
      "        -7.1003e-02, -1.1115e-01, -6.7481e-02, -1.4786e-01, -1.5709e-01,\n",
      "        -7.6829e-02, -1.4763e-01, -9.7179e-02, -2.3381e-01, -3.3488e-03,\n",
      "        -5.7859e-02, -1.4649e-01, -1.2071e-01, -1.4537e-01, -2.3232e-01,\n",
      "        -5.2536e-02, -3.7092e-02, -6.6101e-02, -7.2639e-02, -1.2249e-01,\n",
      "        -4.3050e-02, -1.0981e-01, -5.1804e-02, -5.1236e-02, -9.1955e-02,\n",
      "        -1.4313e-01, -1.1575e-01, -7.7623e-02, -1.3224e-01, -1.2126e-01,\n",
      "         4.8737e-02, -9.1753e-02, -1.2907e-01, -7.2020e-02, -2.8678e-01,\n",
      "        -1.0969e-01, -7.3245e-02, -9.6228e-02, -6.9231e-02, -1.7823e-01,\n",
      "        -1.0272e-01, -1.0191e-01, -1.7671e-01, -7.1449e-02, -1.3001e-01,\n",
      "        -1.6756e-01, -1.5365e-01, -2.5555e-01,  1.0307e+00,  1.0097e+00,\n",
      "         9.7960e-01,  9.3437e-01,  9.9644e-01,  9.9735e-01,  9.2627e-01,\n",
      "         9.5446e-01,  9.0603e-01,  9.5524e-01,  8.6566e-01,  9.0445e-01,\n",
      "         9.1388e-01,  1.0156e+00,  9.8502e-01,  9.2971e-01,  1.0108e+00,\n",
      "         9.3561e-01,  1.1727e+00,  9.8044e-01,  9.5580e-01,  9.7707e-01,\n",
      "         9.4246e-01,  9.3341e-01,  9.8534e-01,  8.8791e-01,  9.6259e-01,\n",
      "         1.0003e+00,  9.6536e-01,  8.8664e-01,  9.4500e-01,  9.0937e-01,\n",
      "         1.0734e+00,  9.7960e-01,  9.2357e-01,  1.0822e+00,  9.8082e-01,\n",
      "         9.7615e-01,  9.8269e-01,  9.3300e-01,  1.0619e+00,  8.2430e-01,\n",
      "         1.0142e+00,  9.1345e-01,  9.9243e-01,  9.3895e-01,  9.2954e-01,\n",
      "         9.4406e-01,  9.8452e-01,  9.7645e-01,  1.1321e+00,  9.0149e-01,\n",
      "         9.7080e-01,  9.2900e-01,  1.0148e+00,  9.4871e-01,  1.0706e+00,\n",
      "         8.8983e-01,  9.0273e-01,  9.3823e-01,  9.7287e-01,  9.2474e-01,\n",
      "         1.0176e+00,  1.0770e+00,  8.9427e-01,  1.0330e+00,  1.1506e+00,\n",
      "         9.3894e-01,  8.6824e-01,  8.9765e-01,  1.1267e+00,  1.0316e+00,\n",
      "         9.2037e-01,  9.2834e-01,  9.6822e-01,  9.4348e-01,  9.4259e-01,\n",
      "         9.7745e-01,  8.8862e-01,  9.6086e-01,  9.5250e-01,  1.0394e+00,\n",
      "         9.6807e-01,  9.7362e-01,  9.1692e-01,  9.5339e-01,  1.0646e+00,\n",
      "         9.2926e-01,  1.2362e+00,  8.9090e-01,  9.4003e-01,  8.9816e-01,\n",
      "         9.3749e-01,  9.8976e-01,  9.4942e-01,  9.4500e-01,  1.0514e+00,\n",
      "         9.1954e-01,  9.5975e-01,  1.0341e+00,  9.0761e-01,  9.5349e-01,\n",
      "         9.4937e-01,  9.1209e-01,  9.7346e-01,  1.0735e+00,  9.8282e-01,\n",
      "         9.7773e-01,  8.9224e-01,  9.2077e-01,  1.0357e+00,  9.4738e-01,\n",
      "         9.0138e-01,  9.1119e-01,  9.6732e-01,  8.9769e-01,  9.4924e-01,\n",
      "         9.4429e-01,  9.5377e-01,  1.0086e+00,  9.2795e-01,  9.1398e-01,\n",
      "         9.8837e-01,  8.9559e-01,  9.9901e-01,  1.1355e+00,  1.0551e+00,\n",
      "         1.0502e+00,  2.5557e-02, -3.1866e-02,  2.9763e-02, -4.0597e-03,\n",
      "         6.5740e-03, -1.7482e-02,  6.1152e-02,  6.2009e-02,  1.8224e-02,\n",
      "         5.1042e-02, -7.8829e-03, -1.6065e-03,  5.8496e-02,  2.3771e-02,\n",
      "         7.6414e-02,  6.0277e-02, -1.9457e-02,  4.9397e-03,  1.5280e-02,\n",
      "        -3.7175e-02, -8.6488e-02,  1.2823e-03,  2.1069e-02, -1.1067e-02,\n",
      "        -2.4581e-02, -7.9506e-02, -9.9483e-03,  4.9619e-03, -1.1314e-01,\n",
      "         2.0110e-02, -9.4625e-03, -5.2735e-02, -1.7621e-02, -3.4546e-02,\n",
      "        -4.0121e-02, -2.7151e-02, -7.6504e-03,  4.2734e-03, -1.8634e-02,\n",
      "         4.6851e-02, -1.1887e-02, -2.8773e-02, -2.5681e-02, -6.2235e-02,\n",
      "        -1.4244e-02, -2.7810e-02,  2.1152e-02, -1.2581e-02, -4.5406e-02,\n",
      "         4.8112e-02, -3.9672e-02, -6.0065e-02,  4.2947e-02,  1.2937e-02,\n",
      "        -1.7068e-02,  4.8072e-02, -2.4341e-02, -2.3690e-02, -9.0318e-03,\n",
      "         1.7759e-02,  3.6102e-02,  2.7796e-02, -2.2810e-02,  1.0479e-01,\n",
      "         8.2776e-02,  4.7589e-02, -2.5331e-03,  3.6392e-02, -7.0061e-02,\n",
      "         2.5889e-02, -2.8233e-03, -2.3754e-02, -3.2170e-02,  8.7912e-04,\n",
      "        -5.5741e-02,  1.9169e-02,  1.7852e-02, -6.1280e-02,  2.2004e-02,\n",
      "         3.2185e-02, -2.5907e-03,  2.9626e-02,  1.4335e-02,  1.2065e-02,\n",
      "         2.6490e-02,  1.8484e-02, -2.3345e-02, -3.1525e-02,  1.5690e-02,\n",
      "         5.0509e-02,  3.2686e-02,  2.7005e-03, -8.3580e-02,  4.1314e-03,\n",
      "         3.5838e-02, -3.7084e-03,  3.5450e-02, -7.1217e-03, -2.3569e-02,\n",
      "         1.8196e-02, -3.2813e-03, -3.8223e-02,  2.5790e-02, -1.2062e-01,\n",
      "        -4.4047e-02,  6.9442e-02,  3.7536e-02,  1.2976e-02, -1.1883e-02,\n",
      "         1.3338e-02, -7.7000e-02, -6.9047e-02,  5.4427e-02,  4.3183e-02,\n",
      "        -3.3870e-02, -2.0889e-02, -7.5473e-02, -1.7393e-02, -3.8119e-02,\n",
      "         2.2810e-02,  6.9389e-02,  3.5893e-02, -3.5938e-02,  5.5486e-03,\n",
      "        -4.6391e-03, -2.9358e-02,  1.1712e-02,  1.8585e-02, -1.3219e-01,\n",
      "        -6.5523e-02, -4.6751e-02, -3.3938e-02, -1.3547e-01, -3.4730e-02,\n",
      "        -4.6434e-02, -4.6042e-02, -4.0815e-02, -1.1214e-01, -2.2622e-02,\n",
      "         3.5646e-02, -1.1394e-02, -2.4341e-02, -5.0188e-02,  1.3036e-02,\n",
      "        -1.0952e-03, -6.0577e-02, -6.3912e-02, -3.8344e-02, -4.3805e-02,\n",
      "        -7.8589e-02, -5.1659e-02, -4.3497e-02, -1.2581e-01,  1.3100e-02,\n",
      "        -4.9897e-02, -4.7072e-02, -3.4516e-02, -7.0376e-02, -9.0956e-02,\n",
      "         9.4300e-03, -5.6398e-02, -4.2262e-02, -9.1958e-02, -3.2661e-02,\n",
      "        -9.6438e-02, -4.7134e-02, -2.5675e-02,  2.0910e-02, -6.7422e-02,\n",
      "        -1.6923e-02, -2.6818e-02,  1.2182e-03,  1.4488e-02, -3.0451e-02,\n",
      "        -7.1607e-02, -4.9744e-02,  3.8017e-03, -4.7116e-03, -6.1419e-02,\n",
      "        -2.5526e-02, -7.5104e-02, -9.2306e-03, -1.4832e-01, -6.1647e-02,\n",
      "         1.2744e-02,  4.2097e-02, -6.5980e-03, -2.7495e-02, -4.9473e-04,\n",
      "        -1.0641e-01, -5.1827e-02, -7.9768e-02, -3.9207e-02, -1.0940e-01,\n",
      "        -6.3215e-02, -1.6789e-02, -4.5338e-02, -2.5494e-02, -7.0577e-02,\n",
      "        -5.0708e-02, -5.9634e-02,  8.5346e-03, -4.5691e-02, -9.1357e-02,\n",
      "        -5.5483e-02, -3.2910e-02, -4.3004e-02, -3.8516e-02, -4.4911e-02,\n",
      "        -6.3647e-02, -4.6708e-02, -1.5311e-01, -5.6764e-02,  1.1581e-03,\n",
      "        -1.0773e-01, -3.8051e-02, -4.2264e-03,  3.1141e-02, -1.0627e-02,\n",
      "        -8.0646e-02,  6.2712e-03, -6.2201e-02, -4.7627e-02,  6.2752e-03,\n",
      "        -5.7101e-02, -2.8487e-02, -2.2245e-02, -6.5351e-02, -9.0739e-03,\n",
      "        -4.9154e-02,  1.2206e-02,  2.1756e-02, -8.2818e-02, -3.0882e-02,\n",
      "        -4.6510e-02, -8.8099e-02, -1.1316e-01, -7.8393e-02, -5.8894e-02,\n",
      "        -4.3134e-02, -1.3824e-01, -9.5935e-02, -2.7176e-02, -6.7410e-02,\n",
      "        -8.7178e-02, -5.4250e-02, -3.8502e-02, -5.6318e-02, -2.2855e-02,\n",
      "        -3.7587e-02,  3.7075e-03, -4.7792e-02, -6.2914e-02, -3.0066e-02,\n",
      "        -6.4896e-02, -3.4268e-02], device='cuda:0', requires_grad=True))\n",
      "('encoder.rnn.rnns.0._module.weight_ih_l1', Parameter containing:\n",
      "tensor([[ 0.0435, -0.2326,  0.1110,  ...,  0.0590,  0.0029, -0.0675],\n",
      "        [-0.0225, -0.1873,  0.0393,  ..., -0.3120,  0.1069,  0.2008],\n",
      "        [ 0.0332, -0.1329, -0.0140,  ..., -0.0608,  0.0821,  0.1568],\n",
      "        ...,\n",
      "        [ 0.0320,  0.2789,  0.1542,  ..., -0.1340,  0.0797,  0.2200],\n",
      "        [ 0.0534, -0.0412, -0.1349,  ...,  0.0007, -0.1133, -0.1471],\n",
      "        [-0.0987,  0.0600,  0.1789,  ...,  0.1297, -0.0343,  0.0327]],\n",
      "       device='cuda:0', requires_grad=True))\n",
      "('encoder.rnn.rnns.0._module.weight_hh_l1', Parameter containing:\n",
      "tensor([[-0.0212, -0.1322,  0.0278,  ..., -0.0710, -0.0695, -0.1426],\n",
      "        [ 0.0184,  0.2130,  0.1293,  ..., -0.0598, -0.1296,  0.1553],\n",
      "        [ 0.0398, -0.1136,  0.0071,  ..., -0.0255,  0.0356, -0.0594],\n",
      "        ...,\n",
      "        [ 0.1866, -0.0440,  0.0405,  ..., -0.1219, -0.0266,  0.0488],\n",
      "        [ 0.0079,  0.0484,  0.0795,  ...,  0.1687, -0.0606, -0.0371],\n",
      "        [ 0.0974,  0.1171,  0.1304,  ...,  0.0304, -0.0236, -0.1198]],\n",
      "       device='cuda:0', requires_grad=True))\n",
      "('encoder.rnn.rnns.0._module.bias_ih_l1', Parameter containing:\n",
      "tensor([-1.7583e-01, -6.8582e-02, -3.7923e-02, -1.3595e-02, -1.2735e-01,\n",
      "         9.2553e-02, -1.0962e-01, -1.8729e-01, -6.5077e-02, -1.7870e-01,\n",
      "        -1.4241e-01, -1.3126e-01, -2.5722e-01, -1.7516e-01, -2.4214e-01,\n",
      "        -1.5737e-01, -8.3902e-02, -1.5028e-01, -1.5500e-01, -1.4833e-01,\n",
      "        -1.5268e-01, -1.7461e-01, -7.2257e-02, -2.9383e-02, -5.4798e-02,\n",
      "        -1.8099e-01, -1.0361e-01, -7.0091e-02, -1.5849e-01, -8.7095e-02,\n",
      "        -2.3277e-01, -1.7453e-01, -1.3099e-01, -8.7404e-02, -2.2757e-02,\n",
      "        -1.2018e-01,  9.3466e-03, -9.9904e-02, -1.5128e-01, -6.9594e-02,\n",
      "        -1.1932e-01, -5.3909e-02, -1.3046e-01, -1.1739e-01, -8.5062e-03,\n",
      "        -1.8433e-01, -2.1285e-01, -1.3040e-01, -1.4306e-01, -1.0710e-01,\n",
      "        -1.3997e-01, -1.4927e-01, -1.1089e-01,  8.1709e-03, -9.3736e-02,\n",
      "        -5.3084e-02, -3.4243e-02, -6.0463e-02,  1.6574e-02, -1.9907e-01,\n",
      "        -2.0515e-01, -1.9256e-01, -4.4962e-02, -4.8626e-02, -1.7139e-01,\n",
      "         2.1489e-02, -1.2279e-01, -6.8664e-02, -1.2007e-01, -6.2422e-02,\n",
      "        -6.9512e-02, -7.1663e-02, -1.3227e-01, -1.4172e-01, -7.1662e-02,\n",
      "        -1.1108e-01, -7.9166e-02, -1.9248e-01, -1.9825e-01, -1.3689e-01,\n",
      "        -1.2874e-01, -1.2638e-01, -1.4880e-01, -1.4937e-01, -6.1662e-02,\n",
      "        -4.6141e-02, -1.0807e-01,  8.5392e-02, -1.4873e-01, -7.8552e-02,\n",
      "        -2.1504e-01, -1.3059e-01, -1.4240e-01, -8.8054e-02, -2.1200e-01,\n",
      "        -6.6706e-02, -6.6601e-02, -1.1683e-01,  2.1626e-02, -1.5902e-01,\n",
      "        -1.9307e-01, -1.1410e-01, -1.1164e-01, -1.4930e-01,  3.2233e-02,\n",
      "        -7.7330e-02, -1.0806e-01, -6.6624e-02, -1.3467e-01, -1.5811e-01,\n",
      "         7.0173e-02, -9.7803e-02, -1.9594e-01, -2.0050e-01, -3.6580e-02,\n",
      "        -5.1659e-02, -5.3162e-02, -8.0219e-02, -1.0635e-01, -1.6347e-01,\n",
      "        -1.2443e-01, -1.6899e-02, -1.4331e-01, -2.2777e-01, -8.4425e-02,\n",
      "        -8.6491e-02, -8.6737e-02, -2.0568e-01,  1.0568e+00,  8.8343e-01,\n",
      "         9.9685e-01,  9.6140e-01,  7.9164e-01,  1.1864e+00,  8.7235e-01,\n",
      "         1.0311e+00,  9.3527e-01,  8.9694e-01,  8.5597e-01,  7.6645e-01,\n",
      "         1.1689e+00,  9.0855e-01,  7.5651e-01,  9.4689e-01,  8.8346e-01,\n",
      "         8.4990e-01,  7.8836e-01,  8.4780e-01,  1.4431e+00,  9.5758e-01,\n",
      "         1.0235e+00,  1.1555e+00,  8.4475e-01,  1.0310e+00,  8.3800e-01,\n",
      "         8.3623e-01,  7.9453e-01,  8.8240e-01,  1.1409e+00,  7.3724e-01,\n",
      "         8.4530e-01,  9.2300e-01,  8.9269e-01,  7.6521e-01,  9.4142e-01,\n",
      "         8.9931e-01,  9.3951e-01,  9.8989e-01,  8.3717e-01,  7.7847e-01,\n",
      "         8.4319e-01,  8.6754e-01,  6.7788e-01,  8.7619e-01,  7.0170e-01,\n",
      "         9.2290e-01,  1.1869e+00,  8.1995e-01,  8.8086e-01,  8.6066e-01,\n",
      "         7.9597e-01,  7.6375e-01,  8.2710e-01,  1.0164e+00,  9.7926e-01,\n",
      "         7.5281e-01,  1.0121e+00,  8.2645e-01,  1.1220e+00,  1.2544e+00,\n",
      "         8.4652e-01,  8.6882e-01,  8.3466e-01,  1.2434e+00,  1.1629e+00,\n",
      "         8.1040e-01,  8.9449e-01,  8.9771e-01,  9.4913e-01,  1.0786e+00,\n",
      "         8.8804e-01,  8.5271e-01,  8.7583e-01,  8.4775e-01,  7.8830e-01,\n",
      "         9.6931e-01,  8.2772e-01,  8.2659e-01,  8.9113e-01,  8.9190e-01,\n",
      "         7.8690e-01,  1.0883e+00,  8.8515e-01,  9.7608e-01,  1.0434e+00,\n",
      "         8.8723e-01,  8.5873e-01,  8.1670e-01,  8.4395e-01,  8.2940e-01,\n",
      "         9.2736e-01,  7.6380e-01,  8.5197e-01,  1.0683e+00,  1.1185e+00,\n",
      "         1.0268e+00,  9.9702e-01,  8.9841e-01,  1.1415e+00,  8.2640e-01,\n",
      "         9.3243e-01,  1.0575e+00,  1.1338e+00,  1.0240e+00,  7.6704e-01,\n",
      "         8.0891e-01,  8.7091e-01,  7.6302e-01,  1.0694e+00,  1.0823e+00,\n",
      "         1.2594e+00,  7.8461e-01,  9.2382e-01,  8.9983e-01,  9.2007e-01,\n",
      "         8.7199e-01,  8.2303e-01,  1.1656e+00,  9.8461e-01,  9.2462e-01,\n",
      "         6.8956e-01,  9.9380e-01,  8.8297e-01,  5.9773e-01,  8.2704e-01,\n",
      "         1.1397e+00, -8.1031e-03,  1.0498e-02, -4.8113e-02,  1.2993e-02,\n",
      "        -4.3104e-03,  5.9474e-02,  4.7312e-02, -2.9205e-02,  2.4501e-04,\n",
      "        -5.9289e-02,  3.3117e-02,  7.5198e-02, -4.9252e-02, -7.2948e-02,\n",
      "         7.9771e-02,  4.0533e-02,  4.0295e-02, -1.1559e-02,  2.4674e-02,\n",
      "        -7.4330e-02,  1.9396e-02, -1.7416e-02, -2.3931e-02, -7.1206e-02,\n",
      "        -5.9275e-02, -2.8195e-02,  3.0626e-02, -1.9108e-02,  2.7153e-02,\n",
      "         5.1569e-02,  3.2368e-02, -3.7625e-02,  7.4174e-03, -3.5738e-02,\n",
      "         7.4419e-03, -5.4605e-02, -4.5684e-02, -6.0131e-02, -2.2289e-02,\n",
      "         1.1110e-02, -4.4015e-02,  5.4796e-02, -1.3701e-02, -4.4356e-02,\n",
      "         6.6029e-02, -2.4792e-02,  5.0498e-02, -6.5300e-02,  2.6450e-02,\n",
      "        -1.4033e-02,  9.8482e-03,  1.5373e-01, -7.2197e-02,  3.8374e-03,\n",
      "        -1.7942e-02,  6.7489e-03, -5.3188e-02, -9.0637e-02,  7.2591e-02,\n",
      "        -3.7758e-02,  7.0571e-02, -5.6007e-03, -5.0731e-02, -6.6845e-02,\n",
      "        -5.3944e-03, -1.0487e-01, -2.1003e-02, -1.4567e-02, -2.8627e-02,\n",
      "        -4.3882e-02,  4.2177e-02,  4.8148e-02, -3.1481e-02, -4.3597e-02,\n",
      "        -4.6965e-02, -1.1312e-02, -2.8724e-03,  4.2330e-02,  5.9213e-03,\n",
      "         3.7778e-03,  1.0654e-02, -6.9038e-03,  5.9464e-03,  1.2541e-02,\n",
      "         4.1399e-02, -5.3773e-02, -3.6634e-02, -2.0141e-01,  2.4083e-02,\n",
      "         4.7501e-02, -5.2992e-02,  6.1490e-02, -6.0472e-02, -3.5847e-02,\n",
      "        -7.0360e-03, -2.2799e-02, -4.9112e-02, -8.5505e-02, -9.6661e-02,\n",
      "        -1.0032e-03, -5.4166e-02, -9.9392e-03, -6.4473e-04, -8.4395e-02,\n",
      "        -8.9196e-02, -1.5464e-02, -3.3263e-02,  7.5671e-02, -1.8876e-02,\n",
      "        -3.9325e-02, -9.0957e-02, -4.6718e-02,  3.2860e-02, -9.0401e-02,\n",
      "        -6.9605e-02, -1.5480e-01,  6.6328e-03,  3.9926e-02, -4.7689e-02,\n",
      "         1.9755e-02, -6.9249e-02, -7.9252e-02, -8.0011e-02, -2.1109e-02,\n",
      "         2.6005e-02, -2.1671e-02,  1.6851e-02,  3.3341e-02, -2.1436e-01,\n",
      "        -1.6596e-01, -1.8772e-01, -1.2339e-01, -9.9764e-02, -2.5291e-01,\n",
      "        -1.2566e-01, -1.4991e-01, -1.3874e-01, -1.9761e-01, -1.4909e-01,\n",
      "        -2.5209e-01, -1.4733e-01, -1.4335e-01, -1.4435e-01, -1.6826e-01,\n",
      "        -1.3660e-01, -1.2887e-01, -1.5307e-01, -9.3868e-02, -8.1865e-02,\n",
      "        -2.2594e-01, -1.4885e-01, -2.3755e-01, -1.5438e-01, -1.1025e-01,\n",
      "        -1.1531e-01, -1.6621e-01, -1.2361e-01, -1.3520e-01, -2.0825e-01,\n",
      "        -5.5708e-02, -1.0885e-01, -1.4230e-01, -1.6398e-01, -1.6895e-01,\n",
      "        -1.6647e-01, -9.1760e-02, -2.1679e-01, -1.9829e-01, -1.4891e-01,\n",
      "        -1.4373e-01, -1.1366e-01, -1.3239e-01, -9.2968e-02, -1.4905e-01,\n",
      "        -1.4850e-01, -1.2997e-01, -2.1168e-01, -2.2262e-02, -1.6795e-01,\n",
      "        -2.3811e-01, -6.6834e-02, -1.5299e-01, -1.7369e-01, -1.5439e-01,\n",
      "        -1.7558e-01, -1.3412e-01, -1.9359e-01, -2.8575e-01, -2.3229e-01,\n",
      "        -9.2826e-02, -2.1657e-01, -1.3913e-02, -1.5620e-01, -1.5448e-01,\n",
      "        -3.1109e-01, -1.4938e-03, -9.7904e-02, -1.4987e-01, -1.7923e-01,\n",
      "        -2.0112e-01, -1.3815e-01, -1.3467e-01, -1.4499e-01, -1.5252e-01,\n",
      "        -8.3864e-02, -1.6200e-01, -1.6609e-01, -1.4611e-01, -1.9590e-01,\n",
      "        -2.0703e-01, -1.1631e-01, -1.4295e-01, -1.3787e-01, -1.3853e-01,\n",
      "        -2.6773e-01, -1.4597e-01, -1.3200e-01, -8.6192e-02, -1.8442e-01,\n",
      "        -2.9547e-02, -1.6512e-01, -1.1436e-01, -1.2859e-01, -2.9404e-01,\n",
      "        -2.7824e-01, -2.4153e-01, -2.4915e-01, -1.8609e-01, -2.1040e-01,\n",
      "        -1.8534e-01, -1.4536e-01, -2.3879e-01, -2.1147e-01, -1.6484e-01,\n",
      "        -1.1914e-02, -1.5494e-01, -1.3169e-01, -1.1108e-01, -2.1760e-01,\n",
      "        -1.6723e-01, -9.3111e-02, -1.9372e-01, -1.9242e-01, -1.8493e-01,\n",
      "        -2.0593e-01, -1.1009e-01, -1.5700e-01, -2.0265e-01, -1.7678e-01,\n",
      "        -1.8699e-01, -2.0853e-02, -1.5835e-01, -1.4441e-01, -1.9193e-02,\n",
      "        -1.5230e-01, -1.7933e-01], device='cuda:0', requires_grad=True))\n",
      "('encoder.rnn.rnns.0._module.bias_hh_l1', Parameter containing:\n",
      "tensor([-1.7583e-01, -6.8582e-02, -3.7923e-02, -1.3595e-02, -1.2735e-01,\n",
      "         9.2553e-02, -1.0962e-01, -1.8729e-01, -6.5077e-02, -1.7870e-01,\n",
      "        -1.4241e-01, -1.3126e-01, -2.5722e-01, -1.7516e-01, -2.4214e-01,\n",
      "        -1.5737e-01, -8.3902e-02, -1.5028e-01, -1.5500e-01, -1.4833e-01,\n",
      "        -1.5268e-01, -1.7461e-01, -7.2257e-02, -2.9383e-02, -5.4798e-02,\n",
      "        -1.8099e-01, -1.0361e-01, -7.0091e-02, -1.5849e-01, -8.7095e-02,\n",
      "        -2.3277e-01, -1.7453e-01, -1.3099e-01, -8.7404e-02, -2.2757e-02,\n",
      "        -1.2018e-01,  9.3466e-03, -9.9904e-02, -1.5128e-01, -6.9594e-02,\n",
      "        -1.1932e-01, -5.3909e-02, -1.3046e-01, -1.1739e-01, -8.5062e-03,\n",
      "        -1.8433e-01, -2.1285e-01, -1.3040e-01, -1.4306e-01, -1.0710e-01,\n",
      "        -1.3997e-01, -1.4927e-01, -1.1089e-01,  8.1709e-03, -9.3736e-02,\n",
      "        -5.3084e-02, -3.4243e-02, -6.0463e-02,  1.6574e-02, -1.9907e-01,\n",
      "        -2.0515e-01, -1.9256e-01, -4.4962e-02, -4.8626e-02, -1.7139e-01,\n",
      "         2.1489e-02, -1.2279e-01, -6.8664e-02, -1.2007e-01, -6.2422e-02,\n",
      "        -6.9512e-02, -7.1663e-02, -1.3227e-01, -1.4172e-01, -7.1662e-02,\n",
      "        -1.1108e-01, -7.9166e-02, -1.9248e-01, -1.9825e-01, -1.3689e-01,\n",
      "        -1.2874e-01, -1.2638e-01, -1.4880e-01, -1.4937e-01, -6.1662e-02,\n",
      "        -4.6141e-02, -1.0807e-01,  8.5392e-02, -1.4873e-01, -7.8552e-02,\n",
      "        -2.1504e-01, -1.3059e-01, -1.4240e-01, -8.8054e-02, -2.1200e-01,\n",
      "        -6.6706e-02, -6.6601e-02, -1.1683e-01,  2.1626e-02, -1.5902e-01,\n",
      "        -1.9307e-01, -1.1410e-01, -1.1164e-01, -1.4930e-01,  3.2233e-02,\n",
      "        -7.7330e-02, -1.0806e-01, -6.6624e-02, -1.3467e-01, -1.5811e-01,\n",
      "         7.0173e-02, -9.7803e-02, -1.9594e-01, -2.0050e-01, -3.6580e-02,\n",
      "        -5.1659e-02, -5.3162e-02, -8.0219e-02, -1.0635e-01, -1.6347e-01,\n",
      "        -1.2443e-01, -1.6899e-02, -1.4331e-01, -2.2777e-01, -8.4425e-02,\n",
      "        -8.6491e-02, -8.6737e-02, -2.0568e-01,  1.0568e+00,  8.8343e-01,\n",
      "         9.9685e-01,  9.6140e-01,  7.9164e-01,  1.1864e+00,  8.7235e-01,\n",
      "         1.0311e+00,  9.3527e-01,  8.9694e-01,  8.5597e-01,  7.6645e-01,\n",
      "         1.1689e+00,  9.0855e-01,  7.5651e-01,  9.4689e-01,  8.8346e-01,\n",
      "         8.4990e-01,  7.8836e-01,  8.4780e-01,  1.4431e+00,  9.5758e-01,\n",
      "         1.0235e+00,  1.1555e+00,  8.4475e-01,  1.0310e+00,  8.3800e-01,\n",
      "         8.3623e-01,  7.9453e-01,  8.8240e-01,  1.1409e+00,  7.3724e-01,\n",
      "         8.4530e-01,  9.2300e-01,  8.9269e-01,  7.6521e-01,  9.4142e-01,\n",
      "         8.9931e-01,  9.3951e-01,  9.8989e-01,  8.3717e-01,  7.7847e-01,\n",
      "         8.4319e-01,  8.6754e-01,  6.7788e-01,  8.7619e-01,  7.0170e-01,\n",
      "         9.2290e-01,  1.1869e+00,  8.1995e-01,  8.8086e-01,  8.6066e-01,\n",
      "         7.9597e-01,  7.6375e-01,  8.2710e-01,  1.0164e+00,  9.7926e-01,\n",
      "         7.5281e-01,  1.0121e+00,  8.2645e-01,  1.1220e+00,  1.2544e+00,\n",
      "         8.4652e-01,  8.6882e-01,  8.3466e-01,  1.2434e+00,  1.1629e+00,\n",
      "         8.1040e-01,  8.9449e-01,  8.9771e-01,  9.4913e-01,  1.0786e+00,\n",
      "         8.8804e-01,  8.5271e-01,  8.7583e-01,  8.4775e-01,  7.8830e-01,\n",
      "         9.6931e-01,  8.2772e-01,  8.2659e-01,  8.9113e-01,  8.9190e-01,\n",
      "         7.8690e-01,  1.0883e+00,  8.8515e-01,  9.7608e-01,  1.0434e+00,\n",
      "         8.8723e-01,  8.5873e-01,  8.1670e-01,  8.4395e-01,  8.2940e-01,\n",
      "         9.2736e-01,  7.6380e-01,  8.5197e-01,  1.0683e+00,  1.1185e+00,\n",
      "         1.0268e+00,  9.9702e-01,  8.9841e-01,  1.1415e+00,  8.2640e-01,\n",
      "         9.3243e-01,  1.0575e+00,  1.1338e+00,  1.0240e+00,  7.6704e-01,\n",
      "         8.0891e-01,  8.7091e-01,  7.6302e-01,  1.0694e+00,  1.0823e+00,\n",
      "         1.2594e+00,  7.8461e-01,  9.2382e-01,  8.9983e-01,  9.2007e-01,\n",
      "         8.7199e-01,  8.2303e-01,  1.1656e+00,  9.8461e-01,  9.2462e-01,\n",
      "         6.8956e-01,  9.9380e-01,  8.8297e-01,  5.9773e-01,  8.2704e-01,\n",
      "         1.1397e+00, -8.1031e-03,  1.0498e-02, -4.8113e-02,  1.2993e-02,\n",
      "        -4.3104e-03,  5.9474e-02,  4.7312e-02, -2.9205e-02,  2.4501e-04,\n",
      "        -5.9289e-02,  3.3117e-02,  7.5198e-02, -4.9252e-02, -7.2948e-02,\n",
      "         7.9771e-02,  4.0533e-02,  4.0295e-02, -1.1559e-02,  2.4674e-02,\n",
      "        -7.4330e-02,  1.9396e-02, -1.7416e-02, -2.3931e-02, -7.1206e-02,\n",
      "        -5.9275e-02, -2.8195e-02,  3.0626e-02, -1.9108e-02,  2.7153e-02,\n",
      "         5.1569e-02,  3.2368e-02, -3.7625e-02,  7.4174e-03, -3.5738e-02,\n",
      "         7.4419e-03, -5.4605e-02, -4.5684e-02, -6.0131e-02, -2.2289e-02,\n",
      "         1.1110e-02, -4.4015e-02,  5.4796e-02, -1.3701e-02, -4.4356e-02,\n",
      "         6.6029e-02, -2.4792e-02,  5.0498e-02, -6.5300e-02,  2.6450e-02,\n",
      "        -1.4033e-02,  9.8482e-03,  1.5373e-01, -7.2197e-02,  3.8374e-03,\n",
      "        -1.7942e-02,  6.7489e-03, -5.3188e-02, -9.0637e-02,  7.2591e-02,\n",
      "        -3.7758e-02,  7.0571e-02, -5.6007e-03, -5.0731e-02, -6.6845e-02,\n",
      "        -5.3944e-03, -1.0487e-01, -2.1003e-02, -1.4567e-02, -2.8627e-02,\n",
      "        -4.3882e-02,  4.2177e-02,  4.8148e-02, -3.1481e-02, -4.3597e-02,\n",
      "        -4.6965e-02, -1.1312e-02, -2.8724e-03,  4.2330e-02,  5.9213e-03,\n",
      "         3.7778e-03,  1.0654e-02, -6.9038e-03,  5.9464e-03,  1.2541e-02,\n",
      "         4.1399e-02, -5.3773e-02, -3.6634e-02, -2.0141e-01,  2.4083e-02,\n",
      "         4.7501e-02, -5.2992e-02,  6.1490e-02, -6.0472e-02, -3.5847e-02,\n",
      "        -7.0360e-03, -2.2799e-02, -4.9112e-02, -8.5505e-02, -9.6661e-02,\n",
      "        -1.0032e-03, -5.4166e-02, -9.9392e-03, -6.4473e-04, -8.4395e-02,\n",
      "        -8.9196e-02, -1.5464e-02, -3.3263e-02,  7.5671e-02, -1.8876e-02,\n",
      "        -3.9325e-02, -9.0957e-02, -4.6718e-02,  3.2860e-02, -9.0401e-02,\n",
      "        -6.9605e-02, -1.5480e-01,  6.6328e-03,  3.9926e-02, -4.7689e-02,\n",
      "         1.9755e-02, -6.9249e-02, -7.9252e-02, -8.0011e-02, -2.1109e-02,\n",
      "         2.6005e-02, -2.1671e-02,  1.6851e-02,  3.3341e-02, -2.1436e-01,\n",
      "        -1.6596e-01, -1.8772e-01, -1.2339e-01, -9.9764e-02, -2.5291e-01,\n",
      "        -1.2566e-01, -1.4991e-01, -1.3874e-01, -1.9761e-01, -1.4909e-01,\n",
      "        -2.5209e-01, -1.4733e-01, -1.4335e-01, -1.4435e-01, -1.6826e-01,\n",
      "        -1.3660e-01, -1.2887e-01, -1.5307e-01, -9.3868e-02, -8.1865e-02,\n",
      "        -2.2594e-01, -1.4885e-01, -2.3755e-01, -1.5438e-01, -1.1025e-01,\n",
      "        -1.1531e-01, -1.6621e-01, -1.2361e-01, -1.3520e-01, -2.0825e-01,\n",
      "        -5.5708e-02, -1.0885e-01, -1.4230e-01, -1.6398e-01, -1.6895e-01,\n",
      "        -1.6647e-01, -9.1760e-02, -2.1679e-01, -1.9829e-01, -1.4891e-01,\n",
      "        -1.4373e-01, -1.1366e-01, -1.3239e-01, -9.2968e-02, -1.4905e-01,\n",
      "        -1.4850e-01, -1.2997e-01, -2.1168e-01, -2.2262e-02, -1.6795e-01,\n",
      "        -2.3811e-01, -6.6834e-02, -1.5299e-01, -1.7369e-01, -1.5439e-01,\n",
      "        -1.7558e-01, -1.3412e-01, -1.9359e-01, -2.8575e-01, -2.3229e-01,\n",
      "        -9.2826e-02, -2.1657e-01, -1.3913e-02, -1.5620e-01, -1.5448e-01,\n",
      "        -3.1109e-01, -1.4938e-03, -9.7904e-02, -1.4987e-01, -1.7923e-01,\n",
      "        -2.0112e-01, -1.3815e-01, -1.3467e-01, -1.4499e-01, -1.5252e-01,\n",
      "        -8.3864e-02, -1.6200e-01, -1.6609e-01, -1.4611e-01, -1.9590e-01,\n",
      "        -2.0703e-01, -1.1631e-01, -1.4295e-01, -1.3787e-01, -1.3853e-01,\n",
      "        -2.6773e-01, -1.4597e-01, -1.3200e-01, -8.6192e-02, -1.8442e-01,\n",
      "        -2.9547e-02, -1.6512e-01, -1.1436e-01, -1.2859e-01, -2.9404e-01,\n",
      "        -2.7824e-01, -2.4153e-01, -2.4915e-01, -1.8609e-01, -2.1040e-01,\n",
      "        -1.8534e-01, -1.4536e-01, -2.3879e-01, -2.1147e-01, -1.6484e-01,\n",
      "        -1.1914e-02, -1.5494e-01, -1.3169e-01, -1.1108e-01, -2.1760e-01,\n",
      "        -1.6723e-01, -9.3111e-02, -1.9372e-01, -1.9242e-01, -1.8493e-01,\n",
      "        -2.0593e-01, -1.1009e-01, -1.5700e-01, -2.0265e-01, -1.7678e-01,\n",
      "        -1.8699e-01, -2.0853e-02, -1.5835e-01, -1.4441e-01, -1.9193e-02,\n",
      "        -1.5230e-01, -1.7933e-01], device='cuda:0', requires_grad=True))\n",
      "('encoder.rnn.rnns.0._module.weight_ih_l1_reverse', Parameter containing:\n",
      "tensor([[-0.0016, -0.0412, -0.0058,  ...,  0.0961,  0.0504,  0.1913],\n",
      "        [-0.1295,  0.1104,  0.3233,  ..., -0.2552,  0.1413,  0.1306],\n",
      "        [-0.0167, -0.0360, -0.0523,  ...,  0.1115, -0.0455, -0.0763],\n",
      "        ...,\n",
      "        [ 0.0189, -0.1381,  0.1281,  ..., -0.0425, -0.0359,  0.1962],\n",
      "        [-0.0697,  0.1381, -0.2123,  ..., -0.0935,  0.0046, -0.0450],\n",
      "        [ 0.0516, -0.1795, -0.2425,  ...,  0.1414,  0.0035, -0.0199]],\n",
      "       device='cuda:0', requires_grad=True))\n",
      "('encoder.rnn.rnns.0._module.weight_hh_l1_reverse', Parameter containing:\n",
      "tensor([[ 0.0329, -0.0054,  0.0380,  ...,  0.1881, -0.0019, -0.0107],\n",
      "        [-0.2051,  0.1786,  0.0428,  ..., -0.1931,  0.3066,  0.0403],\n",
      "        [ 0.0069,  0.0419,  0.0579,  ...,  0.0462,  0.1610,  0.0303],\n",
      "        ...,\n",
      "        [-0.1464, -0.0696,  0.1003,  ...,  0.0374,  0.0180,  0.0318],\n",
      "        [ 0.0253,  0.1070,  0.0168,  ..., -0.0253,  0.1949,  0.0888],\n",
      "        [-0.0894, -0.0998, -0.1278,  ...,  0.4966,  0.0201,  0.1327]],\n",
      "       device='cuda:0', requires_grad=True))\n",
      "('encoder.rnn.rnns.0._module.bias_ih_l1_reverse', Parameter containing:\n",
      "tensor([-6.5019e-02, -1.8636e-01, -1.0129e-01,  3.7488e-02, -1.4993e-01,\n",
      "        -4.8842e-02, -9.8273e-02, -2.2122e-01, -7.6185e-02, -1.5528e-01,\n",
      "        -1.6346e-01, -1.8886e-01, -1.7481e-01, -1.6533e-01, -2.0880e-01,\n",
      "        -8.3760e-02, -1.8120e-01, -2.4571e-01, -1.1183e-01, -5.7981e-02,\n",
      "        -1.0916e-01, -8.1665e-02, -6.6499e-02, -1.3108e-01, -8.6282e-02,\n",
      "         8.5520e-02, -1.7576e-01, -1.2931e-01, -1.1248e-01, -1.1177e-01,\n",
      "        -6.6240e-02, -1.9975e-01,  2.1456e-02,  8.8363e-02,  3.5644e-02,\n",
      "        -1.2428e-01, -7.3952e-02,  3.1377e-02, -1.3165e-01, -7.5625e-02,\n",
      "        -1.6962e-01, -9.4084e-02, -4.9901e-02, -4.3446e-02, -1.8727e-01,\n",
      "         6.9796e-03, -1.9815e-01,  2.8788e-02, -1.1624e-03, -1.3406e-01,\n",
      "         1.1696e-01, -1.3157e-01,  1.6618e-02, -1.0460e-01, -1.0398e-01,\n",
      "        -1.4633e-01, -2.0114e-01, -1.2172e-01, -1.1423e-01, -2.1356e-01,\n",
      "        -1.4004e-01,  1.5522e-02, -1.5583e-01, -9.2493e-02, -1.2125e-01,\n",
      "         5.3667e-02, -1.0196e-01, -1.4725e-01,  1.7927e-02, -1.1338e-01,\n",
      "        -3.7172e-02, -1.4244e-01,  3.1238e-02,  5.8258e-02, -1.4844e-01,\n",
      "        -5.9043e-02, -5.8601e-02,  4.8317e-02, -1.3174e-01, -1.1049e-01,\n",
      "        -1.6790e-01, -8.4723e-02, -1.1533e-01, -7.0640e-02, -1.7024e-01,\n",
      "         1.6338e-02,  1.7585e-01, -5.4976e-02, -5.9553e-02, -4.8372e-02,\n",
      "        -3.1962e-01,  3.0294e-02,  2.5991e-02, -2.7794e-02, -6.4094e-02,\n",
      "        -1.1018e-01, -5.0637e-02, -1.3358e-01, -2.0270e-02,  4.7347e-02,\n",
      "        -3.1713e-02, -2.2982e-02, -1.0868e-01,  8.5087e-02,  5.4981e-03,\n",
      "        -3.2967e-02,  8.5698e-02, -8.9194e-02, -1.3368e-01,  4.2023e-02,\n",
      "        -2.0484e-01, -1.0355e-01, -1.3616e-01, -2.4221e-01, -1.4484e-01,\n",
      "        -2.0008e-01, -6.9573e-02,  4.0362e-02, -3.8208e-02,  2.0331e-02,\n",
      "        -1.6935e-01, -1.3818e-01, -1.8256e-01, -5.2685e-02, -1.4261e-01,\n",
      "         4.7903e-02,  6.4398e-03, -3.9863e-02,  9.3146e-01,  7.8031e-01,\n",
      "         8.4330e-01,  9.6044e-01,  9.2518e-01,  9.0179e-01,  9.3901e-01,\n",
      "         1.0344e+00,  8.6587e-01,  1.1021e+00,  9.4781e-01,  8.6166e-01,\n",
      "         8.7501e-01,  8.1145e-01,  1.3041e+00,  7.8145e-01,  7.8155e-01,\n",
      "         7.5641e-01,  1.0021e+00,  1.0632e+00,  8.5029e-01,  7.6682e-01,\n",
      "         1.0603e+00,  9.0009e-01,  8.8882e-01,  1.2935e+00,  8.7776e-01,\n",
      "         9.1012e-01,  9.0970e-01,  1.1018e+00,  7.8719e-01,  9.4971e-01,\n",
      "         1.0007e+00,  1.1549e+00,  9.0624e-01,  8.4531e-01,  7.6515e-01,\n",
      "         1.1391e+00,  8.6117e-01,  9.0213e-01,  7.3585e-01,  1.1980e+00,\n",
      "         1.0186e+00,  9.8853e-01,  8.7069e-01,  9.3572e-01,  8.3916e-01,\n",
      "         1.0452e+00,  1.0090e+00,  8.6526e-01,  1.0260e+00,  7.7041e-01,\n",
      "         9.6020e-01,  8.3152e-01,  8.6269e-01,  1.0813e+00,  8.2854e-01,\n",
      "         6.9826e-01,  9.3090e-01,  1.2450e+00,  1.2713e+00,  1.0919e+00,\n",
      "         9.2113e-01,  8.0358e-01,  8.0242e-01,  1.1135e+00,  8.5661e-01,\n",
      "         8.5556e-01,  1.1335e+00,  7.8250e-01,  1.0524e+00,  9.3645e-01,\n",
      "         9.5503e-01,  1.0251e+00,  8.2931e-01,  8.6895e-01,  1.1464e+00,\n",
      "         1.1084e+00,  1.0210e+00,  9.3479e-01,  8.9923e-01,  8.6998e-01,\n",
      "         8.7573e-01,  9.4378e-01,  7.4363e-01,  9.8805e-01,  9.7930e-01,\n",
      "         1.0350e+00,  9.0009e-01,  1.1108e+00,  1.2366e+00,  1.0603e+00,\n",
      "         9.1628e-01,  8.6756e-01,  8.6751e-01,  8.5964e-01,  9.3634e-01,\n",
      "         9.5075e-01,  1.0086e+00,  9.2422e-01,  1.0400e+00,  9.2818e-01,\n",
      "         8.3037e-01,  8.5797e-01,  1.0804e+00,  1.0641e+00,  1.1636e+00,\n",
      "         9.9924e-01,  9.2830e-01,  9.9675e-01,  7.5105e-01,  9.8647e-01,\n",
      "         9.9393e-01,  8.1179e-01,  9.3872e-01,  7.8783e-01,  7.8506e-01,\n",
      "         8.7986e-01,  9.3029e-01,  1.0017e+00,  8.9950e-01,  8.1364e-01,\n",
      "         1.2459e+00,  8.4087e-01,  8.7716e-01,  7.1131e-01,  1.0314e+00,\n",
      "         8.7463e-01,  4.8650e-02, -2.2218e-03, -6.5143e-02,  8.2285e-02,\n",
      "        -6.0678e-04, -1.3369e-02, -1.5166e-02,  2.4329e-04,  6.4268e-02,\n",
      "        -1.4710e-01, -2.0409e-02, -3.6658e-02, -4.4549e-03,  2.5512e-03,\n",
      "        -8.3963e-02, -5.9381e-02, -6.1500e-02, -1.1238e-02, -1.6286e-02,\n",
      "         8.6161e-02,  2.4577e-02, -5.1337e-02, -7.1806e-02, -1.6761e-03,\n",
      "         3.3975e-02, -1.4625e-01, -6.0464e-02, -2.1764e-02,  4.1178e-02,\n",
      "        -1.1640e-02, -1.5508e-02, -4.9719e-02, -5.1761e-02,  6.2143e-02,\n",
      "        -5.4104e-03, -8.2474e-03, -8.6776e-03, -1.9889e-02,  7.3224e-02,\n",
      "         5.7529e-02, -6.3087e-03, -4.4468e-03,  7.8442e-02,  6.0689e-02,\n",
      "        -7.4715e-03, -1.5668e-01, -3.1307e-02, -5.7261e-02,  9.7896e-02,\n",
      "        -8.1555e-02, -5.8269e-02,  1.2424e-02,  2.7674e-03, -6.1214e-03,\n",
      "        -3.6848e-02, -1.5523e-02,  1.7495e-02,  3.1287e-02,  4.4081e-02,\n",
      "        -2.9952e-02,  3.3311e-02, -5.9672e-02, -5.4137e-03, -3.9705e-02,\n",
      "         6.6328e-02, -7.1504e-02, -8.0981e-03, -2.0361e-02,  7.2825e-02,\n",
      "         1.6487e-02,  1.0494e-01,  7.1799e-03,  3.9517e-02, -6.0162e-02,\n",
      "        -4.1154e-02,  1.4920e-02, -9.5623e-02,  2.8409e-02, -5.7008e-02,\n",
      "         4.4372e-02, -3.4609e-02, -5.3566e-02, -9.3574e-02,  1.5842e-02,\n",
      "        -1.2662e-01,  1.4892e-01, -7.6471e-02, -4.8601e-02,  4.6272e-02,\n",
      "        -8.5785e-02,  9.3099e-02,  7.0583e-02,  7.8555e-02, -3.3627e-02,\n",
      "         3.8565e-02,  7.8103e-03,  1.2751e-02, -1.3946e-02,  8.0468e-02,\n",
      "        -5.2753e-02, -7.1504e-02, -1.1387e-01,  4.7191e-02, -3.1646e-02,\n",
      "         5.5725e-02,  3.0052e-02, -6.1218e-02,  3.6641e-03,  5.1513e-02,\n",
      "        -7.7752e-02, -7.5129e-02, -4.4367e-02,  2.6655e-02, -6.6432e-02,\n",
      "        -1.2627e-02,  4.9519e-02, -3.2526e-02,  9.8972e-02,  3.6079e-02,\n",
      "         2.1106e-02, -2.5177e-02, -4.1661e-02, -8.0737e-02,  7.0685e-02,\n",
      "        -1.2833e-02, -3.0545e-02, -1.2805e-01, -6.9499e-02, -1.4181e-01,\n",
      "        -1.8078e-01, -1.0336e-01, -2.8022e-01, -1.8500e-01, -3.0420e-02,\n",
      "        -8.6895e-02, -1.0807e-01, -2.0127e-01, -2.6620e-01, -1.6467e-01,\n",
      "         1.4952e-02, -1.1108e-01, -1.0898e-01, -7.7129e-02, -9.9933e-02,\n",
      "        -5.2249e-02, -6.0961e-02, -1.9131e-01, -2.2075e-01, -8.5028e-02,\n",
      "        -1.1090e-01, -2.2031e-01, -5.7221e-02, -1.8079e-01, -2.2518e-01,\n",
      "        -1.8949e-01, -1.0937e-01, -1.1687e-01, -1.6178e-01, -2.6436e-02,\n",
      "        -1.8373e-02, -1.8793e-01, -2.2430e-01, -1.8805e-01, -1.3104e-01,\n",
      "        -6.4337e-02, -2.2193e-01, -1.8672e-01, -1.0318e-01, -9.5496e-02,\n",
      "        -1.0239e-01, -2.0907e-01, -2.5769e-01, -1.2386e-01, -5.0255e-02,\n",
      "        -1.0385e-01, -1.8571e-01, -1.8256e-01, -1.2870e-01, -1.0407e-01,\n",
      "        -6.2339e-02, -2.5401e-01, -1.0902e-01, -8.6856e-02, -1.3993e-01,\n",
      "        -1.7476e-01, -1.0129e-02, -1.4783e-01, -1.4321e-01, -7.6004e-02,\n",
      "        -2.1820e-01, -9.1732e-02, -1.6283e-01, -1.2902e-01, -2.1744e-01,\n",
      "        -2.2795e-02, -1.0937e-01, -1.8477e-01, -1.0946e-01, -2.3181e-01,\n",
      "        -5.5325e-02, -1.9685e-01, -1.9543e-01, -9.4958e-02, -5.7908e-02,\n",
      "        -2.6656e-01, -2.4412e-01, -1.6330e-01, -1.4138e-01, -1.4674e-01,\n",
      "        -1.9135e-01, -1.6495e-01, -2.0541e-01, -5.8032e-02, -2.8666e-01,\n",
      "        -1.0343e-01, -2.0113e-01, -1.9686e-01, -1.5287e-01, -2.6767e-01,\n",
      "        -3.1541e-01, -1.5508e-01, -1.6250e-01, -1.2387e-01, -1.7115e-01,\n",
      "        -9.1747e-02, -1.5436e-01, -2.2974e-01, -2.0711e-01, -3.0382e-01,\n",
      "        -1.7053e-01, -1.0694e-01, -1.6026e-01, -2.5709e-01, -2.4089e-01,\n",
      "        -2.2762e-01, -2.0329e-01, -1.0762e-01, -1.6904e-01, -1.4553e-01,\n",
      "        -1.5808e-01, -1.1560e-01, -1.4967e-01, -1.9500e-01, -2.0328e-01,\n",
      "        -2.5024e-01, -1.7957e-01, -2.8323e-01, -2.6209e-01, -1.3826e-01,\n",
      "        -8.9325e-02, -1.0752e-01, -5.6675e-02, -1.1597e-01, -4.3874e-02,\n",
      "        -1.5138e-01, -8.0868e-02], device='cuda:0', requires_grad=True))\n",
      "('encoder.rnn.rnns.0._module.bias_hh_l1_reverse', Parameter containing:\n",
      "tensor([-6.5019e-02, -1.8636e-01, -1.0129e-01,  3.7488e-02, -1.4993e-01,\n",
      "        -4.8842e-02, -9.8273e-02, -2.2122e-01, -7.6185e-02, -1.5528e-01,\n",
      "        -1.6346e-01, -1.8886e-01, -1.7481e-01, -1.6533e-01, -2.0880e-01,\n",
      "        -8.3760e-02, -1.8120e-01, -2.4571e-01, -1.1183e-01, -5.7981e-02,\n",
      "        -1.0916e-01, -8.1665e-02, -6.6499e-02, -1.3108e-01, -8.6282e-02,\n",
      "         8.5520e-02, -1.7576e-01, -1.2931e-01, -1.1248e-01, -1.1177e-01,\n",
      "        -6.6240e-02, -1.9975e-01,  2.1456e-02,  8.8363e-02,  3.5644e-02,\n",
      "        -1.2428e-01, -7.3952e-02,  3.1377e-02, -1.3165e-01, -7.5625e-02,\n",
      "        -1.6962e-01, -9.4084e-02, -4.9901e-02, -4.3446e-02, -1.8727e-01,\n",
      "         6.9796e-03, -1.9815e-01,  2.8788e-02, -1.1624e-03, -1.3406e-01,\n",
      "         1.1696e-01, -1.3157e-01,  1.6618e-02, -1.0460e-01, -1.0398e-01,\n",
      "        -1.4633e-01, -2.0114e-01, -1.2172e-01, -1.1423e-01, -2.1356e-01,\n",
      "        -1.4004e-01,  1.5522e-02, -1.5583e-01, -9.2493e-02, -1.2125e-01,\n",
      "         5.3667e-02, -1.0196e-01, -1.4725e-01,  1.7927e-02, -1.1338e-01,\n",
      "        -3.7172e-02, -1.4244e-01,  3.1238e-02,  5.8258e-02, -1.4844e-01,\n",
      "        -5.9043e-02, -5.8601e-02,  4.8317e-02, -1.3174e-01, -1.1049e-01,\n",
      "        -1.6790e-01, -8.4723e-02, -1.1533e-01, -7.0640e-02, -1.7024e-01,\n",
      "         1.6338e-02,  1.7585e-01, -5.4976e-02, -5.9553e-02, -4.8372e-02,\n",
      "        -3.1962e-01,  3.0294e-02,  2.5991e-02, -2.7794e-02, -6.4094e-02,\n",
      "        -1.1018e-01, -5.0637e-02, -1.3358e-01, -2.0270e-02,  4.7347e-02,\n",
      "        -3.1713e-02, -2.2982e-02, -1.0868e-01,  8.5087e-02,  5.4981e-03,\n",
      "        -3.2967e-02,  8.5698e-02, -8.9194e-02, -1.3368e-01,  4.2023e-02,\n",
      "        -2.0484e-01, -1.0355e-01, -1.3616e-01, -2.4221e-01, -1.4484e-01,\n",
      "        -2.0008e-01, -6.9573e-02,  4.0362e-02, -3.8208e-02,  2.0331e-02,\n",
      "        -1.6935e-01, -1.3818e-01, -1.8256e-01, -5.2685e-02, -1.4261e-01,\n",
      "         4.7903e-02,  6.4398e-03, -3.9863e-02,  9.3146e-01,  7.8031e-01,\n",
      "         8.4330e-01,  9.6044e-01,  9.2518e-01,  9.0179e-01,  9.3901e-01,\n",
      "         1.0344e+00,  8.6587e-01,  1.1021e+00,  9.4781e-01,  8.6166e-01,\n",
      "         8.7501e-01,  8.1145e-01,  1.3041e+00,  7.8145e-01,  7.8155e-01,\n",
      "         7.5641e-01,  1.0021e+00,  1.0632e+00,  8.5029e-01,  7.6682e-01,\n",
      "         1.0603e+00,  9.0009e-01,  8.8882e-01,  1.2935e+00,  8.7776e-01,\n",
      "         9.1012e-01,  9.0970e-01,  1.1018e+00,  7.8719e-01,  9.4971e-01,\n",
      "         1.0007e+00,  1.1549e+00,  9.0624e-01,  8.4531e-01,  7.6515e-01,\n",
      "         1.1391e+00,  8.6117e-01,  9.0213e-01,  7.3585e-01,  1.1980e+00,\n",
      "         1.0186e+00,  9.8853e-01,  8.7069e-01,  9.3572e-01,  8.3916e-01,\n",
      "         1.0452e+00,  1.0090e+00,  8.6526e-01,  1.0260e+00,  7.7041e-01,\n",
      "         9.6020e-01,  8.3152e-01,  8.6269e-01,  1.0813e+00,  8.2854e-01,\n",
      "         6.9826e-01,  9.3090e-01,  1.2450e+00,  1.2713e+00,  1.0919e+00,\n",
      "         9.2113e-01,  8.0358e-01,  8.0242e-01,  1.1135e+00,  8.5661e-01,\n",
      "         8.5556e-01,  1.1335e+00,  7.8250e-01,  1.0524e+00,  9.3645e-01,\n",
      "         9.5503e-01,  1.0251e+00,  8.2931e-01,  8.6895e-01,  1.1464e+00,\n",
      "         1.1084e+00,  1.0210e+00,  9.3479e-01,  8.9923e-01,  8.6998e-01,\n",
      "         8.7573e-01,  9.4378e-01,  7.4363e-01,  9.8805e-01,  9.7930e-01,\n",
      "         1.0350e+00,  9.0009e-01,  1.1108e+00,  1.2366e+00,  1.0603e+00,\n",
      "         9.1628e-01,  8.6756e-01,  8.6751e-01,  8.5964e-01,  9.3634e-01,\n",
      "         9.5075e-01,  1.0086e+00,  9.2422e-01,  1.0400e+00,  9.2818e-01,\n",
      "         8.3037e-01,  8.5797e-01,  1.0804e+00,  1.0641e+00,  1.1636e+00,\n",
      "         9.9924e-01,  9.2830e-01,  9.9675e-01,  7.5105e-01,  9.8647e-01,\n",
      "         9.9393e-01,  8.1179e-01,  9.3872e-01,  7.8783e-01,  7.8506e-01,\n",
      "         8.7986e-01,  9.3029e-01,  1.0017e+00,  8.9950e-01,  8.1364e-01,\n",
      "         1.2459e+00,  8.4087e-01,  8.7716e-01,  7.1131e-01,  1.0314e+00,\n",
      "         8.7463e-01,  4.8650e-02, -2.2218e-03, -6.5143e-02,  8.2285e-02,\n",
      "        -6.0678e-04, -1.3369e-02, -1.5166e-02,  2.4329e-04,  6.4268e-02,\n",
      "        -1.4710e-01, -2.0409e-02, -3.6658e-02, -4.4549e-03,  2.5512e-03,\n",
      "        -8.3963e-02, -5.9381e-02, -6.1500e-02, -1.1238e-02, -1.6286e-02,\n",
      "         8.6161e-02,  2.4577e-02, -5.1337e-02, -7.1806e-02, -1.6761e-03,\n",
      "         3.3975e-02, -1.4625e-01, -6.0464e-02, -2.1764e-02,  4.1178e-02,\n",
      "        -1.1640e-02, -1.5508e-02, -4.9719e-02, -5.1761e-02,  6.2143e-02,\n",
      "        -5.4104e-03, -8.2474e-03, -8.6776e-03, -1.9889e-02,  7.3224e-02,\n",
      "         5.7529e-02, -6.3087e-03, -4.4468e-03,  7.8442e-02,  6.0689e-02,\n",
      "        -7.4715e-03, -1.5668e-01, -3.1307e-02, -5.7261e-02,  9.7896e-02,\n",
      "        -8.1555e-02, -5.8269e-02,  1.2424e-02,  2.7674e-03, -6.1214e-03,\n",
      "        -3.6848e-02, -1.5523e-02,  1.7495e-02,  3.1287e-02,  4.4081e-02,\n",
      "        -2.9952e-02,  3.3311e-02, -5.9672e-02, -5.4137e-03, -3.9705e-02,\n",
      "         6.6328e-02, -7.1504e-02, -8.0981e-03, -2.0361e-02,  7.2825e-02,\n",
      "         1.6487e-02,  1.0494e-01,  7.1799e-03,  3.9517e-02, -6.0162e-02,\n",
      "        -4.1154e-02,  1.4920e-02, -9.5623e-02,  2.8409e-02, -5.7008e-02,\n",
      "         4.4372e-02, -3.4609e-02, -5.3566e-02, -9.3574e-02,  1.5842e-02,\n",
      "        -1.2662e-01,  1.4892e-01, -7.6471e-02, -4.8601e-02,  4.6272e-02,\n",
      "        -8.5785e-02,  9.3099e-02,  7.0583e-02,  7.8555e-02, -3.3627e-02,\n",
      "         3.8565e-02,  7.8103e-03,  1.2751e-02, -1.3946e-02,  8.0468e-02,\n",
      "        -5.2753e-02, -7.1504e-02, -1.1387e-01,  4.7191e-02, -3.1646e-02,\n",
      "         5.5725e-02,  3.0052e-02, -6.1218e-02,  3.6641e-03,  5.1513e-02,\n",
      "        -7.7752e-02, -7.5129e-02, -4.4367e-02,  2.6655e-02, -6.6432e-02,\n",
      "        -1.2627e-02,  4.9519e-02, -3.2526e-02,  9.8972e-02,  3.6079e-02,\n",
      "         2.1106e-02, -2.5177e-02, -4.1661e-02, -8.0737e-02,  7.0685e-02,\n",
      "        -1.2833e-02, -3.0545e-02, -1.2805e-01, -6.9499e-02, -1.4181e-01,\n",
      "        -1.8078e-01, -1.0336e-01, -2.8022e-01, -1.8500e-01, -3.0420e-02,\n",
      "        -8.6895e-02, -1.0807e-01, -2.0127e-01, -2.6620e-01, -1.6467e-01,\n",
      "         1.4952e-02, -1.1108e-01, -1.0898e-01, -7.7129e-02, -9.9933e-02,\n",
      "        -5.2249e-02, -6.0961e-02, -1.9131e-01, -2.2075e-01, -8.5028e-02,\n",
      "        -1.1090e-01, -2.2031e-01, -5.7221e-02, -1.8079e-01, -2.2518e-01,\n",
      "        -1.8949e-01, -1.0937e-01, -1.1687e-01, -1.6178e-01, -2.6436e-02,\n",
      "        -1.8373e-02, -1.8793e-01, -2.2430e-01, -1.8805e-01, -1.3104e-01,\n",
      "        -6.4337e-02, -2.2193e-01, -1.8672e-01, -1.0318e-01, -9.5496e-02,\n",
      "        -1.0239e-01, -2.0907e-01, -2.5769e-01, -1.2386e-01, -5.0255e-02,\n",
      "        -1.0385e-01, -1.8571e-01, -1.8256e-01, -1.2870e-01, -1.0407e-01,\n",
      "        -6.2339e-02, -2.5401e-01, -1.0902e-01, -8.6856e-02, -1.3993e-01,\n",
      "        -1.7476e-01, -1.0129e-02, -1.4783e-01, -1.4321e-01, -7.6004e-02,\n",
      "        -2.1820e-01, -9.1732e-02, -1.6283e-01, -1.2902e-01, -2.1744e-01,\n",
      "        -2.2795e-02, -1.0937e-01, -1.8477e-01, -1.0946e-01, -2.3181e-01,\n",
      "        -5.5325e-02, -1.9685e-01, -1.9543e-01, -9.4958e-02, -5.7908e-02,\n",
      "        -2.6656e-01, -2.4412e-01, -1.6330e-01, -1.4138e-01, -1.4674e-01,\n",
      "        -1.9135e-01, -1.6495e-01, -2.0541e-01, -5.8032e-02, -2.8666e-01,\n",
      "        -1.0343e-01, -2.0113e-01, -1.9686e-01, -1.5287e-01, -2.6767e-01,\n",
      "        -3.1541e-01, -1.5508e-01, -1.6250e-01, -1.2387e-01, -1.7115e-01,\n",
      "        -9.1747e-02, -1.5436e-01, -2.2974e-01, -2.0711e-01, -3.0382e-01,\n",
      "        -1.7053e-01, -1.0694e-01, -1.6026e-01, -2.5709e-01, -2.4089e-01,\n",
      "        -2.2762e-01, -2.0329e-01, -1.0762e-01, -1.6904e-01, -1.4553e-01,\n",
      "        -1.5808e-01, -1.1560e-01, -1.4967e-01, -1.9500e-01, -2.0328e-01,\n",
      "        -2.5024e-01, -1.7957e-01, -2.8323e-01, -2.6209e-01, -1.3826e-01,\n",
      "        -8.9325e-02, -1.0752e-01, -5.6675e-02, -1.1597e-01, -4.3874e-02,\n",
      "        -1.5138e-01, -8.0868e-02], device='cuda:0', requires_grad=True))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('encoder.pool.attn.vw', Parameter containing:\n",
      "tensor([[-5.9329e-02],\n",
      "        [ 3.3675e-02],\n",
      "        [-4.0304e-02],\n",
      "        [ 1.0284e-02],\n",
      "        [ 6.9844e-03],\n",
      "        [ 3.5175e-02],\n",
      "        [ 9.1015e-02],\n",
      "        [ 6.8048e-03],\n",
      "        [-8.3562e-03],\n",
      "        [-5.7484e-02],\n",
      "        [ 3.9696e-02],\n",
      "        [-7.6986e-03],\n",
      "        [ 5.9806e-02],\n",
      "        [-1.1036e-01],\n",
      "        [ 7.0588e-02],\n",
      "        [-1.8803e-02],\n",
      "        [ 8.8789e-02],\n",
      "        [ 9.8090e-02],\n",
      "        [-4.6929e-02],\n",
      "        [-2.4419e-03],\n",
      "        [ 1.0095e-01],\n",
      "        [-3.7647e-03],\n",
      "        [ 2.1844e-02],\n",
      "        [ 1.1895e-01],\n",
      "        [-4.4266e-02],\n",
      "        [-2.0162e-02],\n",
      "        [-1.7606e-02],\n",
      "        [ 3.8982e-02],\n",
      "        [ 2.9649e-02],\n",
      "        [-2.2275e-02],\n",
      "        [ 1.8519e-03],\n",
      "        [-4.2612e-02],\n",
      "        [ 6.2232e-04],\n",
      "        [-1.1299e-02],\n",
      "        [ 3.8047e-02],\n",
      "        [-2.4077e-02],\n",
      "        [-2.0435e-02],\n",
      "        [ 5.6781e-02],\n",
      "        [-5.0576e-02],\n",
      "        [-9.9747e-04],\n",
      "        [ 2.0378e-02],\n",
      "        [ 4.5340e-02],\n",
      "        [-1.2665e-03],\n",
      "        [ 1.0574e-02],\n",
      "        [-1.2851e-01],\n",
      "        [-9.5212e-02],\n",
      "        [ 5.4860e-02],\n",
      "        [ 9.5627e-03],\n",
      "        [ 1.1335e-01],\n",
      "        [ 6.8574e-02],\n",
      "        [ 3.8939e-03],\n",
      "        [ 2.6843e-02],\n",
      "        [-4.4108e-02],\n",
      "        [-5.5561e-03],\n",
      "        [ 2.6186e-02],\n",
      "        [-3.6601e-02],\n",
      "        [ 1.1239e-02],\n",
      "        [-5.4044e-02],\n",
      "        [-6.6254e-02],\n",
      "        [ 1.0444e-02],\n",
      "        [ 5.8818e-02],\n",
      "        [-3.9919e-02],\n",
      "        [-8.1038e-02],\n",
      "        [-6.3912e-03],\n",
      "        [ 1.0677e-01],\n",
      "        [ 9.4369e-05],\n",
      "        [ 5.7177e-02],\n",
      "        [-5.9694e-03],\n",
      "        [ 7.4453e-03],\n",
      "        [ 8.7318e-03],\n",
      "        [ 8.5631e-02],\n",
      "        [-5.0424e-02],\n",
      "        [-2.3697e-02],\n",
      "        [-8.0379e-03],\n",
      "        [-4.6665e-02],\n",
      "        [ 3.7606e-02],\n",
      "        [ 6.5156e-02],\n",
      "        [-2.0889e-02],\n",
      "        [-4.4870e-03],\n",
      "        [-5.9149e-02],\n",
      "        [-2.9951e-02],\n",
      "        [ 6.9339e-02],\n",
      "        [ 9.7909e-04],\n",
      "        [ 6.6628e-02],\n",
      "        [ 3.1740e-02],\n",
      "        [ 2.7749e-03],\n",
      "        [-1.5769e-02],\n",
      "        [ 2.1760e-02],\n",
      "        [ 1.2080e-01],\n",
      "        [-1.2167e-01],\n",
      "        [-5.2648e-03],\n",
      "        [ 6.5398e-02],\n",
      "        [-1.4357e-02],\n",
      "        [-1.0279e-01],\n",
      "        [ 1.2689e-01],\n",
      "        [-5.7459e-03],\n",
      "        [-5.8474e-02],\n",
      "        [-4.2185e-02],\n",
      "        [-9.7229e-02],\n",
      "        [ 3.7637e-03],\n",
      "        [-6.3715e-03],\n",
      "        [ 6.7124e-02],\n",
      "        [-2.0125e-02],\n",
      "        [ 2.6440e-02],\n",
      "        [-1.1813e-01],\n",
      "        [ 1.3043e-02],\n",
      "        [-5.1243e-02],\n",
      "        [-4.3505e-03],\n",
      "        [ 2.8376e-03],\n",
      "        [-7.1929e-02],\n",
      "        [-2.2005e-02],\n",
      "        [ 2.2381e-02],\n",
      "        [ 9.7674e-02],\n",
      "        [-8.9500e-03],\n",
      "        [ 1.8957e-02],\n",
      "        [-4.0205e-03],\n",
      "        [ 2.4115e-02],\n",
      "        [-5.3548e-02],\n",
      "        [ 6.5800e-02],\n",
      "        [-5.6665e-03],\n",
      "        [-2.3216e-02],\n",
      "        [ 5.4274e-03],\n",
      "        [-8.6915e-02],\n",
      "        [-1.3310e-03],\n",
      "        [ 5.3015e-03],\n",
      "        [-2.7878e-02],\n",
      "        [ 8.8214e-03],\n",
      "        [-9.7261e-03],\n",
      "        [ 3.7762e-02],\n",
      "        [ 3.9228e-02],\n",
      "        [-5.7511e-02],\n",
      "        [-2.2835e-03],\n",
      "        [ 3.6934e-02],\n",
      "        [ 6.6459e-03],\n",
      "        [-5.3508e-02],\n",
      "        [ 1.7157e-02],\n",
      "        [-7.8741e-02],\n",
      "        [ 5.3289e-02],\n",
      "        [ 3.6811e-03],\n",
      "        [ 2.0362e-02],\n",
      "        [ 6.5225e-03],\n",
      "        [-1.5501e-02],\n",
      "        [ 1.1276e-01],\n",
      "        [ 4.0718e-02],\n",
      "        [-6.4614e-02],\n",
      "        [-3.8191e-02],\n",
      "        [ 6.5903e-02],\n",
      "        [ 2.8117e-02],\n",
      "        [-6.5952e-03],\n",
      "        [ 1.4563e-02],\n",
      "        [ 7.4978e-02],\n",
      "        [ 6.7840e-02],\n",
      "        [ 7.8242e-03],\n",
      "        [-3.7628e-03],\n",
      "        [-2.9448e-02],\n",
      "        [-8.0618e-03],\n",
      "        [ 5.1975e-02],\n",
      "        [-6.4390e-02],\n",
      "        [ 6.8280e-02],\n",
      "        [-9.1118e-02],\n",
      "        [-7.7155e-03],\n",
      "        [ 6.2064e-03],\n",
      "        [-2.0275e-03],\n",
      "        [ 3.5104e-02],\n",
      "        [ 9.0109e-02],\n",
      "        [ 1.3152e-01],\n",
      "        [ 1.2499e-03],\n",
      "        [ 3.0778e-03],\n",
      "        [-8.5448e-02],\n",
      "        [ 5.9737e-02],\n",
      "        [ 7.2529e-02],\n",
      "        [ 2.2897e-02],\n",
      "        [-1.5916e-02],\n",
      "        [ 8.0920e-02],\n",
      "        [ 7.0277e-02],\n",
      "        [ 1.8797e-03],\n",
      "        [ 3.9017e-03],\n",
      "        [-7.4564e-02],\n",
      "        [-7.0480e-02],\n",
      "        [-9.7518e-04],\n",
      "        [ 2.6298e-02],\n",
      "        [-2.2266e-02],\n",
      "        [ 1.2706e-01],\n",
      "        [ 2.6349e-02],\n",
      "        [ 2.6697e-02],\n",
      "        [-2.3421e-05],\n",
      "        [ 7.3436e-02],\n",
      "        [ 7.9473e-03],\n",
      "        [-6.0583e-03],\n",
      "        [ 2.0386e-02],\n",
      "        [-7.1627e-03],\n",
      "        [ 1.5307e-02],\n",
      "        [ 9.5650e-02],\n",
      "        [-5.0138e-02],\n",
      "        [-5.9789e-02],\n",
      "        [ 3.7588e-02],\n",
      "        [ 1.2551e-02],\n",
      "        [ 2.1360e-02],\n",
      "        [-1.0424e-01],\n",
      "        [ 3.4916e-02],\n",
      "        [-9.2563e-02],\n",
      "        [-6.1027e-02],\n",
      "        [-7.6485e-02],\n",
      "        [ 8.2762e-02],\n",
      "        [-1.7629e-02],\n",
      "        [ 8.2251e-02],\n",
      "        [ 1.0602e-02],\n",
      "        [-9.8637e-02],\n",
      "        [-1.1469e-01],\n",
      "        [ 2.3014e-02],\n",
      "        [-2.9658e-02],\n",
      "        [ 2.2921e-02],\n",
      "        [ 1.1087e-02],\n",
      "        [ 1.3176e-02],\n",
      "        [-6.8838e-02],\n",
      "        [-5.8160e-03],\n",
      "        [-1.1083e-03],\n",
      "        [ 2.0935e-02],\n",
      "        [ 4.2946e-02],\n",
      "        [ 6.6131e-03],\n",
      "        [-2.0768e-02],\n",
      "        [ 4.8665e-02],\n",
      "        [-8.2736e-02],\n",
      "        [-1.0100e-02],\n",
      "        [ 1.8601e-02],\n",
      "        [-5.5387e-02],\n",
      "        [ 5.7053e-02],\n",
      "        [-1.6329e-03],\n",
      "        [ 1.5655e-02],\n",
      "        [ 4.7809e-02],\n",
      "        [-1.5799e-02],\n",
      "        [ 1.6380e-02],\n",
      "        [ 4.8858e-02],\n",
      "        [ 2.4529e-03],\n",
      "        [ 2.6734e-02],\n",
      "        [ 4.2878e-02],\n",
      "        [ 1.3593e-02],\n",
      "        [ 1.0510e-01],\n",
      "        [-3.8965e-02],\n",
      "        [-2.5643e-02],\n",
      "        [ 9.7498e-02],\n",
      "        [ 5.8115e-02],\n",
      "        [-7.7273e-02],\n",
      "        [ 6.2551e-02],\n",
      "        [ 1.2640e-01],\n",
      "        [ 5.9447e-02],\n",
      "        [-1.6285e-02],\n",
      "        [ 4.4100e-02],\n",
      "        [-2.9516e-03],\n",
      "        [-6.0100e-02],\n",
      "        [-6.1949e-02],\n",
      "        [-6.3760e-02],\n",
      "        [ 4.6666e-02],\n",
      "        [-1.0024e-01],\n",
      "        [-8.9913e-02],\n",
      "        [ 9.0997e-02],\n",
      "        [-6.0372e-03],\n",
      "        [-1.1673e-01],\n",
      "        [-3.7113e-02],\n",
      "        [-7.5945e-03],\n",
      "        [ 5.1767e-03],\n",
      "        [ 2.0039e-02],\n",
      "        [-2.2528e-02],\n",
      "        [ 9.7246e-02],\n",
      "        [ 8.3592e-03],\n",
      "        [ 7.7007e-02],\n",
      "        [-1.1115e-01],\n",
      "        [ 7.3960e-02],\n",
      "        [-3.5354e-02],\n",
      "        [-1.5626e-02],\n",
      "        [ 2.2079e-02],\n",
      "        [-5.9877e-02],\n",
      "        [-7.1557e-02],\n",
      "        [-1.9967e-02],\n",
      "        [ 3.9284e-02],\n",
      "        [ 1.7407e-02],\n",
      "        [ 9.9808e-02],\n",
      "        [-8.7654e-02],\n",
      "        [ 5.7648e-03],\n",
      "        [ 2.7667e-02],\n",
      "        [ 8.1569e-02],\n",
      "        [ 6.2785e-02],\n",
      "        [ 4.2332e-02],\n",
      "        [ 9.0993e-02],\n",
      "        [-3.6137e-03],\n",
      "        [-1.8017e-02],\n",
      "        [-4.7976e-02],\n",
      "        [-5.4507e-03],\n",
      "        [ 1.1109e-01],\n",
      "        [-6.7086e-02],\n",
      "        [-3.3149e-02],\n",
      "        [-3.5682e-02],\n",
      "        [-2.5704e-02],\n",
      "        [ 2.3179e-02],\n",
      "        [ 6.5549e-03],\n",
      "        [ 5.6835e-02],\n",
      "        [ 4.3206e-02],\n",
      "        [ 4.1473e-02],\n",
      "        [ 1.1593e-02],\n",
      "        [ 2.7341e-02],\n",
      "        [ 1.5597e-02],\n",
      "        [ 2.5289e-02],\n",
      "        [ 2.0264e-02],\n",
      "        [-1.9757e-03],\n",
      "        [ 5.9633e-03],\n",
      "        [-7.1918e-02],\n",
      "        [ 6.0562e-02],\n",
      "        [-1.2956e-01],\n",
      "        [ 1.2476e-01],\n",
      "        [ 4.8800e-03],\n",
      "        [ 1.2802e-02],\n",
      "        [-3.1924e-02],\n",
      "        [ 1.6250e-02],\n",
      "        [-6.2118e-02],\n",
      "        [ 5.1566e-02],\n",
      "        [-7.1173e-02],\n",
      "        [ 2.6897e-02],\n",
      "        [ 8.5037e-02],\n",
      "        [ 1.4543e-02],\n",
      "        [ 9.4745e-03],\n",
      "        [ 3.2796e-02],\n",
      "        [ 1.4092e-02],\n",
      "        [ 2.9196e-03],\n",
      "        [-7.7324e-02],\n",
      "        [ 8.5921e-03],\n",
      "        [-5.8368e-02],\n",
      "        [-1.0407e-01],\n",
      "        [ 2.3291e-02],\n",
      "        [-9.3312e-02],\n",
      "        [-4.7378e-02],\n",
      "        [ 1.8206e-02],\n",
      "        [-3.4756e-03],\n",
      "        [-4.6296e-02],\n",
      "        [-3.2651e-02],\n",
      "        [-3.1981e-03],\n",
      "        [ 3.9256e-03],\n",
      "        [-5.3839e-02],\n",
      "        [ 9.0875e-04],\n",
      "        [ 9.3372e-02],\n",
      "        [-7.2631e-03],\n",
      "        [-3.4142e-02],\n",
      "        [ 3.2615e-02],\n",
      "        [-7.7218e-02],\n",
      "        [ 4.4859e-02],\n",
      "        [ 2.7298e-02],\n",
      "        [ 5.8187e-03],\n",
      "        [-3.9558e-02],\n",
      "        [ 4.8622e-03],\n",
      "        [ 1.0855e-01],\n",
      "        [ 2.4593e-02],\n",
      "        [ 1.2883e-01],\n",
      "        [-6.4050e-02],\n",
      "        [ 4.7689e-02],\n",
      "        [ 1.6544e-02],\n",
      "        [ 3.4081e-02],\n",
      "        [-1.7268e-02],\n",
      "        [-5.5383e-02],\n",
      "        [ 1.5072e-02],\n",
      "        [-2.8836e-02],\n",
      "        [-8.9427e-03],\n",
      "        [ 1.1882e-01],\n",
      "        [-3.4119e-03],\n",
      "        [ 4.9908e-02],\n",
      "        [ 5.5865e-02],\n",
      "        [-1.6043e-02],\n",
      "        [ 3.9049e-03],\n",
      "        [-6.2404e-02],\n",
      "        [ 6.8794e-02],\n",
      "        [-4.0252e-02],\n",
      "        [ 2.3640e-03],\n",
      "        [ 2.9373e-02],\n",
      "        [ 1.5601e-03],\n",
      "        [-9.0405e-03],\n",
      "        [-2.2319e-02],\n",
      "        [ 2.0031e-03],\n",
      "        [ 9.9470e-02],\n",
      "        [ 4.5539e-02],\n",
      "        [ 6.7270e-02],\n",
      "        [ 3.0483e-02],\n",
      "        [-3.8935e-02],\n",
      "        [-8.1123e-02],\n",
      "        [ 1.2489e-02],\n",
      "        [ 4.8199e-02],\n",
      "        [-5.3027e-02],\n",
      "        [-3.6359e-02],\n",
      "        [-6.0799e-03],\n",
      "        [ 3.1971e-02],\n",
      "        [ 1.1349e-02],\n",
      "        [ 5.6963e-03],\n",
      "        [-6.6354e-02],\n",
      "        [-1.2984e-03],\n",
      "        [-1.7540e-02],\n",
      "        [-4.4147e-03],\n",
      "        [-1.4320e-02],\n",
      "        [-4.9425e-02],\n",
      "        [-2.1126e-02],\n",
      "        [-1.0077e-03],\n",
      "        [-5.2471e-02],\n",
      "        [ 2.9919e-02],\n",
      "        [-7.6733e-02],\n",
      "        [ 1.8176e-02],\n",
      "        [-6.3174e-02],\n",
      "        [ 7.0641e-02],\n",
      "        [ 1.1655e-01],\n",
      "        [-1.3347e-01],\n",
      "        [ 6.3251e-02],\n",
      "        [ 4.2936e-02],\n",
      "        [ 8.1997e-03],\n",
      "        [-5.7252e-02],\n",
      "        [ 3.5726e-02],\n",
      "        [ 3.7723e-03],\n",
      "        [ 9.7166e-03],\n",
      "        [-1.0371e-02],\n",
      "        [ 5.9353e-02],\n",
      "        [-5.7714e-02],\n",
      "        [-4.6041e-02],\n",
      "        [ 3.4827e-02],\n",
      "        [ 1.8852e-02],\n",
      "        [ 1.2580e-02],\n",
      "        [-1.7433e-02],\n",
      "        [-5.3903e-02],\n",
      "        [ 4.2951e-02],\n",
      "        [-2.8476e-02],\n",
      "        [ 4.2521e-02],\n",
      "        [-1.6506e-02],\n",
      "        [-3.9361e-03],\n",
      "        [-1.1889e-02],\n",
      "        [-7.9479e-02],\n",
      "        [ 5.1709e-02],\n",
      "        [ 9.1370e-04],\n",
      "        [-6.1151e-02],\n",
      "        [-3.8886e-02],\n",
      "        [ 2.7311e-02],\n",
      "        [-1.4362e-03],\n",
      "        [-1.6469e-03],\n",
      "        [-1.0789e-02],\n",
      "        [ 1.3076e-02],\n",
      "        [ 2.9965e-02],\n",
      "        [ 1.9687e-02],\n",
      "        [ 1.0413e-02],\n",
      "        [-2.6248e-03],\n",
      "        [ 3.1467e-02],\n",
      "        [-5.4191e-02],\n",
      "        [-7.9179e-03],\n",
      "        [ 7.2677e-03],\n",
      "        [-8.0610e-02],\n",
      "        [-2.2893e-02],\n",
      "        [ 7.6724e-02],\n",
      "        [-1.0012e-02],\n",
      "        [ 4.4504e-02],\n",
      "        [ 9.2025e-02],\n",
      "        [ 1.1292e-01],\n",
      "        [-9.9538e-02],\n",
      "        [ 1.3770e-01],\n",
      "        [-9.4737e-02],\n",
      "        [-2.7039e-02],\n",
      "        [ 4.0795e-02],\n",
      "        [ 3.8812e-02],\n",
      "        [ 6.4996e-02],\n",
      "        [ 8.4588e-03],\n",
      "        [-3.1391e-02],\n",
      "        [ 5.8912e-02],\n",
      "        [-1.3346e-02],\n",
      "        [ 7.9850e-03],\n",
      "        [-1.8836e-02],\n",
      "        [ 3.8493e-02],\n",
      "        [-4.7939e-02],\n",
      "        [ 6.7030e-02],\n",
      "        [ 9.0603e-02],\n",
      "        [-5.2772e-02],\n",
      "        [ 2.0014e-02],\n",
      "        [-1.5150e-02],\n",
      "        [-1.5532e-02],\n",
      "        [ 4.7898e-03],\n",
      "        [ 1.5834e-02],\n",
      "        [-6.9381e-03],\n",
      "        [-3.2089e-02],\n",
      "        [-8.9321e-02],\n",
      "        [-5.2529e-02],\n",
      "        [-9.2336e-02],\n",
      "        [ 4.5574e-03],\n",
      "        [-1.3900e-02],\n",
      "        [ 9.1818e-03],\n",
      "        [-8.6925e-02],\n",
      "        [-4.9611e-02],\n",
      "        [-1.4781e-02],\n",
      "        [-8.0829e-02],\n",
      "        [-3.9198e-02],\n",
      "        [-1.2920e-01],\n",
      "        [ 7.1729e-02],\n",
      "        [-1.7793e-02],\n",
      "        [-2.1001e-02],\n",
      "        [ 4.7705e-02],\n",
      "        [ 5.9049e-02],\n",
      "        [-7.6521e-02],\n",
      "        [-1.0076e-02],\n",
      "        [ 2.5487e-02],\n",
      "        [-8.2326e-03],\n",
      "        [ 8.6280e-02],\n",
      "        [-1.3445e-01],\n",
      "        [ 8.0594e-03],\n",
      "        [-8.9379e-03],\n",
      "        [ 4.2630e-03],\n",
      "        [ 7.4090e-02],\n",
      "        [-8.9268e-02],\n",
      "        [-3.5121e-02],\n",
      "        [-7.6547e-03],\n",
      "        [-4.8285e-02],\n",
      "        [ 4.5218e-02],\n",
      "        [-4.4200e-02],\n",
      "        [-5.7958e-02],\n",
      "        [-3.6984e-02]], device='cuda:0', requires_grad=True))\n",
      "('encoder.pool.attn.b', Parameter containing:\n",
      "tensor([0.0007], device='cuda:0', requires_grad=True))\n",
      "('encoder.pool.attn.l1.weight', Parameter containing:\n",
      "tensor([[-0.1005,  0.1849, -0.0189,  ..., -0.2309,  0.1423, -0.0688],\n",
      "        [-0.1224,  0.0651,  0.0844,  ..., -0.2289,  0.1084, -0.0568],\n",
      "        [ 0.0370,  0.2625,  0.0586,  ..., -0.1926,  0.1323,  0.0099],\n",
      "        ...,\n",
      "        [-0.0932,  0.1010,  0.0019,  ..., -0.2211,  0.1127,  0.0660],\n",
      "        [-0.0947,  0.1994,  0.1281,  ..., -0.1672,  0.1716, -0.0468],\n",
      "        [ 0.0233,  0.0916,  0.0400,  ..., -0.1930, -0.0019, -0.1260]],\n",
      "       device='cuda:0', requires_grad=True))\n",
      "('encoder.pool.attn.l1.bias', Parameter containing:\n",
      "tensor([-0.1180, -0.3601, -0.0650,  0.2019,  0.1532,  0.1028, -0.0924, -0.4537,\n",
      "        -0.2208, -0.1430,  0.0612, -0.1720,  0.0451, -0.0613,  0.0711, -0.1563,\n",
      "         0.0810,  0.1153, -0.1058, -0.3712,  0.0010, -0.2818,  0.1225,  0.0702,\n",
      "         0.3517, -0.1573, -0.3945,  0.0929,  0.0964, -0.1653,  0.3263, -0.1377,\n",
      "         0.2385, -0.1517,  0.1188, -0.1149, -0.1680,  0.1076, -0.1195, -0.4030,\n",
      "         0.1732,  0.0904,  0.2444, -0.2880, -0.0610, -0.0557,  0.1345, -0.3104,\n",
      "         0.0864,  0.0744,  0.3117,  0.1288, -0.0853,  0.3659,  0.1110, -0.1723,\n",
      "        -0.3099, -0.0869, -0.0818, -0.2051,  0.1191,  0.2666, -0.0202,  0.1871,\n",
      "         0.0993,  0.5499,  0.0855,  0.2628, -0.1371,  0.3559,  0.0720, -0.0863,\n",
      "        -0.1241, -0.2100,  0.2443,  0.1359,  0.1239, -0.1734,  0.2149, -0.1078,\n",
      "        -0.1096,  0.0876,  0.2500,  0.0988,  0.1384,  0.3543, -0.1210, -0.2249,\n",
      "         0.0590, -0.0647,  0.2457,  0.0798, -0.3551, -0.0717,  0.0348, -0.2125,\n",
      "        -0.1119, -0.1087, -0.0313, -0.3165, -0.1955,  0.0934, -0.1211, -0.2129,\n",
      "        -0.0868,  0.1367, -0.0630, -0.2031,  0.0833, -0.0960, -0.1325,  0.1007,\n",
      "         0.0820,  0.2422,  0.1898, -0.1826,  0.1195, -0.1232,  0.0774, -0.2537,\n",
      "        -0.2051, -0.3790, -0.0587, -0.1979,  0.4936,  0.1520,  0.1679, -0.2923,\n",
      "         0.0900,  0.0721, -0.1099, -0.1579,  0.0923,  0.3429, -0.0766, -0.2131,\n",
      "        -0.0665,  0.1184,  0.2892,  0.1436,  0.2066, -0.1727,  0.0821,  0.0941,\n",
      "        -0.0817, -0.1070,  0.0740,  0.1133, -0.1403,  0.1732,  0.1031,  0.1190,\n",
      "         0.0992, -0.2020, -0.1240,  0.2148,  0.1302, -0.0875,  0.0876, -0.0703,\n",
      "        -0.1694,  0.1927,  0.2582, -0.2987,  0.0824,  0.0472, -0.3891, -0.2523,\n",
      "        -0.0930,  0.1249,  0.0761,  0.2028,  0.2247,  0.0900,  0.0788,  0.1101,\n",
      "         0.3142, -0.0992, -0.0751,  0.3151,  0.1444, -0.1093,  0.0479,  0.1047,\n",
      "        -0.2338,  0.2276,  0.0774, -0.0397, -0.1520,  0.1243, -0.1800,  0.1530,\n",
      "         0.0847, -0.0686, -0.1067,  0.1020,  0.1203,  0.1118, -0.0353,  0.1037,\n",
      "        -0.0827, -0.1046, -0.0932, -0.1178, -0.1620,  0.0996, -0.2179, -0.0758,\n",
      "        -0.0731,  0.1136, -0.1142,  0.1597,  0.2868, -0.2417, -0.0904, -0.1317,\n",
      "         0.2099,  0.1717,  0.1257, -0.2492, -0.1422,  0.0826, -0.1126, -0.1863,\n",
      "        -0.3951, -0.0869,  0.1178, -0.2789,  0.1610,  0.1191, -0.2718,  0.1381,\n",
      "         0.0596, -0.2877,  0.1364,  0.0481,  0.1662,  0.0633, -0.1053,  0.4010,\n",
      "         0.0919,  0.0739, -0.1105,  0.0975,  0.0591, -0.0516, -0.1005,  0.0870,\n",
      "        -0.2828, -0.0754, -0.0564, -0.1022,  0.1276, -0.1121, -0.0712,  0.1013,\n",
      "         0.2025, -0.0508, -0.1034, -0.1850,  0.1109,  0.0952, -0.1132,  0.0779,\n",
      "         0.1229,  0.0813, -0.0756,  0.0432, -0.1185, -0.1200,  0.1115, -0.1031,\n",
      "        -0.0828, -0.1496,  0.0739, -0.3300,  0.0807, -0.0815,  0.3937,  0.1587,\n",
      "         0.0868,  0.0956,  0.1094,  0.0834, -0.0782, -0.1474, -0.1091, -0.1996,\n",
      "         0.0771, -0.0770,  0.1550, -0.1149, -0.1357,  0.1182, -0.3629,  0.0761,\n",
      "         0.0737,  0.0985,  0.1375,  0.1324,  0.1229,  0.1402,  0.1659, -0.1358,\n",
      "        -0.2122, -0.1059,  0.0824, -0.0180,  0.0411, -0.1664, -0.1439, -0.1175,\n",
      "         0.1928, -0.0977,  0.1002, -0.0981,  0.1203,  0.0662, -0.1998,  0.2235,\n",
      "         0.1316, -0.5431, -0.3659, -0.1049, -0.2819, -0.1136, -0.0847,  0.1113,\n",
      "        -0.0736, -0.0761,  0.1437,  0.2030, -0.1031, -0.0889, -0.1610, -0.1952,\n",
      "        -0.1379, -0.2112,  0.0854, -0.2715, -0.1135,  0.0952, -0.0798, -0.1419,\n",
      "         0.1583,  0.1751, -0.0905,  0.1253,  0.0355,  0.0911,  0.0455, -0.0992,\n",
      "         0.1067,  0.1244,  0.1079, -0.1383, -0.0836, -0.1729, -0.1451,  0.3451,\n",
      "         0.0661,  0.3797,  0.0961,  0.0857, -0.1889, -0.2316, -0.0378,  0.1169,\n",
      "        -0.0869,  0.2109,  0.1028, -0.3025,  0.2457, -0.1376,  0.1998,  0.0770,\n",
      "         0.1124,  0.0934, -0.1002, -0.1213, -0.0777,  0.1611,  0.0612, -0.0935,\n",
      "        -0.1052, -0.2270,  0.1154,  0.2467,  0.3356, -0.0924,  0.1650, -0.1429,\n",
      "         0.1549,  0.3326, -0.1150, -0.1633,  0.2650, -0.0780,  0.1079, -0.1047,\n",
      "         0.1899, -0.1264,  0.0593,  0.0442, -0.0552,  0.0856,  0.1114, -0.2279,\n",
      "        -0.1238,  0.1076, -0.4007,  0.3568, -0.2016,  0.1079, -0.0904, -0.0804,\n",
      "         0.1079,  0.1666,  0.1279, -0.1676, -0.1117,  0.0926, -0.1110,  0.0530,\n",
      "         0.1910, -0.2217,  0.2452, -0.0802,  0.0673,  0.1706, -0.0623, -0.0740,\n",
      "         0.1048,  0.1760,  0.3301, -0.1818,  0.1562,  0.1117,  0.1621,  0.1864,\n",
      "        -0.3742,  0.1031, -0.0885, -0.1559, -0.2915, -0.0861, -0.1471,  0.0854,\n",
      "        -0.1688,  0.1018,  0.0791,  0.0927, -0.0724,  0.0805, -0.0733,  0.3011,\n",
      "         0.1106,  0.1257,  0.1089, -0.1616, -0.0999,  0.1007,  0.3582,  0.2307,\n",
      "         0.1721,  0.0868, -0.0423,  0.0939,  0.0875,  0.0433,  0.1418, -0.1205,\n",
      "        -0.1264, -0.3118,  0.1491, -0.1314, -0.1342, -0.0906, -0.0910, -0.0691,\n",
      "         0.3421,  0.2880,  0.2042, -0.1294, -0.0826, -0.1673, -0.0977, -0.0859,\n",
      "        -0.0356,  0.0703,  0.2345, -0.1358,  0.1230,  0.0684, -0.1116, -0.0989,\n",
      "         0.1223, -0.1695,  0.0747, -0.0270,  0.2262, -0.3926,  0.4043,  0.0638,\n",
      "        -0.0763, -0.1030, -0.1749, -0.1109,  0.1163, -0.1029, -0.1167, -0.1189],\n",
      "       device='cuda:0', requires_grad=True))\n",
      "('projection.0.weight', Parameter containing:\n",
      "tensor([[ 7.3843e-02, -9.9485e-02, -7.1799e-02,  ...,  7.5911e-02,\n",
      "         -6.8646e-02,  8.2546e-02],\n",
      "        [ 3.4228e-02, -6.2800e-02, -5.6004e-02,  ...,  1.7348e-02,\n",
      "          1.5303e-01, -2.7850e-02],\n",
      "        [-1.1851e-01,  6.6661e-02,  1.2681e-01,  ..., -1.2879e-01,\n",
      "          4.0132e-02, -9.1635e-02],\n",
      "        ...,\n",
      "        [ 1.3954e-03, -1.8726e-02, -1.5481e-02,  ...,  2.7090e-03,\n",
      "          5.1098e-02, -3.7230e-02],\n",
      "        [ 1.0090e-01, -1.7241e-01,  5.5879e-03,  ..., -5.1475e-05,\n",
      "         -4.8349e-02,  4.7962e-02],\n",
      "        [-5.0832e-02, -7.5518e-03,  1.5364e-01,  ..., -4.7815e-02,\n",
      "          8.5228e-02, -2.4961e-02]], device='cuda:0', requires_grad=True))\n",
      "('projection.0.bias', Parameter containing:\n",
      "tensor([ 0.0497, -0.0470,  0.0096,  0.0076,  0.0036, -0.0231,  0.0143, -0.0391,\n",
      "        -0.0674,  0.0366,  0.0228,  0.0523,  0.0410, -0.0132,  0.0957, -0.0057,\n",
      "        -0.0285,  0.0067,  0.0146, -0.0275,  0.0497,  0.0308,  0.0039, -0.0456,\n",
      "        -0.0059, -0.0804, -0.0490, -0.0121,  0.0239,  0.0225, -0.0038, -0.1148,\n",
      "        -0.0352,  0.0598,  0.0732, -0.0207, -0.0113,  0.0171, -0.0076,  0.0761,\n",
      "         0.0130, -0.0027,  0.0310, -0.0165, -0.0534,  0.0684, -0.0079, -0.0180,\n",
      "        -0.0299, -0.0161], device='cuda:0', requires_grad=True))\n",
      "('projection.3.weight', Parameter containing:\n",
      "tensor([[ 0.1268, -0.0096, -0.1004,  0.0911,  0.0066, -0.0880, -0.0054, -0.0942,\n",
      "         -0.0422,  0.0374,  0.0975,  0.0510, -0.0858, -0.0656,  0.0437, -0.0726,\n",
      "         -0.0906, -0.1008, -0.0718, -0.0723,  0.0414,  0.0624,  0.0073,  0.1157,\n",
      "         -0.1018, -0.0267, -0.0944,  0.0057, -0.1032,  0.0329,  0.0753, -0.0716,\n",
      "         -0.0733,  0.0827,  0.0475,  0.0511, -0.0667, -0.1136,  0.0797,  0.0187,\n",
      "         -0.1014, -0.0615, -0.0769, -0.0675, -0.0330,  0.0797, -0.0829, -0.0673,\n",
      "          0.0904, -0.1020],\n",
      "        [ 0.0123, -0.1319, -0.1040,  0.0185, -0.1722, -0.0708,  0.0506, -0.1429,\n",
      "         -0.1218,  0.0063,  0.0422, -0.0034, -0.0399, -0.0821, -0.0583, -0.0730,\n",
      "         -0.1217, -0.1000, -0.0533, -0.1074,  0.0630,  0.0183, -0.0064,  0.0194,\n",
      "         -0.1009, -0.1023, -0.0842, -0.0502, -0.1063,  0.0139,  0.0283, -0.1218,\n",
      "         -0.0766,  0.0332,  0.0314,  0.0277, -0.1134, -0.0868, -0.0022,  0.0051,\n",
      "         -0.1476, -0.0602, -0.1005, -0.1232, -0.1255,  0.0624, -0.1398, -0.1256,\n",
      "          0.0421, -0.0837],\n",
      "        [ 0.0483, -0.0916, -0.0665,  0.0121, -0.0511, -0.0889,  0.0278, -0.1184,\n",
      "         -0.0414,  0.0720,  0.0353,  0.0301, -0.0751, -0.0839, -0.1593, -0.0688,\n",
      "         -0.0765, -0.0483, -0.0982, -0.0978,  0.0591,  0.0688,  0.0509,  0.0667,\n",
      "         -0.0973, -0.0191, -0.0598,  0.0197, -0.1030,  0.0070,  0.0307, -0.0515,\n",
      "         -0.0859,  0.0548, -0.1774,  0.0593, -0.0747, -0.0431,  0.0803, -0.1623,\n",
      "         -0.0597, -0.1110, -0.1027, -0.0556, -0.0730,  0.0513, -0.0721, -0.1001,\n",
      "          0.0454, -0.0565],\n",
      "        [-0.0685, -0.1390, -0.1192,  0.1133, -0.1284, -0.1121,  0.0066, -0.1475,\n",
      "         -0.0869,  0.1495,  0.0684,  0.0020, -0.0865, -0.1356,  0.0050, -0.1560,\n",
      "         -0.1475, -0.2104, -0.1804, -0.2177,  0.0363,  0.0113, -0.2954, -0.0264,\n",
      "         -0.1493, -0.1276, -0.1610, -0.2115, -0.1165, -0.0363,  0.0067, -0.2071,\n",
      "         -0.1294,  0.0266, -0.1410, -0.2014, -0.1595, -0.1164, -0.0747,  0.0942,\n",
      "         -0.0639, -0.1467,  0.0029, -0.0844, -0.2130,  0.0198, -0.1264, -0.0462,\n",
      "          0.0334, -0.0924],\n",
      "        [ 0.0177,  0.0486, -0.0997,  0.0735, -0.1402, -0.0648,  0.0378, -0.0711,\n",
      "         -0.0723, -0.1191,  0.0449,  0.0704, -0.0901, -0.0748, -0.0931, -0.0836,\n",
      "         -0.0733, -0.0621, -0.0697, -0.0774, -0.0126,  0.0344,  0.0132,  0.0419,\n",
      "         -0.0863,  0.0579, -0.0681, -0.0390, -0.0914,  0.0800,  0.0483, -0.0598,\n",
      "         -0.0719,  0.0491,  0.0029, -0.0211, -0.0803, -0.0855,  0.0284, -0.0712,\n",
      "         -0.1173, -0.0876, -0.1286, -0.0982, -0.0766,  0.0256, -0.0783, -0.0791,\n",
      "          0.0260, -0.1036],\n",
      "        [ 0.0030, -0.0890, -0.0253, -0.0673, -0.1076, -0.1363, -0.0711, -0.1183,\n",
      "         -0.1306, -0.1625, -0.0205,  0.0579, -0.1118, -0.1202, -0.0538, -0.1204,\n",
      "         -0.1317, -0.1418, -0.1452, -0.0898,  0.0381,  0.0339, -0.2116,  0.0414,\n",
      "         -0.1314, -0.1976, -0.1377,  0.0801, -0.1327,  0.0362,  0.0347, -0.1217,\n",
      "         -0.1287,  0.0470,  0.0221, -0.0888, -0.1066, -0.1124, -0.0719,  0.0417,\n",
      "         -0.0987, -0.0632, -0.0492, -0.0653, -0.1667,  0.0019, -0.1045, -0.0784,\n",
      "          0.0316, -0.1357]], device='cuda:0', requires_grad=True))\n",
      "('projection.3.bias', Parameter containing:\n",
      "tensor([-2.0281, -4.3278, -2.6897, -5.4087, -2.7571, -4.4033], device='cuda:0',\n",
      "       requires_grad=True))\n"
     ]
    }
   ],
   "source": [
    "for p in model.named_parameters():\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import expit\n",
    "from collections import defaultdict\n",
    "from allennlp.common.tqdm import Tqdm\n",
    "\n",
    "def dict_append(d: Dict[str, List], upd: Dict[str, Any]) -> Dict[str, List]:\n",
    "    for k, v in upd.items(): d[k].append(v)\n",
    "\n",
    "def tonp(tsr): return tsr.detach().cpu().numpy()\n",
    "        \n",
    "class Predictor:\n",
    "    def __init__(self, model: Model, iterator: DataIterator,\n",
    "                 cuda_device: int=-1) -> None:\n",
    "        self.model = model\n",
    "        self.iterator = iterator\n",
    "        self.cuda_device = cuda_device\n",
    "        \n",
    "    def _extract_data(self, batch) -> Dict[str, np.ndarray]:\n",
    "        out_dict = self.model(**batch)\n",
    "        lens = tonp(get_text_field_mask(batch[\"tokens\"]).sum(1))\n",
    "        \n",
    "        if self.model.loss._get_name()==\"MBernLossWithLogits\":\n",
    "            preds = tonp(out_dict[\"marginal_probs\"])\n",
    "        else:\n",
    "            preds = expit(tonp(out_dict[\"class_logits\"]))\n",
    "        \n",
    "        return {\n",
    "                \"preds\": preds,\n",
    "                \"oov_ratio\": tonp((batch[\"tokens\"][\"tokens\"] == 1).sum(1)) / lens,\n",
    "                \"lens\": lens,\n",
    "               }\n",
    "        \n",
    "    def _postprocess(self, predictions: Dict[str, np.ndarray]) -> np.ndarray:\n",
    "        return {k: np.concatenate(v, axis=0) for k, v in predictions.items()}\n",
    "    \n",
    "    @gpu_mem_restore\n",
    "    def predict(self, ds: Iterable[Instance]) -> np.ndarray:\n",
    "        pred_generator = self.iterator(ds, num_epochs=1, shuffle=False)\n",
    "        self.model.eval()\n",
    "        pred_generator_tqdm = Tqdm.tqdm(pred_generator,\n",
    "                                        total=self.iterator.get_num_batches(ds))\n",
    "        preds = defaultdict(list)\n",
    "        with torch.no_grad():\n",
    "            for batch in pred_generator_tqdm:\n",
    "                batch = nn_util.move_to_device(batch, self.cuda_device)\n",
    "                dict_append(preds, self._extract_data(batch))\n",
    "        return self._postprocess(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.data.iterators import BasicIterator\n",
    "seq_iterator = BasicIterator(batch_size=64)\n",
    "seq_iterator.index_with(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Horrible solution to the shuffling problem with BasicIterator\n",
    "# TODO: Solve more elegantly?\n",
    "if not config.bucket:\n",
    "    del train_ds; import gc; gc.collect()\n",
    "    if config.val_ratio > 0.0:\n",
    "        train_ds = reader.read(DATA_ROOT / \"train_wo_val.csv\")\n",
    "    else:\n",
    "        train_ds = reader.read(DATA_ROOT / \"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2494/2494 [04:20<00:00, 10.51it/s]\n",
      "100%|██████████| 1000/1000 [01:54<00:00,  6.78it/s]\n"
     ]
    }
   ],
   "source": [
    "predictor = Predictor(model, seq_iterator, cuda_device=0 if USE_GPU else -1)\n",
    "train_meta = predictor.predict(train_ds) \n",
    "train_preds = train_meta.pop(\"preds\")\n",
    "test_meta = predictor.predict(test_ds)\n",
    "test_preds = test_meta.pop(\"preds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst_df = pd.read_csv(DATA_ROOT / \"test_proced.csv\")\n",
    "test_labels = tst_df[label_cols].values\n",
    "test_texts = tst_df[\"comment_text\"].values\n",
    "if config.testing:\n",
    "    test_labels = test_labels[:len(test_ds), :]\n",
    "    test_texts = test_texts[:len(test_ds)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, f1_score, accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluator:\n",
    "    def __init__(self, thres=0.5):\n",
    "        if isinstance(thres, float):\n",
    "            self.thres = np.ones(len(label_cols)) * thres\n",
    "        else:\n",
    "            self.thres = thres\n",
    "    \n",
    "    def _to_metric_dict(self, t: np.ndarray, y: np.ndarray, thres: float) -> Dict:\n",
    "        tn, fp, fn, tp = confusion_matrix(t, y >= thres).ravel()\n",
    "        return {\"auc\": roc_auc_score(t, y),\n",
    "                \"f1\": f1_score(t, y >= thres),\n",
    "                \"acc\": accuracy_score(t, y >= thres),\n",
    "                \"tnr\": tn / len(t), \"fpr\": fp / len(t),\n",
    "                \"fnr\": fn / len(t), \"tpr\": tp / len(t),\n",
    "                \"precision\": tp / (tp + fp), \"recall\": tp / (tp + fn),\n",
    "        }\n",
    "\n",
    "    def _stats_per_quadrant(self, tgt, preds, \n",
    "                            metadata: Dict[str, np.ndarray],\n",
    "                            texts: np.ndarray=None):\n",
    "        out_data = {}\n",
    "        for i, lbl in enumerate(label_cols):\n",
    "            # get indicies of each quadrant`\n",
    "            preds_bin = preds[:, i] >= self.thres[i]\n",
    "            quads = {\n",
    "                \"tp\": np.where((tgt[:, i] == 1) & preds_bin)[0],\n",
    "                \"fp\": np.where((tgt[:, i] == 0) & preds_bin)[0],\n",
    "                \"tn\": np.where((tgt[:, i] == 0) & ~preds_bin)[0],\n",
    "                \"fn\": np.where((tgt[:, i] == 1) & ~preds_bin)[0],\n",
    "            }\n",
    "            \n",
    "            # get stats for metadata\n",
    "            out_data[lbl] = {}\n",
    "            for quad, qidxs in quads.items():\n",
    "                quad_data = {}\n",
    "                for k, full_data in metadata.items():\n",
    "                    data = full_data[qidxs]\n",
    "                    for metric in [\"mean\", \"std\", \"min\", \"max\"]:\n",
    "                        if len(data) > 0:\n",
    "                            quad_data[f\"{k}_{metric}\"] = getattr(data, metric)()\n",
    "                        else:\n",
    "                            quad_data[f\"{k}_{metric}\"] = np.nan\n",
    "\n",
    "                out_data[lbl][quad] = quad_data\n",
    "            \n",
    "            # do error analysis\n",
    "            if texts is not None:\n",
    "                for quad, qidxs in quads.items():\n",
    "                    quad_preds = preds[qidxs, i]\n",
    "                    if len(quad_preds) == 0: continue\n",
    "                    if quad in [\"tp\", \"fp\"]:\n",
    "                        out_data[lbl][quad][\"most_confident\"] = texts[qidxs[quad_preds.argmax()]]\n",
    "                        out_data[lbl][quad][\"most_confident_prob\"] = quad_preds.max()\n",
    "                        out_data[lbl][quad][\"least_confident\"] = texts[qidxs[quad_preds.argmin()]]\n",
    "                        out_data[lbl][quad][\"least_confident_prob\"] = quad_preds.min()\n",
    "                    else:\n",
    "                        out_data[lbl][quad][\"most_confident\"] = texts[qidxs[quad_preds.argmin()]]\n",
    "                        out_data[lbl][quad][\"most_confident_prob\"] = quad_preds.min()\n",
    "                        out_data[lbl][quad][\"least_confident\"] = texts[qidxs[quad_preds.argmax()]]\n",
    "                        out_data[lbl][quad][\"least_confident_prob\"] = quad_preds.max()\n",
    "        return out_data        \n",
    "    \n",
    "    @gpu_mem_restore\n",
    "    def evaluate(self, tgt: np.ndarray, preds: np.ndarray,\n",
    "                 trn_tgt: np.ndarray, trn_preds: np.ndarray,\n",
    "                 metadata: Dict[str, np.ndarray]={}, \n",
    "                 texts: np.ndarray=None) -> Dict:\n",
    "        \"\"\"\n",
    "        Metadata: Data about the inputs (e.g. length, OOV ratio)\n",
    "        \"\"\"\n",
    "        train_label_metrics = {}\n",
    "        label_metrics = {}\n",
    "                \n",
    "        # get per-label stats\n",
    "        for i, lbl in enumerate(label_cols):\n",
    "            train_label_metrics[lbl] = self._to_metric_dict(trn_tgt[:, i],\n",
    "                                                            trn_preds[:, i],\n",
    "                                                            self.thres[i])\n",
    "            label_metrics[lbl] = self._to_metric_dict(tgt[:, i], preds[:, i],\n",
    "                                                      self.thres[i])\n",
    "            print(f\"========{lbl}=========\")\n",
    "            print(label_metrics[lbl])\n",
    "        \n",
    "        # get global stats\n",
    "        for mtrc in label_metrics[\"toxic\"].keys():\n",
    "            label_metrics[f\"global_{mtrc}\"] = \\\n",
    "                np.mean([label_metrics[col][mtrc] for col in label_cols])\n",
    "            \n",
    "        # get per-label-quadrant stats\n",
    "        quad_stats = self._stats_per_quadrant(tgt, preds, metadata=metadata, texts=texts)\n",
    "        if len(quad_stats) > 0:\n",
    "            for c in label_cols:\n",
    "                label_metrics[c][\"quad_stats\"] = quad_stats[c]\n",
    "\n",
    "        label_metrics[\"train\"] = train_label_metrics,\n",
    "        return label_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anna/neuralnlp/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# Compute best threshold based on training data\n",
    "if config.compute_thres_on_test:\n",
    "    lbls, pds = test_labels, test_preds\n",
    "else:\n",
    "    lbls, pds = train_labels, train_preds\n",
    "    \n",
    "thres = np.zeros(len(label_cols))\n",
    "best_scores = np.zeros(len(label_cols))\n",
    "for i, col in enumerate(label_cols):\n",
    "    best_score = -1\n",
    "    best_thres = -1\n",
    "    for x in np.linspace(0, 1.0, num=999):\n",
    "        scr = f1_score(lbls[:, i], pds[:, i] >= x)\n",
    "        if scr > best_score:\n",
    "            best_thres = x\n",
    "            best_score = scr\n",
    "    thres[i] = best_thres\n",
    "    best_scores[i] = best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.43587174, 0.37374749, 0.43887776, 0.42685371, 0.42184369,\n",
       "       0.40581162])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========toxic=========\n",
      "{'auc': 0.9723433226700832, 'f1': 0.6739305392187209, 'acc': 0.9176748257213417, 'tnr': 0.8325987058051205, 'fpr': 0.07221232298602645, 'fnr': 0.010112851292631841, 'tpr': 0.0850761199162212, 'precision': 0.5408923780184836, 'recall': 0.89376026272578}\n",
      "========severe_toxic=========\n",
      "{'auc': 0.9902904819514148, 'f1': 0.4338807260155575, 'acc': 0.9897621057238426, 'tnr': 0.9858388821157272, 'fpr': 0.00842477101503642, 'fnr': 0.0018131232611210103, 'tpr': 0.00392322360811529, 'precision': 0.31772151898734174, 'recall': 0.6839237057220708}\n",
      "========obscene=========\n",
      "{'auc': 0.9813568814791929, 'f1': 0.693617514189737, 'acc': 0.9586576635718528, 'tnr': 0.9118603269874019, 'fpr': 0.030447966488480416, 'fnr': 0.01089436993966676, 'tpr': 0.046797336584450906, 'precision': 0.6058276001618778, 'recall': 0.8111622866431861}\n",
      "========threat=========\n",
      "{'auc': 0.9947407018011442, 'f1': 0.5450733752620546, 'acc': 0.9966082090718684, 'tnr': 0.9945762605895777, 'fpr': 0.0021257307199349777, 'fnr': 0.0012660602081965675, 'tpr': 0.0020319484822907876, 'precision': 0.48872180451127817, 'recall': 0.6161137440758294}\n",
      "========insult=========\n",
      "{'auc': 0.9797194402997236, 'f1': 0.6904123579726796, 'acc': 0.9620963456188064, 'tnr': 0.9198318171871581, 'fpr': 0.026602894745068616, 'fnr': 0.011300759636124917, 'tpr': 0.04226452843164838, 'precision': 0.613708579210168, 'recall': 0.7890283046396265}\n",
      "========identity_hate=========\n",
      "{'auc': 0.9891014157452555, 'f1': 0.6355987055016181, 'acc': 0.9912001000343869, 'tnr': 0.983525586920504, 'fpr': 0.0053455875457188405, 'fnr': 0.0034543124198943387, 'tpr': 0.007674513113882897, 'precision': 0.5894357743097239, 'recall': 0.6896067415730337}\n"
     ]
    }
   ],
   "source": [
    "evaluator = Evaluator(thres=thres)\n",
    "label_metrics = evaluator.evaluate(\n",
    "    test_labels, test_preds,\n",
    "    train_labels, train_preds,\n",
    "    metadata=test_meta, texts=test_texts,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9845920406578025"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_metrics[\"global_auc\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'toxic': {'auc': 0.9723433226700832,\n",
       "  'f1': 0.6739305392187209,\n",
       "  'acc': 0.9176748257213417,\n",
       "  'tnr': 0.8325987058051205,\n",
       "  'fpr': 0.07221232298602645,\n",
       "  'fnr': 0.010112851292631841,\n",
       "  'tpr': 0.0850761199162212,\n",
       "  'precision': 0.5408923780184836,\n",
       "  'recall': 0.89376026272578,\n",
       "  'quad_stats': {'tp': {'oov_ratio_mean': 0.0,\n",
       "    'oov_ratio_std': 0.0,\n",
       "    'oov_ratio_min': 0.0,\n",
       "    'oov_ratio_max': 0.0,\n",
       "    'lens_mean': 52.09810766121624,\n",
       "    'lens_std': 140.31583204633995,\n",
       "    'lens_min': 1,\n",
       "    'lens_max': 4950,\n",
       "    'most_confident': 'FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK',\n",
       "    'most_confident_prob': 0.9937523,\n",
       "    'least_confident': '\"Could you be any more ignorant about this issue? \"\"Normalizing a regressive tax\"\" is a progressive tax. You declaring otherwise is just sophism and idiocy.    \\n :\"',\n",
       "    'least_confident_prob': 0.43602708},\n",
       "   'fp': {'oov_ratio_mean': 0.0,\n",
       "    'oov_ratio_std': 0.0,\n",
       "    'oov_ratio_min': 0.0,\n",
       "    'oov_ratio_max': 0.0,\n",
       "    'lens_mean': 66.43766233766233,\n",
       "    'lens_std': 140.0636783495412,\n",
       "    'lens_min': 1,\n",
       "    'lens_max': 3307,\n",
       "    'most_confident': 'fuck u ill do whatever I fucking want boy',\n",
       "    'most_confident_prob': 0.9614954,\n",
       "    'least_confident': 'תן לי את סיסמתך',\n",
       "    'least_confident_prob': 0.43588886},\n",
       "   'tn': {'oov_ratio_mean': 0.0,\n",
       "    'oov_ratio_std': 0.0,\n",
       "    'oov_ratio_min': 0.0,\n",
       "    'oov_ratio_max': 0.0,\n",
       "    'lens_mean': 85.4778854096268,\n",
       "    'lens_std': 121.57968926109325,\n",
       "    'lens_min': 1,\n",
       "    'lens_max': 1640,\n",
       "    'most_confident': 'GA Review \\n\\n :This review is transcluded from Talk:Flight Unlimited/GA1. The edit link for this section can be used to add comments to the review. \\n\\n Reviewer: Creating review page.',\n",
       "    'most_confident_prob': 0.014355209,\n",
       "    'least_confident': \":Fine, as long as he doesn't do anything so blitheringly stupid again. Mass WP:NONFREE violation from an admin?! -\",\n",
       "    'least_confident_prob': 0.43576577},\n",
       "   'fn': {'oov_ratio_mean': 0.0,\n",
       "    'oov_ratio_std': 0.0,\n",
       "    'oov_ratio_min': 0.0,\n",
       "    'oov_ratio_max': 0.0,\n",
       "    'lens_mean': 59.343122102009275,\n",
       "    'lens_std': 90.8152482523533,\n",
       "    'lens_min': 3,\n",
       "    'lens_max': 1505,\n",
       "    'most_confident': '\" \\n\\n ==Suggestions== \\n hi, i found the article about you using the random article button. i noticed when i was reading it that it seemed too long and so i went to the talk page to see if there was a reason. i noticed that some editors had made really uncivil comments, and i decided to stick around to convince you that the wikipedia system is not against you, nor do all wikipedians do that sort of thing. i request that you ignore what the other editors said about your, um, sanity (not to imply that you\\'d be interested in pursuing such pettiness of course!). \\n\\n as for the article itself, i think it is definitely too long. i suggest offhand deleting the entire early life section or summarizing it like \"\"you always were fascinated with physical and social science\"\" and not distracting the focus from what you\\'ve done to deserve an encyclopedia article. don\\'t worry, you\\'re a published author, what you did way back when will not be lost just because it\\'s not included on wikipedia. in any event, removing the early life section will give them a lot less pretext to make stupid comments and will probably make the cleanup crew really happy.  \"',\n",
       "    'most_confident_prob': 0.053819165,\n",
       "    'least_confident': \"*Going out of your way to add the photos to cause disruption, the snide remarks, this is all pure trolling and frankly, the community isn't here to be the butt of your joke.  This kind of sheer stupidity in behavior has a parasitic effect on the time of good editors and I have zero patience for it.  If it continues, an indef block will continue. \\xa0|\\xa0\\xa0|\\xa0WER\",\n",
       "    'least_confident_prob': 0.43571183}}},\n",
       " 'severe_toxic': {'auc': 0.9902904819514148,\n",
       "  'f1': 0.4338807260155575,\n",
       "  'acc': 0.9897621057238426,\n",
       "  'tnr': 0.9858388821157272,\n",
       "  'fpr': 0.00842477101503642,\n",
       "  'fnr': 0.0018131232611210103,\n",
       "  'tpr': 0.00392322360811529,\n",
       "  'precision': 0.31772151898734174,\n",
       "  'recall': 0.6839237057220708,\n",
       "  'quad_stats': {'tp': {'oov_ratio_mean': 0.0,\n",
       "    'oov_ratio_std': 0.0,\n",
       "    'oov_ratio_min': 0.0,\n",
       "    'oov_ratio_max': 0.0,\n",
       "    'lens_mean': 168.4621513944223,\n",
       "    'lens_std': 297.25282939514517,\n",
       "    'lens_min': 3,\n",
       "    'lens_max': 1345,\n",
       "    'most_confident': '== fuck you mother fucker. == \\n\\n fuck you mother fucker.fuck you mother fucker. fuck you mother fucker.fuck you mother fucker. fuck you mother fucker.fuck you mother fucker. fuck you mother fucker.fuck you mother fucker. fuck you mother fucker.fuck you mother fucker. fuck you mother fucker.fuck you mother fucker. fuck you mother fucker.fuck you mother fucker. fuck you mother fucker.fuck you mother fucker. fuck you mother fucker.fuck you mother fucker. fuck you mother fucker.fuck you mother fucker. fuck you mother fucker.fuck you mother fucker. fuck you mother fucker.fuck you mother fucjer. fuck you mother fucker.fuck you mother fucker. fuck you mother fucker.fuck you mother fucker. fuck you mother fucker.fuck you mother fucker. fuck you mother fucker.fuck you mother fucker. fuck you mother fucker.fuck you mother fucker. fuck you mother fucker.fuck you mother fucker. fuck you mother fucker.fuck you mother fucker. fuck you mother fucker.fuck you mother fucker. fuck you mother fucker.fuck you mother fucker. fuck you mother fucker.fuck you mother fucker. fuck you mother fucker.fuck you mother fucker. fuck you mother fucker.fuck you mother fucker. fuck you mother fucker.fuck you mother fucker. fuck you mother fucker.fuck you mother fucker. fuck you mother fucker.fuck you mother fucker. fuck you mother fucker.fuck you mother fucker. fuck you mother fucker.fuck you mother fucker. fuck you mother fucker.fuck you mother fucker. fuck you mother fucker.fuck you mother fucker. fuck you mother fucker.fuck you mother fucker. fuck you mother fucker.fuck you mother fucker. fuck you mother fucker.fuck you mother fucker. fuck you mother fucker.fuck you mother fucker. fuck you mother fucker.fuck you mother fucker. fuck you mother fucker.fuck you mother fucker. fuck you mother fucker.fuck you mother fucker. fuck you mother fucker.fuck you mother fucker. fuck you mother fucker.fuck you mother fucker. fuck you mother fucker.fuck you mother fucker. fuck you mother fucker.fuck you mother fucker. fuck you mother fucker.fuck you mother fucker. fuck you mother fucker.fuck you mother fucker. fuck you mother fucker.fuck you mother fucker. fuck you mother fucker.fuck you mother fucker. fuck you mother fucker.fuck you mother fucker. fuck you mother fucker.fuck you mother fucker. fuck you mother fucker.fuck you mother fucker. fuck you mother fucker.fuck you mother fucker. fuck you mother fucker.fuck you mother fucker. fuck you mother fucker.fuck you mother fucker. fuck you mother fucker.fuck you mother fucker. fuck you mother fucker.fuck you mother fucker. fuck you mother fucker.fuck you mother fucker. fuck you mother fucker.fuck you mother fucker. fuck you mother fucker.fuck you mother fucker. fuck you mother fucker.fuck you mother fucker. fuck you mother fucker.fuck you mother fucker. fuck you mother fucker.fuck you mother fucker. fuck you mother fucker.fuck you mother fucker. fuck you mother fucker.fuck you mother fucker. fuck you mother fucker.fuck you mother fucker. fuck you mother fucker.fuck you mother fucker. fuck you mother fucker.fuck you mother fucker. fuck you mother fucker.fuck you mother fucker. fuck you mother fucker.fuck you mother fucker. fuck you mother fucker.fuck you mother fucker. fuck you mother fucker.fuck you mother fucker. fuck you mother fucker.fuck you mother fucker. fuck you mother fucker.fuck you mother fucker. fuck you mother fucker.fuck you mother fucker. fuck you mother fucker.fuck you mother fucker. fuck you mother fucker.fuck you mother fucker. fuck you mother fucker.fuck you mother fucker. fuck you mother fucker.fuck you mother fucker. fuck you mother fucker.fuck you mother fucker. fuck you mother fucker.fuck you mother fucker. fuck you mother fucker.fuck you mother fucker. fuck you mother fucker.fuck you mother fucker. fuck you mother fucker.fuck you mother fucker. fuck you mother fucker.fuck you mother fucker. fuck you mother fucker.fuck you mother fucker. fuck you mother fucker.fuck you mother fucker. fuck you mother fucker.fuck you mother fucker. fuck you mother fucker.fuck you mother fucker. fuck you mother fucker.fuck you mother fucker. fuck you mother fucker.fuck you mother fucker. fuck you mother fucker.fuck you mother fucker. fuck you mother fucker.fuck you mother fucker. fuck you mother fucker.fuck you mother fucker. fuck you mother fucker.fuck you mother fucker. fuck you mother fucker.fuck you mother fucker. fuck you mother fucker.fuck you mother fucker. fuck you mother fucker.fuck you mother fucker. fuck you mother fucker.fuck you mother fucker. fuck you mother fucker.fuck you mother fucker. fuck you mother fucker.fuck you mother fucker. fuck you mother fucker.fuck you mother fucker. fuck you mother fucker.fuck you mother fucker. fuck you mother fucker.fuck you mother fucker. fuck you mother fucker.fuck you mother fucker. fuck you mother fucker.fuck you mother fucker. fuck you mother fucker.fuck you mother fucker. fuck you mother fucker.fuck you mother fucker. fuck you',\n",
       "    'most_confident_prob': 0.68688625,\n",
       "    'least_confident': 'YOU GAY PRICK, WHY THE FUCK DID U REVERT MY EDIT, ALL MUSLIM TERRORISTS SHOULD BE NUKED',\n",
       "    'least_confident_prob': 0.37603047},\n",
       "   'fp': {'oov_ratio_mean': 0.0,\n",
       "    'oov_ratio_std': 0.0,\n",
       "    'oov_ratio_min': 0.0,\n",
       "    'oov_ratio_max': 0.0,\n",
       "    'lens_mean': 74.78664192949907,\n",
       "    'lens_std': 200.17612126503988,\n",
       "    'lens_min': 2,\n",
       "    'lens_max': 2321,\n",
       "    'most_confident': 'FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH vFUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH vFUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH FUCK YOU BITCH vFUCK YOU BITCH F',\n",
       "    'most_confident_prob': 0.74239516,\n",
       "    'least_confident': '== Fuck out of my sight! == \\n\\n Hello, Loser \\n\\n Fuck out of my sight! Shit! Never let me find on earth!',\n",
       "    'least_confident_prob': 0.37401262},\n",
       "   'tn': {'oov_ratio_mean': 0.0,\n",
       "    'oov_ratio_std': 0.0,\n",
       "    'oov_ratio_min': 0.0,\n",
       "    'oov_ratio_max': 0.0,\n",
       "    'lens_mean': 80.65637683916793,\n",
       "    'lens_std': 122.4799560667611,\n",
       "    'lens_min': 1,\n",
       "    'lens_max': 4950,\n",
       "    'most_confident': '\" \\n\\n Thank you for experimenting with the page Wikipedia talk:Pokémon Collaborative Project on Wikipedia. Your test worked, and has been reverted or removed. Please use the sandbox for any other tests you want to do. Take a look at the welcome page if you would like to learn more about contributing to our encyclopedia. Thank you for your understanding.    \"',\n",
       "    'most_confident_prob': 0.0018524572,\n",
       "    'least_confident': '== bitch == \\n\\n your a bitch.',\n",
       "    'least_confident_prob': 0.37333605},\n",
       "   'fn': {'oov_ratio_mean': 0.0,\n",
       "    'oov_ratio_std': 0.0,\n",
       "    'oov_ratio_min': 0.0,\n",
       "    'oov_ratio_max': 0.0,\n",
       "    'lens_mean': 106.8103448275862,\n",
       "    'lens_std': 216.83884504527956,\n",
       "    'lens_min': 4,\n",
       "    'lens_max': 1032,\n",
       "    'most_confident': \"== Why all muslims should be burned alive == \\n\\n Because they deserve it.  \\n\\n We hope to finish off some 800 million muslims by the year 2010.  \\n That's a lot of killing to do. But we will sure enjoy it.  \\n In the name of the United States of America in the name of freedom this must be accomplished.  \\n\\n Post your comments on how best to achieve this.\",\n",
       "    'most_confident_prob': 0.052498933,\n",
       "    'least_confident': '== FUCK WIKIPEDIA, FUCK IT HARD FUCK IT GOOD ==',\n",
       "    'least_confident_prob': 0.37200934}}},\n",
       " 'obscene': {'auc': 0.9813568814791929,\n",
       "  'f1': 0.693617514189737,\n",
       "  'acc': 0.9586576635718528,\n",
       "  'tnr': 0.9118603269874019,\n",
       "  'fpr': 0.030447966488480416,\n",
       "  'fnr': 0.01089436993966676,\n",
       "  'tpr': 0.046797336584450906,\n",
       "  'precision': 0.6058276001618778,\n",
       "  'recall': 0.8111622866431861,\n",
       "  'quad_stats': {'tp': {'oov_ratio_mean': 0.0,\n",
       "    'oov_ratio_std': 0.0,\n",
       "    'oov_ratio_min': 0.0,\n",
       "    'oov_ratio_max': 0.0,\n",
       "    'lens_mean': 60.26686706746827,\n",
       "    'lens_std': 173.3083846310902,\n",
       "    'lens_min': 1,\n",
       "    'lens_max': 4950,\n",
       "    'most_confident': 'FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU',\n",
       "    'most_confident_prob': 0.9491349,\n",
       "    'least_confident': ', like with the idea that people of Black African descent are less intelligent because of larger penis size',\n",
       "    'least_confident_prob': 0.43902698},\n",
       "   'fp': {'oov_ratio_mean': 0.0,\n",
       "    'oov_ratio_std': 0.0,\n",
       "    'oov_ratio_min': 0.0,\n",
       "    'oov_ratio_max': 0.0,\n",
       "    'lens_mean': 70.89271047227926,\n",
       "    'lens_std': 141.07785902918675,\n",
       "    'lens_min': 1,\n",
       "    'lens_max': 1990,\n",
       "    'most_confident': 'fuck u ill do whatever I fucking want boy',\n",
       "    'most_confident_prob': 0.8924915,\n",
       "    'least_confident': \":::Wait Gay sex between 15 year olds - dosent that mean he's accusing bully of containing child pornography?\",\n",
       "    'least_confident_prob': 0.43897653},\n",
       "   'tn': {'oov_ratio_mean': 0.0,\n",
       "    'oov_ratio_std': 0.0,\n",
       "    'oov_ratio_min': 0.0,\n",
       "    'oov_ratio_max': 0.0,\n",
       "    'lens_mean': 82.79735682819383,\n",
       "    'lens_std': 121.5431151876762,\n",
       "    'lens_min': 1,\n",
       "    'lens_max': 3307,\n",
       "    'most_confident': 'GA Review \\n\\n :This review is transcluded from Talk:Flight Unlimited/GA1. The edit link for this section can be used to add comments to the review. \\n\\n Reviewer: Creating review page.',\n",
       "    'most_confident_prob': 0.011632719,\n",
       "    'least_confident': \"In that case, you didn't HAVE to delete every image I spent time uploading to this Wiki, did you? Well, according to what you just said, I am perfectly allowed to upload two images onto this Wiki, now aren't I? And that's exactly what I plan on doing now. If you delete them, I'll get head offices to fire your ass, OK?\",\n",
       "    'least_confident_prob': 0.4387877},\n",
       "   'fn': {'oov_ratio_mean': 0.0,\n",
       "    'oov_ratio_std': 0.0,\n",
       "    'oov_ratio_min': 0.0,\n",
       "    'oov_ratio_max': 0.0,\n",
       "    'lens_mean': 47.761836441893834,\n",
       "    'lens_std': 79.91726770211689,\n",
       "    'lens_min': 3,\n",
       "    'lens_max': 946,\n",
       "    'most_confident': \"has a big  [[dick'''Bold text']]\",\n",
       "    'most_confident_prob': 0.053582072,\n",
       "    'least_confident': \"I'll kill and piss in the mouth of anyone who supports Soviet scum, and furthermore support their murder in public.\",\n",
       "    'least_confident_prob': 0.43886602}}},\n",
       " 'threat': {'auc': 0.9947407018011442,\n",
       "  'f1': 0.5450733752620546,\n",
       "  'acc': 0.9966082090718684,\n",
       "  'tnr': 0.9945762605895777,\n",
       "  'fpr': 0.0021257307199349777,\n",
       "  'fnr': 0.0012660602081965675,\n",
       "  'tpr': 0.0020319484822907876,\n",
       "  'precision': 0.48872180451127817,\n",
       "  'recall': 0.6161137440758294,\n",
       "  'quad_stats': {'tp': {'oov_ratio_mean': 0.0,\n",
       "    'oov_ratio_std': 0.0,\n",
       "    'oov_ratio_min': 0.0,\n",
       "    'oov_ratio_max': 0.0,\n",
       "    'lens_mean': 83.64615384615385,\n",
       "    'lens_std': 183.75953608069355,\n",
       "    'lens_min': 2,\n",
       "    'lens_max': 1169,\n",
       "    'most_confident': 'I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.[[I am going to rip off your tiny balls and decapitate you.',\n",
       "    'most_confident_prob': 0.73234886,\n",
       "    'least_confident': 'kill ya niggas i hate fuckin black people klue kluxs klann hell yea!!!!!!!!!!!!!!!!!!',\n",
       "    'least_confident_prob': 0.42868283},\n",
       "   'fp': {'oov_ratio_mean': 0.0,\n",
       "    'oov_ratio_std': 0.0,\n",
       "    'oov_ratio_min': 0.0,\n",
       "    'oov_ratio_max': 0.0,\n",
       "    'lens_mean': 72.63235294117646,\n",
       "    'lens_std': 167.67772874771734,\n",
       "    'lens_min': 2,\n",
       "    'lens_max': 1194,\n",
       "    'most_confident': 'KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF KILL YOURSELF',\n",
       "    'most_confident_prob': 0.7632959,\n",
       "    'least_confident': '==What Neth is== \\n\\n Can Neth be killed',\n",
       "    'least_confident_prob': 0.42761344},\n",
       "   'tn': {'oov_ratio_mean': 0.0,\n",
       "    'oov_ratio_std': 0.0,\n",
       "    'oov_ratio_min': 0.0,\n",
       "    'oov_ratio_max': 0.0,\n",
       "    'lens_mean': 81.03429146170892,\n",
       "    'lens_std': 124.55687798564553,\n",
       "    'lens_min': 1,\n",
       "    'lens_max': 4950,\n",
       "    'most_confident': '\" \\n\\n Thank you for experimenting with the page Wikipedia talk:Pokémon Collaborative Project on Wikipedia. Your test worked, and has been reverted or removed. Please use the sandbox for any other tests you want to do. Take a look at the welcome page if you would like to learn more about contributing to our encyclopedia. Thank you for your understanding.    \"',\n",
       "    'most_confident_prob': 0.0014023561,\n",
       "    'least_confident': 'Did hitler die a virgin',\n",
       "    'least_confident_prob': 0.42643705},\n",
       "   'fn': {'oov_ratio_mean': 0.0,\n",
       "    'oov_ratio_std': 0.0,\n",
       "    'oov_ratio_min': 0.0,\n",
       "    'oov_ratio_max': 0.0,\n",
       "    'lens_mean': 62.93827160493827,\n",
       "    'lens_std': 156.03864238707123,\n",
       "    'lens_min': 4,\n",
       "    'lens_max': 946,\n",
       "    'most_confident': '\" \\n\\n == Child Assasin == \\n\\n I overheard in an interview during the 2008 Olympics in Beijing that when asked why she was sometimes called the \"\"Child Assasin\"\", she simply replied, \"\"\\'Cuz Imma Kill this shit!\"\".\"',\n",
       "    'most_confident_prob': 0.055018183,\n",
       "    'least_confident': '\" \\n\\n == Fucker == \\n You got brains huh bastard, i was actually waiting for you to reply to my \"\"apologizing\"\" message and after that i would insult u back so it would astound you. But you have brains u bastard 44 yr old impotent uncle. You need to be impaled from your anus so that the well \"\"oiled\"\" comes out of your mouth. You cannot even cry because it will be so painful. You will writhe in pain for weeks before you die a slow and painful death. I HAVE DECLARED WAR ON YOU   ) \\n\\n \\'\\'\\'This is not the last you have seen of  and .\"',\n",
       "    'least_confident_prob': 0.4200727}}},\n",
       " 'insult': {'auc': 0.9797194402997236,\n",
       "  'f1': 0.6904123579726796,\n",
       "  'acc': 0.9620963456188064,\n",
       "  'tnr': 0.9198318171871581,\n",
       "  'fpr': 0.026602894745068616,\n",
       "  'fnr': 0.011300759636124917,\n",
       "  'tpr': 0.04226452843164838,\n",
       "  'precision': 0.613708579210168,\n",
       "  'recall': 0.7890283046396265,\n",
       "  'quad_stats': {'tp': {'oov_ratio_mean': 0.0,\n",
       "    'oov_ratio_std': 0.0,\n",
       "    'oov_ratio_min': 0.0,\n",
       "    'oov_ratio_max': 0.0,\n",
       "    'lens_mean': 57.365754437869825,\n",
       "    'lens_std': 144.62275869709663,\n",
       "    'lens_min': 1,\n",
       "    'lens_max': 2321,\n",
       "    'most_confident': \"::Damn you cocksuckers! FUCK YOU AND YOUR MOM. I HOPE ALL OF YOU DIE AND GO TO HELL. GO LICK PUSSY YOU DIRTY PIECE OF SHIT FUCK YOU NIGGA FUCK YOU ASSSHIT I HOPE YOU DIE YOU DIRTY PIECE OF SHIT . YOU'RE SO ULGY FUCK YOU GET A FUCKING LIFE ASS FUCK FUCK FUCK FUCK FUCK YOU COCKSUCKER YOU FUCKING FAG PUSSY BITCH NIGGA FUCK YOU FUCKING CRACKER\",\n",
       "    'most_confident_prob': 0.86402094,\n",
       "    'least_confident': '\" \\n **(< Unindent) Heh... If I look up \"\"asshole\"\", I expect to see an asshole. If I look up \"\"autofellatio,\"\" I expect to see something right next to an asshole.  The point of my previous post was that I don\\'t find the comparison to \"\"necrophilia\"\", for example, credible, because I\\'ve been around here, and I\\'ve seen a lot, and I just don\\'t buy that there\\'s going to be an issue over a corpse-fucking video. I don\\'t buy it! Not in this world. Maybe after 2012. Maybe after the lobster-aliens take over. I dunno. It seems to me to be a strained comparison, because I\\'ve seen nothing in reality that makes me believe in it. Not on the Abortion page; not anywhere. - \"',\n",
       "    'least_confident_prob': 0.42205277},\n",
       "   'fp': {'oov_ratio_mean': 0.0,\n",
       "    'oov_ratio_std': 0.0,\n",
       "    'oov_ratio_min': 0.0,\n",
       "    'oov_ratio_max': 0.0,\n",
       "    'lens_mean': 60.588719153936545,\n",
       "    'lens_std': 145.16782723147298,\n",
       "    'lens_min': 1,\n",
       "    'lens_max': 1990,\n",
       "    'most_confident': '== Edit request on 14 January 2013 == \\n\\n  \\n\\n  \\n\\n    \\n Cheef keef is a bitch ass dude. He is not about that life and hes a hoe ass dude. He didnt kill little Jojo cause he a scared ass hoe, fuck you. you claim you a G and a blood. You stil a fucking hoe. When My Niggas catch you its sad cause you a bitch. Your mom going to get murderd and your daddy going to get raped',\n",
       "    'most_confident_prob': 0.78195393,\n",
       "    'least_confident': '\" \\n\\n PENIS PENIS PENIS   \"',\n",
       "    'least_confident_prob': 0.42186862},\n",
       "   'tn': {'oov_ratio_mean': 0.0,\n",
       "    'oov_ratio_std': 0.0,\n",
       "    'oov_ratio_min': 0.0,\n",
       "    'oov_ratio_max': 0.0,\n",
       "    'lens_mean': 83.07228669985896,\n",
       "    'lens_std': 123.41469845223304,\n",
       "    'lens_min': 1,\n",
       "    'lens_max': 4950,\n",
       "    'most_confident': '\" \\n\\n Welcome! \\n\\n Hello, and welcome to Wikipedia! Thank you for . I hope you like the place and decide to stay. Here are some pages you might like to see: \\n\\n * The five pillars of Wikipedia \\n * Help pages \\n * Tutorial \\n * How to edit a page and How to develop articles \\n * How to create your first article (using the Article Wizard if you wish) \\n * Manual of Style \\n\\n You are welcome to continue editing articles without logging in, but many editors recommend that you [ create an account]. Doing so is free, requires no personal information, and provides several benefits such as the ability to create articles. For a full outline and explanation of the benefits that come with creating an account, please see this page. If you edit without a username, your IP address (124.168.113.77) is used to identify you instead. \\n\\n In any case, I hope you enjoy editing here and being a Wikipedian! Please sign your comments on talk pages using four tildes (~~~~); this will automatically produce your IP address (or username if you\\'re logged in) and the date. If you need help, check out Wikipedia:Questions, ask me on , or ask your question and then place {{helpme}} before the question on this page. Again, welcome!    \"',\n",
       "    'most_confident_prob': 0.0067290547,\n",
       "    'least_confident': \", Entertainment Attorney's, Exclusive Celebrites Scandels&sex; Scandels, Women Dat'Have Had Some Dick-You Know Some Penis-Stuck In Their Ass In There Lifetime.............\",\n",
       "    'least_confident_prob': 0.4217149},\n",
       "   'fn': {'oov_ratio_mean': 0.0,\n",
       "    'oov_ratio_std': 0.0,\n",
       "    'oov_ratio_min': 0.0,\n",
       "    'oov_ratio_max': 0.0,\n",
       "    'lens_mean': 48.66251728907331,\n",
       "    'lens_std': 86.649894543321,\n",
       "    'lens_min': 3,\n",
       "    'lens_max': 1539,\n",
       "    'most_confident': 'Rush In (Where Angels Fear to Tread)|Fools',\n",
       "    'most_confident_prob': 0.058641903,\n",
       "    'least_confident': 'Dude like era sucks',\n",
       "    'least_confident_prob': 0.42154267}}},\n",
       " 'identity_hate': {'auc': 0.9891014157452555,\n",
       "  'f1': 0.6355987055016181,\n",
       "  'acc': 0.9912001000343869,\n",
       "  'tnr': 0.983525586920504,\n",
       "  'fpr': 0.0053455875457188405,\n",
       "  'fnr': 0.0034543124198943387,\n",
       "  'tpr': 0.007674513113882897,\n",
       "  'precision': 0.5894357743097239,\n",
       "  'recall': 0.6896067415730337,\n",
       "  'quad_stats': {'tp': {'oov_ratio_mean': 0.0,\n",
       "    'oov_ratio_std': 0.0,\n",
       "    'oov_ratio_min': 0.0,\n",
       "    'oov_ratio_max': 0.0,\n",
       "    'lens_mean': 70.65784114052953,\n",
       "    'lens_std': 166.63873944962404,\n",
       "    'lens_min': 2,\n",
       "    'lens_max': 1215,\n",
       "    'most_confident': 'fuck a nigger',\n",
       "    'most_confident_prob': 0.75212216,\n",
       "    'least_confident': \"== Is Stewie Gay? == \\n\\n I think Stewie is awesome but is he gay he shows signs of being gay but I don't really know does anyone?\",\n",
       "    'least_confident_prob': 0.40592173},\n",
       "   'fp': {'oov_ratio_mean': 0.0,\n",
       "    'oov_ratio_std': 0.0,\n",
       "    'oov_ratio_min': 0.0,\n",
       "    'oov_ratio_max': 0.0,\n",
       "    'lens_mean': 79.61695906432749,\n",
       "    'lens_std': 190.82509996517132,\n",
       "    'lens_min': 3,\n",
       "    'lens_max': 1373,\n",
       "    'most_confident': 'shit mother fucker bitches what a whore nigger',\n",
       "    'most_confident_prob': 0.7360172,\n",
       "    'least_confident': 'FUCC SLOBKS!  \\n neighkbkorhk60d crip cuhkz piruK fruityTownK SnoovaK!',\n",
       "    'least_confident_prob': 0.40631157},\n",
       "   'tn': {'oov_ratio_mean': 0.0,\n",
       "    'oov_ratio_std': 0.0,\n",
       "    'oov_ratio_min': 0.0,\n",
       "    'oov_ratio_max': 0.0,\n",
       "    'lens_mean': 81.21026953149831,\n",
       "    'lens_std': 124.14929520757502,\n",
       "    'lens_min': 1,\n",
       "    'lens_max': 4950,\n",
       "    'most_confident': '\" \\n\\n Thank you for experimenting with the page Wikipedia talk:Pokémon Collaborative Project on Wikipedia. Your test worked, and has been reverted or removed. Please use the sandbox for any other tests you want to do. Take a look at the welcome page if you would like to learn more about contributing to our encyclopedia. Thank you for your understanding.    \"',\n",
       "    'most_confident_prob': 0.0032370307,\n",
       "    'least_confident': '\"Sir Lord Nathan \"\"Big Pipe\"\"  fucking weekes.  \\n He is one of the brightest prospects in this world. Currently the 1st round pick in the NBA, however he disguised himself as Andrew Wiggins because he was already too famous in other sports.  Sports like football, American football,  Lacrosse, Baseball and hockey. Many of you know Nathan as a hockey legend, you just didn\\'t know it was him.  He had played on Edmonton Oilers for years beaming quite the legend. He later then moved to Boston Bruins in his Career. He also had appeared for the St. Louis Blues, that\\'s right he is the walking legend himself Wayne Gretzky. He then realized his true sport, American football. He had been able to find a clone ability and every black footballer known to man kind is Sir Nathan himself. His baseball career involved negatives at the beginning,  then later he got his legend status. That is right he is Jackie Robbinson, he suffered abuse from racist fans in his early  days. He took the abuse and he ended up being the first ever black baseball player too, truly a legend. After retiring from all those sports he found his true profession in life, Professional football.  He went up through Inter Milan youth system,  him and balotelli fucked shit up. They got 210 goals combined in his first debut season. Him and Balotelli decided to go on and make them own team. It was Balotelli FC where people of white consent were not allowed to play. He went bar down 200 times with Balotelli FC. There team truly went on to be victorious winning 21 trophies in there first season. Their team is  among 7 different leagues, they brought Italy to greatness winning the world cup 2 times, still to this day Balotelli and Nathan fuck shit up. They have scored 29 goals in the past 4 minutes. Being one of the best footballers was too boring for the great nate,  he wanted bigger and better.  He became a scientific hair researcher winning the Global peace prize and the biggest half nigga dick.\"',\n",
       "    'least_confident_prob': 0.4057421},\n",
       "   'fn': {'oov_ratio_mean': 0.0,\n",
       "    'oov_ratio_std': 0.0,\n",
       "    'oov_ratio_min': 0.0,\n",
       "    'oov_ratio_max': 0.0,\n",
       "    'lens_mean': 45.90950226244344,\n",
       "    'lens_std': 72.25495927945875,\n",
       "    'lens_min': 4,\n",
       "    'lens_max': 758,\n",
       "    'most_confident': '::mostly true (some blacks voted in every state)but that happened around 1900 and had no connection with CSA.',\n",
       "    'most_confident_prob': 0.033314705,\n",
       "    'least_confident': 'Ian Reynolds is a faggot, that is all.',\n",
       "    'least_confident_prob': 0.4055722}}},\n",
       " 'global_auc': 0.9845920406578025,\n",
       " 'global_f1': 0.6120855363600614,\n",
       " 'global_acc': 0.9693332082903497,\n",
       " 'global_tnr': 0.938038596600915,\n",
       " 'global_fpr': 0.024193212250044286,\n",
       " 'global_fnr': 0.0064735794596059056,\n",
       " 'global_tpr': 0.03129461168943491,\n",
       " 'global_precision': 0.5260512758664788,\n",
       " 'global_recall': 0.7472658408965877,\n",
       " 'train': ({'toxic': {'auc': 0.9902867387759876,\n",
       "    'f1': 0.8525419829767656,\n",
       "    'acc': 0.9718808555439271,\n",
       "    'tnr': 0.8905941555796479,\n",
       "    'fpr': 0.013561361400254433,\n",
       "    'fnr': 0.014557783055818414,\n",
       "    'tpr': 0.08128669996427923,\n",
       "    'precision': 0.8570201519656425,\n",
       "    'recall': 0.8481103700797699},\n",
       "   'severe_toxic': {'auc': 0.9925192081079575,\n",
       "    'f1': 0.5542168674698795,\n",
       "    'acc': 0.9893339015234598,\n",
       "    'tnr': 0.9827036240920969,\n",
       "    'fpr': 0.007300825337937344,\n",
       "    'fnr': 0.003365273138602879,\n",
       "    'tpr': 0.006630277431362842,\n",
       "    'precision': 0.47593342330184435,\n",
       "    'recall': 0.6633228840125391},\n",
       "   'obscene': {'auc': 0.9960341843625141,\n",
       "    'f1': 0.8642575862867732,\n",
       "    'acc': 0.9853106140840128,\n",
       "    'tnr': 0.9385477311040227,\n",
       "    'fpr': 0.00850405148805234,\n",
       "    'fnr': 0.006185334427934901,\n",
       "    'tpr': 0.0467628829799901,\n",
       "    'precision': 0.8461276788751559,\n",
       "    'recall': 0.8831814415907208},\n",
       "   'threat': {'auc': 0.995680311405447,\n",
       "    'f1': 0.6229166666666667,\n",
       "    'acc': 0.997731417362804,\n",
       "    'tnr': 0.9958576433061145,\n",
       "    'fpr': 0.0011468249243283555,\n",
       "    'fnr': 0.0011217577128676264,\n",
       "    'tpr': 0.0018737740566894987,\n",
       "    'precision': 0.6203319502074689,\n",
       "    'recall': 0.6255230125523012},\n",
       "   'insult': {'auc': 0.9919219224263707,\n",
       "    'f1': 0.7892729439809296,\n",
       "    'acc': 0.9778405850687155,\n",
       "    'tnr': 0.9363418164954785,\n",
       "    'fpr': 0.014294577335480757,\n",
       "    'fnr': 0.00786483759580375,\n",
       "    'tpr': 0.04149876857323699,\n",
       "    'precision': 0.743794226665169,\n",
       "    'recall': 0.8406753840294529},\n",
       "   'identity_hate': {'auc': 0.9944588556290898,\n",
       "    'f1': 0.6459999999999999,\n",
       "    'acc': 0.9933446553571764,\n",
       "    'tnr': 0.9872721233808148,\n",
       "    'fpr': 0.003923018593604101,\n",
       "    'fnr': 0.00273232604921947,\n",
       "    'tpr': 0.00607253197636162,\n",
       "    'precision': 0.6075235109717868,\n",
       "    'recall': 0.6896797153024911}},)}"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'toxic': {'auc': 0.9902867387759876,\n",
       "   'f1': 0.8525419829767656,\n",
       "   'acc': 0.9718808555439271,\n",
       "   'tnr': 0.8905941555796479,\n",
       "   'fpr': 0.013561361400254433,\n",
       "   'fnr': 0.014557783055818414,\n",
       "   'tpr': 0.08128669996427923,\n",
       "   'precision': 0.8570201519656425,\n",
       "   'recall': 0.8481103700797699},\n",
       "  'severe_toxic': {'auc': 0.9925192081079575,\n",
       "   'f1': 0.5542168674698795,\n",
       "   'acc': 0.9893339015234598,\n",
       "   'tnr': 0.9827036240920969,\n",
       "   'fpr': 0.007300825337937344,\n",
       "   'fnr': 0.003365273138602879,\n",
       "   'tpr': 0.006630277431362842,\n",
       "   'precision': 0.47593342330184435,\n",
       "   'recall': 0.6633228840125391},\n",
       "  'obscene': {'auc': 0.9960341843625141,\n",
       "   'f1': 0.8642575862867732,\n",
       "   'acc': 0.9853106140840128,\n",
       "   'tnr': 0.9385477311040227,\n",
       "   'fpr': 0.00850405148805234,\n",
       "   'fnr': 0.006185334427934901,\n",
       "   'tpr': 0.0467628829799901,\n",
       "   'precision': 0.8461276788751559,\n",
       "   'recall': 0.8831814415907208},\n",
       "  'threat': {'auc': 0.995680311405447,\n",
       "   'f1': 0.6229166666666667,\n",
       "   'acc': 0.997731417362804,\n",
       "   'tnr': 0.9958576433061145,\n",
       "   'fpr': 0.0011468249243283555,\n",
       "   'fnr': 0.0011217577128676264,\n",
       "   'tpr': 0.0018737740566894987,\n",
       "   'precision': 0.6203319502074689,\n",
       "   'recall': 0.6255230125523012},\n",
       "  'insult': {'auc': 0.9919219224263707,\n",
       "   'f1': 0.7892729439809296,\n",
       "   'acc': 0.9778405850687155,\n",
       "   'tnr': 0.9363418164954785,\n",
       "   'fpr': 0.014294577335480757,\n",
       "   'fnr': 0.00786483759580375,\n",
       "   'tpr': 0.04149876857323699,\n",
       "   'precision': 0.743794226665169,\n",
       "   'recall': 0.8406753840294529},\n",
       "  'identity_hate': {'auc': 0.9944588556290898,\n",
       "   'f1': 0.6459999999999999,\n",
       "   'acc': 0.9933446553571764,\n",
       "   'tnr': 0.9872721233808148,\n",
       "   'fpr': 0.003923018593604101,\n",
       "   'fnr': 0.00273232604921947,\n",
       "   'tpr': 0.00607253197636162,\n",
       "   'precision': 0.6075235109717868,\n",
       "   'recall': 0.6896797153024911}},)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_metrics[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Record results and save weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.environ[\"IS_COLAB\"] != \"True\":\n",
    "    import sys\n",
    "    sys.path.append(\"../lib\")\n",
    "    from record_experiments import record\n",
    "else:\n",
    "    PASSWORD = \"mongo11747\" # FILL IN IF COLAB\n",
    "\n",
    "    from typing import *\n",
    "    import pymongo\n",
    "    from bson.objectid import ObjectId\n",
    "    import os\n",
    "    import logging\n",
    "\n",
    "    # Logger\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logger.setLevel(logging.INFO)\n",
    "    ch = logging.StreamHandler()\n",
    "    ch.setLevel(logging.DEBUG)\n",
    "    formatter = logging.Formatter('[%(levelname)s] %(asctime)s - %(name)s %(message)s')\n",
    "    ch.setFormatter(formatter)\n",
    "    logger.addHandler(ch)\n",
    "\n",
    "    conn_str = f\"mongodb+srv://root:{PASSWORD}@cluster0-ptgoc.mongodb.net/test?retryWrites=true\"\n",
    "\n",
    "    client = pymongo.MongoClient(conn_str)\n",
    "    db = client.experiments\n",
    "    collection = db.logs\n",
    "\n",
    "    def _cln(v: Any) -> Any:\n",
    "        \"\"\"Ensure variables are serializable\"\"\"\n",
    "        if isinstance(v, (np.float, np.float16, np.float32, np.float64, np.float128)):\n",
    "            return float(v)\n",
    "        elif isinstance(v, (np.int, np.int0, np.int8, np.int16, np.int32, np.int64)):\n",
    "            return int(v)\n",
    "        elif isinstance(v, dict):\n",
    "            return {k: _cln(v_) for k, v_ in v.items()}\n",
    "        else:\n",
    "            return v\n",
    "\n",
    "    def record(log: dict):\n",
    "        res = collection.insert_one({str(k): _cln(v) for k, v in log.items()})\n",
    "        logger.info(f\"Inserted results at id {res.inserted_id}\")\n",
    "        return res\n",
    "\n",
    "    def find(id_: Optional[str]=None, query: Optional[dict]=None):\n",
    "        if query is None: query = {\"_id\": ObjectId(id_)}\n",
    "        res = collection.find_one(query)\n",
    "        return res\n",
    "\n",
    "    def delete(id_: Optional[str]=None, query: Optional[dict]=None):\n",
    "        if query is None: query = {\"_id\": ObjectId(id_)}\n",
    "        res = collection.delete_many(query)\n",
    "        logger.info(f\"Deleted {res.deleted_count} entries\")\n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Record summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] 2019-04-22 22:36:57,560 - record_experiments Inserted results at id 5cbe7a49145ce52eeb24976d\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from pytz import timezone\n",
    "\n",
    "if not config.testing:\n",
    "    experiment_log = dict(config)\n",
    "    tz = timezone('EST')\n",
    "    experiment_log[\"execution_date\"] = datetime.now(tz).strftime(\"%Y-%m-%d %H:%M %Z\")\n",
    "    experiment_log.update(metrics)\n",
    "    experiment_log.update(label_metrics)\n",
    "    res = record(experiment_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(DATA_ROOT / f\"test_basic_preds_{res.inserted_id}.npy\", test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(DATA_ROOT / f\"thres_basic_preds_{res.inserted_id}.npy\", thres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
