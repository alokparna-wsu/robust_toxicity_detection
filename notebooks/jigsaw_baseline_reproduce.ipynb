{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import *\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from functools import partial\n",
    "from overrides import overrides\n",
    "\n",
    "from allennlp.data import Instance\n",
    "from allennlp.data.fields import TextField, SequenceLabelField, LabelField\n",
    "from allennlp.data.token_indexers import TokenIndexer, SingleIdTokenIndexer\n",
    "from allennlp.data.tokenizers import Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from contextlib import contextmanager\n",
    "\n",
    "class Config(dict):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        for k, v in kwargs.items():\n",
    "            setattr(self, k, v)\n",
    "    \n",
    "    def set(self, key, val):\n",
    "        self[key] = val\n",
    "        setattr(self, key, val)\n",
    "\n",
    "@contextmanager\n",
    "def timer(name):\n",
    "    t0 = time.time()\n",
    "    yield\n",
    "    print(f'[{name}] done in {time.time() - t0:.0f} s')\n",
    "    \n",
    "import functools\n",
    "import traceback\n",
    "def get_ref_free_exc_info():\n",
    "    \"Free traceback from references to locals/globals to avoid circular reference leading to gc.collect() unable to reclaim memory\"\n",
    "    type, val, tb = sys.exc_info()\n",
    "    traceback.clear_frames(tb)\n",
    "    return (type, val, tb)\n",
    "\n",
    "def gpu_mem_restore(func):\n",
    "    \"Reclaim GPU RAM if CUDA out of memory happened, or execution was interrupted\"\n",
    "    @functools.wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        try:\n",
    "            return func(*args, **kwargs)\n",
    "        except:\n",
    "            type, val, tb = get_ref_free_exc_info() # must!\n",
    "            raise type(val).with_traceback(tb) from None\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# for papermill\n",
    "testing = True\n",
    "seed = 1\n",
    "computational_batch_size = 256\n",
    "batch_size = 256\n",
    "lr = 0.001\n",
    "epochs = 15\n",
    "hidden_sz = 64\n",
    "dataset = \"jigsaw\"\n",
    "n_classes = 6\n",
    "max_seq_len = 512\n",
    "download_data = False\n",
    "ft_model_path = \"../data/jigsaw/ft_model.bin\"\n",
    "max_vocab_size = 300000\n",
    "dropoute = 0.5\n",
    "run_id = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "if download_data:\n",
    "    for fname in [\"train.csv\", \"test_proced.csv\"]:\n",
    "        subprocess.run([\"aws\", \"s3\", \"cp\", f\"s3://nnfornlp/data/jigsaw/{fname}\"], \n",
    "                       shell=True, check=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Can we make this play better with papermill?\n",
    "config = Config(\n",
    "    testing=testing,\n",
    "    seed=seed,\n",
    "    computational_batch_size=computational_batch_size,\n",
    "    batch_size=batch_size,\n",
    "    lr=lr,\n",
    "    epochs=epochs,\n",
    "    hidden_sz=hidden_sz,\n",
    "    dataset=dataset,\n",
    "    n_classes=n_classes,\n",
    "    max_seq_len=max_seq_len, # necessary to limit memory usage\n",
    "    ft_model_path=ft_model_path,\n",
    "    max_vocab_size=max_vocab_size,\n",
    "    dropoute=dropoute,\n",
    "    run_id=run_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.common.checks import ConfigurationError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "now = datetime.datetime.now()\n",
    "RUN_ID = config.run_id if config.run_id is not None else now.strftime(\"%m_%d_%H:%M:%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_GPU = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = Path(\"../data\") / config.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set random seed manually to replicate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x11861cf30>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(config.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.data.vocabulary import Vocabulary\n",
    "from allennlp.data.dataset_readers import DatasetReader, StanfordSentimentTreeBankDatasetReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader_registry = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def register(name: str):\n",
    "    def dec(x: Callable):\n",
    "        reader_registry[name] = x\n",
    "        return x\n",
    "    return dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_cols = [\"toxic\", \"severe_toxic\", \"obscene\",\n",
    "              \"threat\", \"insult\", \"identity_hate\"]\n",
    "\n",
    "from enum import IntEnum\n",
    "ColIdx = IntEnum('ColIdx', [(x.upper(), i) for i, x in enumerate(label_cols)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "@register(\"jigsaw\")\n",
    "class JigsawDatasetReader(DatasetReader):\n",
    "    def __init__(self, tokenizer: Callable[[str], List[str]]=lambda x: x.split(),\n",
    "                 token_indexers: Dict[str, TokenIndexer] = None, # TODO: Handle mapping from BERT\n",
    "                 max_seq_len: Optional[int]=config.max_seq_len) -> None:\n",
    "        super().__init__(lazy=False)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.token_indexers = token_indexers or {\"tokens\": SingleIdTokenIndexer()}\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "    @overrides\n",
    "    def text_to_instance(self, tokens: List[Token],\n",
    "                         toxic: int, severe_toxic: int, obscene: int,\n",
    "                         threat: int, insult: int, identity_hate: int) -> Instance:\n",
    "        sentence_field = TextField(tokens, self.token_indexers)\n",
    "        fields = {\"tokens\": sentence_field}\n",
    "\n",
    "        toxic_field = LabelField(label=toxic, skip_indexing=True)\n",
    "        fields[\"toxic\"] = toxic_field\n",
    "        \n",
    "        severe_toxic_field = LabelField(label=severe_toxic, skip_indexing=True)\n",
    "        fields[\"severe_toxic\"] = severe_toxic_field\n",
    "        \n",
    "        obscene_field = LabelField(label=obscene, skip_indexing=True)\n",
    "        fields[\"obscene\"] = obscene_field\n",
    "        \n",
    "        threat_field = LabelField(label=threat, skip_indexing=True)\n",
    "        fields[\"threat\"] = threat_field\n",
    "        \n",
    "        insult_field = LabelField(label=insult, skip_indexing=True)\n",
    "        fields[\"insult\"] = insult_field\n",
    "        \n",
    "        identity_hate_field = LabelField(label=identity_hate, skip_indexing=True)\n",
    "        fields[\"identity_hate\"] = identity_hate_field\n",
    "\n",
    "        return Instance(fields)\n",
    "    \n",
    "    @overrides\n",
    "    def _read(self, file_path: str) -> Iterator[Instance]:\n",
    "        df = pd.read_csv(file_path)\n",
    "        if config.testing: df = df.head(10000)\n",
    "        for i, row in df.iterrows():\n",
    "            yield self.text_to_instance(\n",
    "                [Token(x) for x in self.tokenizer(row[\"comment_text\"])],\n",
    "                row[\"toxic\"], row[\"severe_toxic\"], row[\"obscene\"], \n",
    "                row[\"threat\"], row[\"insult\"], row[\"identity_hate\"],\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare token handlers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.data.tokenizers.word_splitter import SpacyWordSplitter\n",
    "from allennlp.data.token_indexers import WordpieceIndexer, SingleIdTokenIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_indexer = SingleIdTokenIndexer(\n",
    "    lowercase_tokens=False,  # don't lowercase by default\n",
    ")\n",
    "def tokenizer(x):\n",
    "    return [w.text for w in\n",
    "            SpacyWordSplitter(language='en_core_web_sm', pos_tags=False).split_words(x)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = JigsawDatasetReader(\n",
    "    tokenizer=tokenizer,\n",
    "    token_indexers={\"tokens\": token_indexer}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10000it [00:24, 412.34it/s]\n",
      "10000it [00:31, 316.64it/s]\n"
     ]
    }
   ],
   "source": [
    "train_ds, test_ds = (reader.read(DATA_ROOT / fname) for fname in [\"train.csv\", \"test_proced.csv\"])\n",
    "val_ds = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/26/2019 18:45:41 - INFO - allennlp.data.vocabulary -   Fitting token dictionary from dataset.\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 17207.79it/s]\n"
     ]
    }
   ],
   "source": [
    "vocab = Vocabulary.from_instances(train_ds, max_vocab_size=config.max_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.data.iterators import BucketIterator, DataIterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Allow for customization\n",
    "iterator = BucketIterator(batch_size=config.batch_size, \n",
    "                          biggest_batch_first=True,\n",
    "                          sorting_keys=[(\"tokens\", \"num_tokens\")],\n",
    "                         )\n",
    "iterator.index_with(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(iterator(train_ds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': {'tokens': tensor([[    5,     7,    26,  ...,     0,     0,     0],\n",
       "          [    5, 23538,  3225,  ...,     0,     0,     0],\n",
       "          [    5,    24, 11937,  ...,     0,     0,     0],\n",
       "          ...,\n",
       "          [  167,    69,     3,  ...,     0,     0,     0],\n",
       "          [    7,    82,   139,  ...,     0,     0,     0],\n",
       "          [   32,   178,   221,  ...,     7,   124,     2]])},\n",
       " 'toxic': tensor([0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1]),\n",
       " 'severe_toxic': tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1]),\n",
       " 'obscene': tensor([0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1]),\n",
       " 'threat': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1]),\n",
       " 'insult': tensor([0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1]),\n",
       " 'identity_hate': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    5,     7,    26,  ...,     0,     0,     0],\n",
       "        [    5, 23538,  3225,  ...,     0,     0,     0],\n",
       "        [    5,    24, 11937,  ...,     0,     0,     0],\n",
       "        ...,\n",
       "        [  167,    69,     3,  ...,     0,     0,     0],\n",
       "        [    7,    82,   139,  ...,     0,     0,     0],\n",
       "        [   32,   178,   221,  ...,     7,   124,     2]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"tokens\"][\"tokens\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 2086])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"tokens\"][\"tokens\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.modules.token_embedders import Embedding\n",
    "from allennlp.modules.token_embedders.bert_token_embedder import BertEmbedder, PretrainedBertEmbedder\n",
    "from allennlp.modules.seq2vec_encoders import Seq2VecEncoder, PytorchSeq2VecWrapper\n",
    "from allennlp.modules.stacked_bidirectional_lstm import StackedBidirectionalLstm\n",
    "from allennlp.nn.util import get_text_field_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, inp_sz, dim=1, eps=1e-9):\n",
    "        super().__init__()\n",
    "        self.inp_sz, self.dim, self.eps = inp_sz, dim, eps\n",
    "        self.l1 = nn.Linear(inp_sz, inp_sz)\n",
    "        nn.init.xavier_uniform_(self.l1.weight.data)\n",
    "        nn.init.zeros_(self.l1.bias.data)\n",
    "        \n",
    "        vw = torch.zeros(inp_sz, 1)\n",
    "        nn.init.xavier_uniform_(vw)        \n",
    "        self.vw = nn.Parameter(vw)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        e = torch.tanh(self.l1(x))\n",
    "        e = torch.einsum(\"bij,jk->bi\", [e, self.vw])            \n",
    "        a = torch.exp(e)\n",
    "        \n",
    "        if mask is not None: a = a.masked_fill(mask, 0)\n",
    "\n",
    "        a = a / (torch.sum(a, dim=self.dim, keepdim=True) + self.eps)\n",
    "\n",
    "        weighted_input = x * a.unsqueeze(-1)\n",
    "        return torch.sum(weighted_input, dim=1), a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_gru_weights(gru: nn.GRU):\n",
    "    \"\"\"Applies orthogonal and xavier uniform initialization according to best practices\"\"\"\n",
    "    for nm, param in gru.named_parameters():\n",
    "        if \"weight_hh\" in nm:\n",
    "            nn.init.orthogonal_(param.data)\n",
    "        elif \"weight_ih\" in nm:\n",
    "            nn.init.xavier_uniform_(param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiGRUAttentionEncoder(Seq2VecEncoder):\n",
    "    def __init__(self, embed_sz: int, hidden_sz: int, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.embed_sz = embed_sz\n",
    "        self.hidden_sz = hidden_sz\n",
    "        self.gru = nn.GRU(self.embed_sz, self.hidden_sz,\n",
    "                          num_layers=num_layers, bidirectional=True)\n",
    "        init_gru_weights(self.gru)\n",
    "        self.attention = Attention(self.hidden_sz * 2, dim=1)\n",
    "        \n",
    "    @overrides\n",
    "    def get_input_dim(self) -> int:\n",
    "        return self.embed_sz\n",
    "    \n",
    "    @overrides\n",
    "    def get_output_dim(self) -> int:\n",
    "        return self.hidden_sz * 2\n",
    "    \n",
    "    @overrides\n",
    "    def forward(self, x: torch.tensor, \n",
    "                mask: Optional[torch.tensor]=None) -> torch.tensor:\n",
    "        x, _ = self.gru(x, None)\n",
    "        x, _ = self.attention(x, mask=mask)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.training.metrics import CategoricalAccuracy, BooleanAccuracy, Metric\n",
    "\n",
    "def prod(x: Iterable):\n",
    "    acc = 1\n",
    "    for v in x: acc *= v\n",
    "    return acc\n",
    "\n",
    "class MultilabelAccuracy(Metric):\n",
    "    def __init__(self, thres=0.5):\n",
    "        self.thres = 0.5\n",
    "        self.correct_count = 0\n",
    "        self.total_count = 0\n",
    "    \n",
    "    def __call__(self, logits: torch.FloatTensor, \n",
    "                 t: torch.LongTensor) -> float:\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        t = t.detach().cpu().numpy()\n",
    "        cc = ((logits >= self.thres) == t).sum()\n",
    "        tc = prod(logits.shape)\n",
    "        self.correct_count += cc\n",
    "        self.total_count += tc\n",
    "        return cc / tc\n",
    "    \n",
    "    def get_metric(self, reset: bool=False):\n",
    "        acc = self.correct_count / self.total_count\n",
    "        if reset:\n",
    "            self.reset()\n",
    "        return acc\n",
    "    \n",
    "    @overrides\n",
    "    def reset(self):\n",
    "        self.correct_count = 0\n",
    "        self.total_count = 0\n",
    "    \n",
    "class MultilabelCrossEntropyLoss(nn.Module):\n",
    "    def forward(self, lgt, tgt: torch.LongTensor):\n",
    "        neg_abs = -lgt.abs()\n",
    "        loss = lgt.clamp(min=0) - lgt * tgt.float() + (1 + neg_abs.exp()).log()\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.models import Model\n",
    "from allennlp.modules.text_field_embedders import TextFieldEmbedder, BasicTextFieldEmbedder\n",
    "\n",
    "class BaselineModel(Model):\n",
    "    def __init__(self, word_embeddings: TextFieldEmbedder,\n",
    "                 encoder: Seq2VecEncoder,\n",
    "                 out_sz: int=config.n_classes,\n",
    "                 multilabel: bool=True):\n",
    "        super().__init__(vocab)\n",
    "        self.word_embeddings = word_embeddings\n",
    "        self.encoder = encoder\n",
    "        self.projection = nn.Linear(self.encoder.get_output_dim(), out_sz)\n",
    "        self.multilabel = multilabel\n",
    "        # TODO: Handle multiclass case\n",
    "        if self.multilabel:\n",
    "            self.accuracy = MultilabelAccuracy()\n",
    "            self.per_label_acc = {c: MultilabelAccuracy() for c in label_cols}\n",
    "            self.loss = MultilabelCrossEntropyLoss()\n",
    "        else:\n",
    "            self.loss = nn.CrossEntropyLoss()\n",
    "            self.accuracy = CategoricalAccuracy()\n",
    "        \n",
    "    def forward(self, tokens: Dict[str, torch.Tensor],\n",
    "                **labels: torch.Tensor) -> torch.Tensor:\n",
    "        mask = get_text_field_mask(tokens) == 0\n",
    "        embeddings = self.word_embeddings(tokens)\n",
    "        state = self.encoder(embeddings, mask)\n",
    "        class_logits = self.projection(state)\n",
    "        \n",
    "        output = {\"class_logits\": class_logits}\n",
    "        if len(labels) > 0:\n",
    "            # This is grossly inefficient...\n",
    "            label = torch.cat([labels[c].unsqueeze(-1) for c in label_cols], dim=1)\n",
    "            output[\"accuracy\"] = self.accuracy(class_logits, label)\n",
    "            for i, c in enumerate(label_cols):\n",
    "                output[f\"{c}_acc\"] = self.per_label_acc[c](class_logits[:, i], \n",
    "                                                          labels[c])\n",
    "            output[\"loss\"] = self.loss(class_logits, label)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def get_metrics(self, reset: bool = False) -> Dict[str, float]:\n",
    "        return {\"accuracy\": self.accuracy.get_metric(reset)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastText\n",
    "\n",
    "def get_fasttext_embeddings(model_path: str, vocab: Vocabulary):\n",
    "    vocab_size = min(vocab.get_vocab_size(), config.max_vocab_size)\n",
    "    ft_model = fastText.load_model(config.ft_model_path)\n",
    "    embedding_dim = ft_model.get_dimension()\n",
    "\n",
    "    # register parameters\n",
    "    config.set(\"vocab_size\", vocab_size)\n",
    "    config.set(\"embedding_dim\", embedding_dim)\n",
    "    \n",
    "    embeddings = np.zeros((vocab_size + 5, embedding_dim))\n",
    "    for idx, token in vocab.get_index_to_token_vocabulary().items():\n",
    "        embeddings[idx, :] = ft_model.get_word_vector(token)\n",
    "    \n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Loading embeddings] done in 1 s\n"
     ]
    }
   ],
   "source": [
    "with timer(\"Loading embeddings\"):\n",
    "    embedding_weights = get_fasttext_embeddings(config.ft_model_path, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomEmbedding(nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_dim,\n",
    "                 padding_index=None, max_norm=None,\n",
    "                 weight=None, dropout=0., scale=None):\n",
    "        super().__init__()\n",
    "        self.dropout = dropout\n",
    "        self.scale = scale\n",
    "        self.padding_idx = padding_index\n",
    "        self.embed = Embedding(num_embeddings, embedding_dim,\n",
    "                               padding_index=padding_index, max_norm=max_norm,\n",
    "                               weight=weight)\n",
    "    \n",
    "    def forward(self, words):\n",
    "        weight = self.embed.weight\n",
    "        if self.dropout > 0.0 and self.training:\n",
    "            mask = self.embed.weight.data.new().resize_((weight.size(0), 1)).bernoulli_(1 - self.dropout).expand_as(weight) / (1 - self.dropout)\n",
    "            masked_embed_weight = mask * weight\n",
    "        else:\n",
    "            masked_embed_weight = weight\n",
    "        if self.scale:\n",
    "            masked_embed_weight = scale.expand_as(masked_embed_weight) * masked_embed_weight\n",
    "\n",
    "        padding_idx = self.padding_idx\n",
    "        if padding_idx is None:\n",
    "            padding_idx = -1\n",
    "\n",
    "        X = torch.nn.functional.embedding(words, masked_embed_weight,\n",
    "            padding_idx, self.embed.max_norm, self.embed.norm_type,\n",
    "            self.embed.scale_grad_by_freq, self.embed.sparse\n",
    "          )\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_embedding = CustomEmbedding(num_embeddings=config.vocab_size + 5,\n",
    "                                  embedding_dim=config.embedding_dim,\n",
    "                                  weight=torch.tensor(embedding_weights, dtype=torch.float),\n",
    "                                  dropout=config.dropoute, padding_index=0)\n",
    "word_embeddings = BasicTextFieldEmbedder({\"tokens\": token_embedding})\n",
    "encoder = BiGRUAttentionEncoder(\n",
    "    config.embedding_dim, \n",
    "    config.hidden_sz,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BaselineModel(\n",
    "    word_embeddings, \n",
    "    encoder, \n",
    "    out_sz=config.n_classes,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize bias according to prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = pd.read_csv(DATA_ROOT / \"train.csv\")[label_cols].values\n",
    "if config.testing: \n",
    "    train_labels = train_labels[:10000, :]\n",
    "    \n",
    "class_bias = torch.zeros(len(label_cols))\n",
    "for i, _ in enumerate(label_cols):\n",
    "    p = train_labels[:, i].mean()\n",
    "    class_bias[i] = np.log(p / (1-p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.projection.bias.data = class_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_GPU: model.cuda()\n",
    "else: model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic sanity checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.isnan(list(model.word_embeddings.parameters())[0].detach().numpy()).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[np.isnan(x.detach().numpy()).any() for x in list(model.encoder.parameters())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[np.isinf(x.detach().numpy()).any() for x in list(model.encoder.parameters())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = batch[\"tokens\"]\n",
    "labels = batch\n",
    "\n",
    "mask = get_text_field_mask(tokens) == 0\n",
    "embeddings = model.word_embeddings(tokens)\n",
    "state = model.encoder(embeddings, mask)\n",
    "class_logits = model.projection(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': tensor([[    5,     7,    26,  ...,     0,     0,     0],\n",
       "         [    5, 23538,  3225,  ...,     0,     0,     0],\n",
       "         [    5,    24, 11937,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [  167,    69,     3,  ...,     0,     0,     0],\n",
       "         [    7,    82,   139,  ...,     0,     0,     0],\n",
       "         [   32,   178,   221,  ...,     7,   124,     2]])}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0,  ..., 1, 1, 1],\n",
       "        [0, 0, 0,  ..., 1, 1, 1],\n",
       "        [0, 0, 0,  ..., 1, 1, 1],\n",
       "        ...,\n",
       "        [0, 0, 0,  ..., 1, 1, 1],\n",
       "        [0, 0, 0,  ..., 1, 1, 1],\n",
       "        [0, 0, 0,  ..., 0, 0, 0]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0,  ..., 1, 1, 1],\n",
       "        [0, 0, 0,  ..., 1, 1, 1],\n",
       "        [0, 0, 0,  ..., 1, 1, 1],\n",
       "        ...,\n",
       "        [0, 0, 0,  ..., 1, 1, 1],\n",
       "        [0, 0, 0,  ..., 1, 1, 1],\n",
       "        [0, 0, 0,  ..., 0, 0, 0]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'class_logits': tensor([[-2.2214, -4.5855, -2.9599, -5.7243, -2.9847, -4.7656],\n",
       "         [-2.2176, -4.5764, -2.9719, -5.7270, -2.9942, -4.7660],\n",
       "         [-2.2172, -4.5728, -2.9734, -5.7256, -3.0006, -4.7664],\n",
       "         [-2.2138, -4.5756, -2.9653, -5.7184, -3.0054, -4.7701],\n",
       "         [-2.2163, -4.5725, -2.9607, -5.7154, -3.0111, -4.7737],\n",
       "         [-2.2163, -4.5768, -2.9561, -5.7147, -3.0129, -4.7753],\n",
       "         [-2.2187, -4.5790, -2.9527, -5.7206, -3.0119, -4.7732],\n",
       "         [-2.2131, -4.5899, -2.9474, -5.7227, -3.0128, -4.7672],\n",
       "         [-2.2074, -4.5879, -2.9397, -5.7344, -3.0128, -4.7548],\n",
       "         [-2.2235, -4.5761, -2.9421, -5.7403, -3.0104, -4.7591],\n",
       "         [-2.2301, -4.5640, -2.9435, -5.7523, -3.0081, -4.7588],\n",
       "         [-2.2085, -4.5381, -2.9409, -5.7687, -3.0019, -4.7552],\n",
       "         [-2.1954, -4.4909, -2.9367, -5.7923, -3.0307, -4.7659],\n",
       "         [-2.2185, -4.4934, -2.9344, -5.7322, -3.0476, -4.8321],\n",
       "         [-2.2306, -4.5141, -2.9241, -5.7001, -3.0660, -4.8367],\n",
       "         [-2.2385, -4.5367, -2.9095, -5.6867, -3.0594, -4.8257]],\n",
       "        grad_fn=<AddmmBackward>),\n",
       " 'accuracy': 0.71875,\n",
       " 'toxic_acc': 0.5,\n",
       " 'severe_toxic_acc': 0.6875,\n",
       " 'obscene_acc': 0.625,\n",
       " 'threat_acc': 0.875,\n",
       " 'insult_acc': 0.625,\n",
       " 'identity_hate_acc': 1.0,\n",
       " 'loss': tensor(0.9496, grad_fn=<MeanBackward1>)}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(**batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = model(**batch)[\"loss\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9506, grad_fn=<MeanBackward1>)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[-4.4918e-06,  1.1074e-05,  1.8466e-06,  ...,  6.4138e-06,\n",
       "          -7.3518e-07, -2.1079e-06],\n",
       "         [-2.3124e-06,  1.0786e-05,  2.0789e-06,  ...,  4.7372e-06,\n",
       "          -2.6070e-06,  7.6327e-07],\n",
       "         [-3.1135e-07, -1.4808e-06, -1.2328e-06,  ...,  3.4717e-07,\n",
       "           8.6560e-07, -1.0413e-06],\n",
       "         ...,\n",
       "         [ 2.8734e-05, -8.5067e-05, -4.6237e-05,  ..., -4.6114e-05,\n",
       "           3.6131e-05, -8.2640e-05],\n",
       "         [-6.1587e-05,  1.9873e-04,  1.6565e-04,  ...,  1.2352e-05,\n",
       "          -2.7602e-05,  1.0193e-04],\n",
       "         [-5.3004e-04,  1.8178e-03,  2.4155e-04,  ...,  8.0014e-04,\n",
       "          -1.3717e-04,  2.3603e-04]]),\n",
       " tensor([[ 5.2614e-06, -2.8081e-06, -4.9264e-06,  ...,  2.5151e-06,\n",
       "           2.2152e-06,  2.3523e-06],\n",
       "         [ 3.0922e-06, -1.1978e-06, -1.0779e-05,  ...,  9.8048e-06,\n",
       "           7.8398e-06, -2.0084e-07],\n",
       "         [-9.5489e-07, -1.6294e-06,  2.9049e-06,  ..., -1.9869e-06,\n",
       "          -8.8423e-07,  5.1661e-07],\n",
       "         ...,\n",
       "         [-5.1531e-05,  2.2465e-05,  9.1148e-05,  ..., -8.2903e-05,\n",
       "          -5.1170e-05, -1.6801e-05],\n",
       "         [ 3.6640e-05, -5.2689e-05, -8.0640e-05,  ...,  6.1030e-05,\n",
       "           3.5803e-05,  4.2936e-05],\n",
       "         [ 4.7197e-04, -2.4024e-04, -6.7957e-04,  ...,  6.0407e-04,\n",
       "           3.4886e-04,  1.9141e-04]]),\n",
       " tensor([ 5.4063e-05,  8.3212e-05, -1.3133e-05,  6.4094e-06,  1.4138e-04,\n",
       "         -4.7047e-05,  1.9840e-06, -2.5732e-05,  3.3819e-05,  4.3702e-04,\n",
       "          1.5476e-04,  5.4693e-05,  9.0799e-05, -1.3986e-05,  2.2185e-05,\n",
       "         -1.7521e-04, -1.1881e-04, -2.5276e-05, -1.2912e-04,  1.1998e-05,\n",
       "          6.5298e-06,  3.5311e-05, -2.6365e-06,  1.7115e-04, -1.4954e-05,\n",
       "         -6.3289e-06,  1.1646e-05, -1.8484e-04, -7.6572e-06, -2.6904e-05,\n",
       "          2.2144e-04, -6.2971e-05,  1.5412e-05,  1.3278e-04,  6.9796e-05,\n",
       "         -2.1171e-05, -1.6056e-04,  2.8283e-04, -7.5473e-05,  1.8709e-05,\n",
       "         -8.8168e-05,  7.2138e-06,  1.6695e-04,  5.1032e-05,  1.5101e-05,\n",
       "         -1.7114e-05,  3.1578e-04,  1.4268e-05,  8.1642e-05, -2.5218e-04,\n",
       "          5.0646e-06, -1.5401e-04,  4.4956e-06, -5.0595e-05,  2.5838e-06,\n",
       "          2.8221e-05, -4.7802e-05,  1.5169e-04, -9.8891e-05, -3.4638e-05,\n",
       "         -4.8203e-06, -2.9980e-05, -7.7753e-06,  2.5749e-04, -7.7073e-05,\n",
       "          2.9928e-05, -3.8056e-05, -7.3242e-05,  5.0994e-05,  3.5298e-05,\n",
       "          3.4744e-06, -1.3285e-05, -2.2035e-05, -5.7745e-05, -1.3240e-05,\n",
       "         -1.1660e-05, -1.2485e-05, -5.7992e-05,  4.9702e-06,  4.4025e-05,\n",
       "          1.9796e-05, -1.1125e-05, -4.9864e-06,  1.8594e-05, -3.4965e-06,\n",
       "          3.9453e-05, -4.4070e-06,  5.8955e-05,  5.4522e-06,  6.4614e-05,\n",
       "         -8.1191e-05,  4.2781e-05,  1.2110e-04, -2.6104e-06, -6.4911e-05,\n",
       "         -8.2407e-05, -4.3388e-05,  9.6253e-06, -4.9819e-05, -6.3169e-05,\n",
       "          9.6237e-06, -8.7442e-06,  6.6759e-05,  7.8145e-05,  1.3156e-04,\n",
       "          1.2019e-06,  7.0136e-05,  1.0397e-05, -1.0387e-05,  6.3862e-06,\n",
       "         -9.2712e-05, -1.8838e-06, -3.2696e-05, -5.3671e-05,  3.9729e-05,\n",
       "         -5.2401e-05, -2.5083e-05,  3.0365e-05, -2.1497e-05,  5.2821e-05,\n",
       "         -9.9413e-07, -1.4975e-05,  1.0733e-05, -2.1930e-05,  3.6539e-05,\n",
       "         -3.4430e-05, -1.5361e-05, -1.2769e-04,  6.4007e-03,  3.4657e-03,\n",
       "          1.6805e-03,  2.4429e-04,  4.7631e-03, -1.0557e-03,  1.5765e-03,\n",
       "          2.1358e-03,  4.3751e-03,  8.7783e-03, -4.2723e-03, -1.6691e-03,\n",
       "          2.3809e-03,  6.9228e-03, -6.9573e-04, -5.5418e-03, -5.0701e-03,\n",
       "         -1.6683e-03, -2.9137e-03, -7.9951e-03,  9.3983e-04, -2.5616e-03,\n",
       "          3.1247e-04,  5.9667e-03,  5.4076e-04, -1.5638e-03, -3.5026e-03,\n",
       "         -4.2566e-03, -5.9135e-03, -1.1599e-03,  4.9230e-03, -6.8077e-03,\n",
       "          5.5233e-03, -5.2538e-03,  4.3637e-03,  1.4448e-03,  6.5642e-03,\n",
       "         -1.0289e-02, -4.5967e-03, -4.1021e-03, -5.9714e-03,  9.9200e-03,\n",
       "         -3.5719e-03,  2.9448e-03,  2.0852e-03,  7.6061e-04, -1.3913e-02,\n",
       "          4.1028e-03, -4.3967e-03,  7.1132e-03, -4.5795e-03, -7.3467e-03,\n",
       "          4.9400e-03, -3.5455e-03,  1.4665e-05,  1.4508e-03, -3.5627e-03,\n",
       "         -6.2103e-03,  3.0542e-03,  5.3485e-03, -2.8346e-03, -1.3653e-03,\n",
       "          1.6007e-03,  1.0926e-02]),\n",
       " tensor([ 5.4063e-05,  8.3212e-05, -1.3133e-05,  6.4094e-06,  1.4138e-04,\n",
       "         -4.7047e-05,  1.9840e-06, -2.5732e-05,  3.3819e-05,  4.3702e-04,\n",
       "          1.5476e-04,  5.4693e-05,  9.0799e-05, -1.3986e-05,  2.2185e-05,\n",
       "         -1.7521e-04, -1.1881e-04, -2.5276e-05, -1.2912e-04,  1.1998e-05,\n",
       "          6.5298e-06,  3.5311e-05, -2.6365e-06,  1.7115e-04, -1.4954e-05,\n",
       "         -6.3289e-06,  1.1646e-05, -1.8484e-04, -7.6572e-06, -2.6904e-05,\n",
       "          2.2144e-04, -6.2971e-05,  1.5412e-05,  1.3278e-04,  6.9796e-05,\n",
       "         -2.1171e-05, -1.6056e-04,  2.8283e-04, -7.5473e-05,  1.8709e-05,\n",
       "         -8.8168e-05,  7.2138e-06,  1.6695e-04,  5.1032e-05,  1.5101e-05,\n",
       "         -1.7114e-05,  3.1578e-04,  1.4268e-05,  8.1642e-05, -2.5218e-04,\n",
       "          5.0646e-06, -1.5401e-04,  4.4956e-06, -5.0595e-05,  2.5838e-06,\n",
       "          2.8221e-05, -4.7802e-05,  1.5169e-04, -9.8891e-05, -3.4638e-05,\n",
       "         -4.8203e-06, -2.9980e-05, -7.7753e-06,  2.5749e-04, -7.7073e-05,\n",
       "          2.9928e-05, -3.8056e-05, -7.3242e-05,  5.0994e-05,  3.5298e-05,\n",
       "          3.4744e-06, -1.3285e-05, -2.2035e-05, -5.7745e-05, -1.3240e-05,\n",
       "         -1.1660e-05, -1.2485e-05, -5.7992e-05,  4.9702e-06,  4.4025e-05,\n",
       "          1.9796e-05, -1.1125e-05, -4.9864e-06,  1.8594e-05, -3.4965e-06,\n",
       "          3.9453e-05, -4.4070e-06,  5.8955e-05,  5.4522e-06,  6.4614e-05,\n",
       "         -8.1191e-05,  4.2781e-05,  1.2110e-04, -2.6104e-06, -6.4911e-05,\n",
       "         -8.2407e-05, -4.3388e-05,  9.6253e-06, -4.9819e-05, -6.3169e-05,\n",
       "          9.6237e-06, -8.7442e-06,  6.6759e-05,  7.8145e-05,  1.3156e-04,\n",
       "          1.2019e-06,  7.0136e-05,  1.0397e-05, -1.0387e-05,  6.3862e-06,\n",
       "         -9.2712e-05, -1.8838e-06, -3.2696e-05, -5.3671e-05,  3.9729e-05,\n",
       "         -5.2401e-05, -2.5083e-05,  3.0365e-05, -2.1497e-05,  5.2821e-05,\n",
       "         -9.9413e-07, -1.4975e-05,  1.0733e-05, -2.1930e-05,  3.6539e-05,\n",
       "         -3.4430e-05, -1.5361e-05, -1.2769e-04,  3.3728e-03,  1.6967e-03,\n",
       "          7.7145e-04,  1.1941e-04,  2.2456e-03, -4.7287e-04,  7.5970e-04,\n",
       "          1.1336e-03,  2.0063e-03,  4.1886e-03, -2.2290e-03, -7.3720e-04,\n",
       "          1.1941e-03,  3.6720e-03, -3.3634e-04, -2.8353e-03, -2.3141e-03,\n",
       "         -8.9479e-04, -1.4907e-03, -3.7048e-03,  4.3743e-04, -1.2636e-03,\n",
       "          1.5213e-04,  2.6436e-03,  2.7641e-04, -8.1226e-04, -1.7089e-03,\n",
       "         -1.9612e-03, -2.8755e-03, -5.8450e-04,  2.2746e-03, -3.6643e-03,\n",
       "          2.8531e-03, -2.5807e-03,  1.9209e-03,  7.6666e-04,  3.3812e-03,\n",
       "         -5.2640e-03, -2.2942e-03, -2.1633e-03, -3.0427e-03,  5.0678e-03,\n",
       "         -1.9333e-03,  1.3950e-03,  1.0026e-03,  3.6470e-04, -6.8728e-03,\n",
       "          1.9126e-03, -2.1981e-03,  3.7145e-03, -2.3869e-03, -3.4550e-03,\n",
       "          2.3353e-03, -1.7261e-03, -4.9595e-06,  7.9799e-04, -1.9155e-03,\n",
       "         -3.2858e-03,  1.5209e-03,  2.8038e-03, -1.5345e-03, -7.1776e-04,\n",
       "          8.4029e-04,  5.9799e-03]),\n",
       " tensor([[ 1.2530e-06,  8.9007e-06,  1.7908e-05,  ...,  1.3450e-05,\n",
       "          -2.1966e-05, -7.8688e-06],\n",
       "         [ 1.6558e-06, -1.0503e-05, -1.0191e-05,  ..., -1.1059e-05,\n",
       "           6.4004e-06,  5.8977e-06],\n",
       "         [ 4.7203e-06, -9.4272e-06,  4.7937e-07,  ..., -5.8393e-06,\n",
       "          -1.4018e-06,  2.7272e-06],\n",
       "         ...,\n",
       "         [-5.2649e-05,  1.4169e-04,  4.2497e-06,  ...,  7.3972e-05,\n",
       "           5.9652e-05,  4.2475e-05],\n",
       "         [-4.7860e-05,  2.4436e-04,  1.0683e-04,  ...,  5.8366e-05,\n",
       "          -3.7209e-05, -9.4825e-06],\n",
       "         [-9.7004e-05, -1.7558e-04, -3.8584e-04,  ..., -7.5467e-05,\n",
       "           3.2311e-04,  9.7560e-05]]),\n",
       " tensor([[ 1.3588e-05, -1.3156e-05,  4.0203e-06,  ...,  8.6471e-06,\n",
       "           1.7927e-06,  1.0653e-05],\n",
       "         [-9.2050e-06,  8.6503e-06, -2.6943e-06,  ..., -3.4186e-06,\n",
       "          -1.1714e-06, -7.8992e-06],\n",
       "         [-7.1719e-06,  4.1805e-06,  2.3711e-06,  ..., -3.0822e-06,\n",
       "          -5.6740e-07, -5.6176e-06],\n",
       "         ...,\n",
       "         [ 8.9965e-05, -5.6252e-05, -2.6097e-05,  ...,  3.6862e-05,\n",
       "          -1.7109e-05,  5.5639e-05],\n",
       "         [ 1.0126e-04, -8.2116e-05, -6.8413e-06,  ...,  5.5904e-05,\n",
       "           3.6328e-06,  7.1878e-05],\n",
       "         [ 1.3637e-05,  1.2564e-05, -1.1314e-05,  ...,  1.2908e-06,\n",
       "          -2.2738e-05,  1.6062e-05]]),\n",
       " tensor([ 9.5060e-05, -5.9279e-05, -3.7702e-05, -5.6807e-05, -2.4722e-05,\n",
       "          1.5849e-05, -1.8376e-04,  6.3973e-05,  1.2868e-04, -8.6714e-06,\n",
       "          4.2075e-05, -6.7614e-05,  1.2277e-04, -1.7688e-05,  7.2718e-05,\n",
       "         -1.2468e-05,  5.4905e-06,  5.8070e-05,  8.7347e-05,  2.5900e-07,\n",
       "          3.8867e-05,  7.3655e-05,  4.0626e-05, -9.4740e-05, -1.1776e-04,\n",
       "         -4.6781e-05, -3.0835e-05, -1.4633e-05, -6.9781e-05, -1.5768e-04,\n",
       "         -8.0500e-06,  1.6310e-06,  5.5754e-05,  4.7006e-05,  5.0285e-05,\n",
       "         -8.3741e-07,  1.3891e-04,  1.3947e-05, -3.5946e-05, -7.3499e-05,\n",
       "         -6.0817e-05,  2.9909e-05,  4.8891e-06,  1.0139e-05, -3.2623e-07,\n",
       "          7.6823e-06, -1.9497e-04, -8.7593e-05, -7.8544e-05,  1.2626e-06,\n",
       "          3.4103e-06,  1.9215e-05, -5.6682e-05, -3.0087e-05, -5.1470e-06,\n",
       "          1.0568e-05,  1.3131e-05,  8.8174e-06, -2.0145e-05, -6.7540e-06,\n",
       "          3.4633e-05,  7.9370e-06,  1.5396e-05, -1.3139e-05, -1.0110e-04,\n",
       "          5.8318e-05,  9.3548e-06, -2.1086e-05, -4.0642e-05, -8.4940e-05,\n",
       "          6.1032e-05,  7.1832e-05,  2.0350e-05,  7.1109e-06,  6.1401e-05,\n",
       "         -4.2296e-05, -1.3479e-05, -9.2803e-06, -1.5894e-04,  3.8546e-05,\n",
       "         -2.1633e-05, -6.9175e-05, -2.2182e-04,  2.1634e-06,  1.5424e-05,\n",
       "          2.2050e-05,  2.5477e-05,  1.8939e-04,  4.0396e-05,  1.8965e-04,\n",
       "          2.4608e-05,  1.4325e-06, -7.3770e-05, -4.6497e-05, -7.8365e-05,\n",
       "          3.6773e-05,  2.4331e-05, -3.8036e-05, -5.7419e-05, -1.5918e-05,\n",
       "          2.7936e-06,  1.5370e-06, -5.4643e-05,  2.4872e-05, -1.2962e-06,\n",
       "          8.1270e-06, -3.1507e-05, -1.3976e-05,  3.4999e-05, -4.9154e-05,\n",
       "         -4.5340e-05,  4.3219e-05,  2.4457e-04,  5.0384e-07, -3.0394e-05,\n",
       "         -5.1216e-05,  1.6606e-05,  4.0397e-05,  1.0566e-05, -6.3246e-06,\n",
       "          3.2331e-05, -4.0154e-05,  3.3092e-05, -5.4761e-07,  1.3412e-04,\n",
       "          8.8510e-06,  1.5925e-05,  4.6183e-05,  4.4286e-03,  3.4587e-03,\n",
       "         -2.9079e-03,  4.7820e-03,  7.2239e-04,  1.9335e-03,  1.0202e-02,\n",
       "          3.9816e-03, -4.0741e-03,  1.1974e-03,  4.0584e-03,  1.8902e-03,\n",
       "          3.8067e-03,  1.4105e-03,  3.2688e-03, -1.9403e-03, -2.4182e-04,\n",
       "         -2.3406e-03, -4.8306e-03,  4.9162e-04, -3.1746e-03,  1.9468e-03,\n",
       "         -5.0031e-03, -5.2862e-03,  4.5581e-03, -4.2868e-03, -3.1711e-03,\n",
       "          8.2225e-04,  1.9862e-03, -6.5420e-03,  5.5124e-03, -6.2291e-04,\n",
       "         -4.5883e-03, -4.3357e-03, -3.5160e-03, -1.6832e-03,  7.5830e-03,\n",
       "          1.6794e-03, -4.2720e-03, -1.8220e-03, -2.5038e-03,  4.1543e-03,\n",
       "          2.8107e-03,  1.4077e-03,  9.9512e-05,  1.5773e-04, -7.4191e-03,\n",
       "          3.8843e-03,  5.8911e-03, -4.9061e-04, -7.5076e-05,  1.7603e-03,\n",
       "         -2.4648e-03, -4.9431e-03,  8.7211e-04, -1.4145e-03,  1.2124e-03,\n",
       "         -6.4716e-04,  8.8056e-04,  8.4979e-04,  5.1572e-03,  1.3133e-03,\n",
       "          1.8274e-03, -5.6932e-04]),\n",
       " tensor([ 9.5060e-05, -5.9279e-05, -3.7702e-05, -5.6807e-05, -2.4722e-05,\n",
       "          1.5849e-05, -1.8376e-04,  6.3973e-05,  1.2868e-04, -8.6714e-06,\n",
       "          4.2075e-05, -6.7614e-05,  1.2277e-04, -1.7688e-05,  7.2718e-05,\n",
       "         -1.2468e-05,  5.4905e-06,  5.8070e-05,  8.7347e-05,  2.5900e-07,\n",
       "          3.8867e-05,  7.3655e-05,  4.0626e-05, -9.4740e-05, -1.1776e-04,\n",
       "         -4.6781e-05, -3.0835e-05, -1.4633e-05, -6.9781e-05, -1.5768e-04,\n",
       "         -8.0500e-06,  1.6310e-06,  5.5754e-05,  4.7006e-05,  5.0285e-05,\n",
       "         -8.3741e-07,  1.3891e-04,  1.3947e-05, -3.5946e-05, -7.3499e-05,\n",
       "         -6.0817e-05,  2.9909e-05,  4.8891e-06,  1.0139e-05, -3.2623e-07,\n",
       "          7.6823e-06, -1.9497e-04, -8.7593e-05, -7.8544e-05,  1.2626e-06,\n",
       "          3.4103e-06,  1.9215e-05, -5.6682e-05, -3.0087e-05, -5.1470e-06,\n",
       "          1.0568e-05,  1.3131e-05,  8.8174e-06, -2.0145e-05, -6.7540e-06,\n",
       "          3.4633e-05,  7.9370e-06,  1.5396e-05, -1.3139e-05, -1.0110e-04,\n",
       "          5.8318e-05,  9.3548e-06, -2.1086e-05, -4.0642e-05, -8.4940e-05,\n",
       "          6.1032e-05,  7.1832e-05,  2.0350e-05,  7.1109e-06,  6.1401e-05,\n",
       "         -4.2296e-05, -1.3479e-05, -9.2803e-06, -1.5894e-04,  3.8546e-05,\n",
       "         -2.1633e-05, -6.9175e-05, -2.2182e-04,  2.1634e-06,  1.5424e-05,\n",
       "          2.2050e-05,  2.5477e-05,  1.8939e-04,  4.0396e-05,  1.8965e-04,\n",
       "          2.4608e-05,  1.4325e-06, -7.3770e-05, -4.6497e-05, -7.8365e-05,\n",
       "          3.6773e-05,  2.4331e-05, -3.8036e-05, -5.7419e-05, -1.5918e-05,\n",
       "          2.7936e-06,  1.5370e-06, -5.4643e-05,  2.4872e-05, -1.2962e-06,\n",
       "          8.1270e-06, -3.1507e-05, -1.3976e-05,  3.4999e-05, -4.9154e-05,\n",
       "         -4.5340e-05,  4.3219e-05,  2.4457e-04,  5.0384e-07, -3.0394e-05,\n",
       "         -5.1216e-05,  1.6606e-05,  4.0397e-05,  1.0566e-05, -6.3246e-06,\n",
       "          3.2331e-05, -4.0154e-05,  3.3092e-05, -5.4761e-07,  1.3412e-04,\n",
       "          8.8510e-06,  1.5925e-05,  4.6183e-05,  2.1281e-03,  1.8618e-03,\n",
       "         -1.4556e-03,  2.5500e-03,  3.5459e-04,  9.6173e-04,  5.1438e-03,\n",
       "          1.9517e-03, -2.1281e-03,  6.2838e-04,  1.9315e-03,  1.0150e-03,\n",
       "          1.8329e-03,  7.6166e-04,  1.6837e-03, -1.0058e-03, -1.2064e-04,\n",
       "         -1.1547e-03, -2.6613e-03,  2.6068e-04, -1.4863e-03,  1.0103e-03,\n",
       "         -2.5416e-03, -2.4825e-03,  2.4886e-03, -2.0426e-03, -1.5528e-03,\n",
       "          3.8977e-04,  9.5136e-04, -3.4157e-03,  2.5023e-03, -2.7100e-04,\n",
       "         -2.3146e-03, -2.0724e-03, -1.6530e-03, -8.9920e-04,  3.7038e-03,\n",
       "          8.5562e-04, -2.0488e-03, -9.4088e-04, -1.2130e-03,  1.9148e-03,\n",
       "          1.3394e-03,  6.7679e-04,  2.2726e-05,  7.6702e-05, -3.7407e-03,\n",
       "          2.0463e-03,  3.0337e-03, -2.3870e-04, -2.4625e-05,  9.5639e-04,\n",
       "         -1.0935e-03, -2.3897e-03,  4.2642e-04, -7.0220e-04,  5.8272e-04,\n",
       "         -3.4946e-04,  3.6784e-04,  4.7353e-04,  2.2937e-03,  5.7474e-04,\n",
       "          7.9532e-04, -2.1847e-04]),\n",
       " tensor([[-3.7329e-06,  4.6439e-06,  6.4850e-06,  ..., -4.7886e-06,\n",
       "          -1.9329e-06, -6.6763e-06],\n",
       "         [ 2.0729e-07, -1.6601e-06, -2.2517e-06,  ...,  1.0450e-06,\n",
       "           8.4128e-08,  2.1057e-06],\n",
       "         [ 2.5045e-05, -1.4042e-05, -2.8824e-05,  ...,  2.0212e-05,\n",
       "           1.6450e-06,  2.7304e-05],\n",
       "         ...,\n",
       "         [-3.5370e-04,  2.1513e-04,  3.9314e-04,  ..., -2.9288e-04,\n",
       "           1.7783e-05, -3.6654e-04],\n",
       "         [ 8.6457e-05, -1.9095e-04, -3.2685e-04,  ...,  1.6599e-04,\n",
       "           3.4280e-06,  2.9146e-04],\n",
       "         [ 5.9386e-04, -4.0496e-04, -8.4149e-04,  ...,  5.4963e-04,\n",
       "          -2.4727e-04,  7.3456e-04]]),\n",
       " tensor([[ 5.9257e-07,  4.4545e-06, -1.4671e-05,  ...,  5.9765e-06,\n",
       "          -3.3598e-07, -3.8911e-06],\n",
       "         [-6.0544e-07, -1.5160e-06,  4.3414e-06,  ..., -1.0632e-06,\n",
       "          -1.3163e-06,  1.9264e-06],\n",
       "         [-7.9828e-06, -1.5897e-05,  6.3176e-05,  ..., -2.2692e-05,\n",
       "          -3.8524e-06,  1.6757e-05],\n",
       "         ...,\n",
       "         [ 2.2745e-05,  1.1600e-04, -3.9264e-04,  ...,  1.2091e-04,\n",
       "           5.8158e-05, -9.9191e-05],\n",
       "         [-2.6673e-05, -9.6635e-05,  2.9312e-04,  ..., -8.2354e-05,\n",
       "          -5.2527e-05,  1.0920e-04],\n",
       "         [-7.2002e-05, -2.6847e-04,  7.6926e-04,  ..., -2.1782e-04,\n",
       "          -1.4491e-04,  2.0890e-04]]),\n",
       " tensor([-7.0647e-05,  1.7962e-05,  2.9795e-04, -1.8668e-04,  6.5683e-05,\n",
       "         -2.4441e-04,  1.9430e-04,  9.8259e-05, -1.6351e-04, -2.4001e-06,\n",
       "          5.8814e-05, -1.9748e-05,  2.0660e-05, -2.2921e-04,  1.7605e-04,\n",
       "          4.3543e-05,  9.7351e-05, -3.1440e-05, -2.3342e-04,  1.7250e-04,\n",
       "         -7.8573e-05, -9.8773e-06,  1.5789e-05,  2.1885e-05,  3.7654e-05,\n",
       "          3.5344e-05, -1.1331e-04,  1.3283e-04, -8.5552e-05,  1.4475e-05,\n",
       "          4.9204e-05, -3.1035e-05, -1.6405e-04,  2.5943e-05, -7.5851e-05,\n",
       "          1.6205e-04, -5.1868e-05,  1.8603e-04, -4.0354e-05,  2.7365e-04,\n",
       "          9.2456e-05,  2.5522e-06, -2.8655e-05,  1.2064e-05,  3.5224e-05,\n",
       "          1.5452e-04,  3.4215e-05,  6.6231e-06, -3.6682e-05,  6.0384e-06,\n",
       "         -2.4022e-05, -6.6344e-05,  2.2273e-04, -1.1707e-05, -6.3380e-05,\n",
       "         -2.8189e-04, -8.1956e-05, -1.1745e-04, -9.6417e-06, -7.1297e-05,\n",
       "          3.0158e-04, -1.4725e-04, -6.9985e-05, -8.3164e-05,  6.0108e-05,\n",
       "         -5.2513e-06,  4.0152e-06,  5.9547e-05,  6.5950e-06,  6.7298e-05,\n",
       "          8.1806e-05, -4.8669e-06,  5.6335e-05,  1.2053e-05, -3.9427e-05,\n",
       "         -1.1954e-04, -3.1249e-05,  5.9664e-05, -1.0987e-04, -4.6886e-05,\n",
       "          2.0407e-05,  3.6907e-06, -8.9191e-05, -1.8515e-05, -3.3998e-05,\n",
       "          8.3909e-05,  3.0079e-05, -5.7091e-06,  3.8803e-05, -4.8115e-05,\n",
       "          7.6431e-05,  3.3390e-06, -6.4130e-05, -2.0735e-05, -7.3462e-05,\n",
       "          1.3944e-05, -5.4592e-05, -1.7869e-05,  4.4612e-05, -6.4715e-05,\n",
       "         -1.3582e-06,  3.2310e-05,  5.4952e-05, -1.0544e-05, -1.1352e-05,\n",
       "         -5.8565e-06, -2.3631e-05,  3.6654e-05,  1.3814e-05, -2.4259e-06,\n",
       "          1.8373e-04,  1.2855e-05, -1.1123e-05, -3.6633e-05,  7.4412e-07,\n",
       "         -1.7295e-05, -9.3874e-05, -1.4493e-05, -2.9436e-05,  1.7385e-05,\n",
       "          7.1331e-06,  2.8400e-05, -1.3385e-05,  4.3834e-05, -1.2811e-05,\n",
       "         -2.3047e-05, -4.2238e-05, -4.5843e-06, -3.4343e-03,  1.5661e-03,\n",
       "          9.8337e-03,  6.1182e-03, -5.5198e-03, -8.2403e-03, -6.9562e-03,\n",
       "          2.6662e-03, -4.8793e-03, -1.2622e-04, -5.7001e-03,  6.3074e-03,\n",
       "          2.3186e-03, -5.5005e-03, -6.0061e-03,  1.3585e-02,  5.5624e-03,\n",
       "         -2.8355e-03,  8.8822e-03,  5.3015e-03, -1.1722e-03, -1.5903e-04,\n",
       "          9.4641e-04,  8.5521e-04, -8.6640e-03, -3.6442e-03, -7.8129e-03,\n",
       "          3.2978e-03,  4.1130e-03, -3.9490e-03,  4.4021e-03,  3.4136e-03,\n",
       "         -5.1201e-03,  6.3718e-04, -6.2887e-03,  1.2417e-02, -1.7142e-03,\n",
       "         -5.0555e-03,  6.9599e-03, -5.4060e-03,  2.5534e-03,  3.7337e-04,\n",
       "          2.4444e-03,  9.0105e-04, -1.2065e-02, -4.2854e-03, -6.9500e-03,\n",
       "          7.1562e-04, -8.1272e-03, -4.8917e-04,  2.4611e-03, -5.5422e-03,\n",
       "          6.0728e-03, -9.7003e-04, -2.8145e-03,  5.9894e-03,  8.4122e-03,\n",
       "          4.2484e-03,  1.3317e-03,  1.5202e-03, -7.8283e-03, -4.2213e-03,\n",
       "          2.8778e-03,  8.0490e-03]),\n",
       " tensor([-7.0647e-05,  1.7962e-05,  2.9795e-04, -1.8668e-04,  6.5683e-05,\n",
       "         -2.4441e-04,  1.9430e-04,  9.8259e-05, -1.6351e-04, -2.4001e-06,\n",
       "          5.8814e-05, -1.9748e-05,  2.0660e-05, -2.2921e-04,  1.7605e-04,\n",
       "          4.3543e-05,  9.7351e-05, -3.1440e-05, -2.3342e-04,  1.7250e-04,\n",
       "         -7.8573e-05, -9.8773e-06,  1.5789e-05,  2.1885e-05,  3.7654e-05,\n",
       "          3.5344e-05, -1.1331e-04,  1.3283e-04, -8.5552e-05,  1.4475e-05,\n",
       "          4.9204e-05, -3.1035e-05, -1.6405e-04,  2.5943e-05, -7.5851e-05,\n",
       "          1.6205e-04, -5.1868e-05,  1.8603e-04, -4.0354e-05,  2.7365e-04,\n",
       "          9.2456e-05,  2.5522e-06, -2.8655e-05,  1.2064e-05,  3.5224e-05,\n",
       "          1.5452e-04,  3.4215e-05,  6.6231e-06, -3.6682e-05,  6.0384e-06,\n",
       "         -2.4022e-05, -6.6344e-05,  2.2273e-04, -1.1707e-05, -6.3380e-05,\n",
       "         -2.8189e-04, -8.1956e-05, -1.1745e-04, -9.6417e-06, -7.1297e-05,\n",
       "          3.0158e-04, -1.4725e-04, -6.9985e-05, -8.3164e-05,  6.0108e-05,\n",
       "         -5.2513e-06,  4.0152e-06,  5.9547e-05,  6.5950e-06,  6.7298e-05,\n",
       "          8.1806e-05, -4.8669e-06,  5.6335e-05,  1.2053e-05, -3.9427e-05,\n",
       "         -1.1954e-04, -3.1249e-05,  5.9664e-05, -1.0987e-04, -4.6886e-05,\n",
       "          2.0407e-05,  3.6907e-06, -8.9191e-05, -1.8515e-05, -3.3998e-05,\n",
       "          8.3909e-05,  3.0079e-05, -5.7091e-06,  3.8803e-05, -4.8115e-05,\n",
       "          7.6431e-05,  3.3390e-06, -6.4130e-05, -2.0735e-05, -7.3462e-05,\n",
       "          1.3944e-05, -5.4592e-05, -1.7869e-05,  4.4612e-05, -6.4715e-05,\n",
       "         -1.3582e-06,  3.2310e-05,  5.4952e-05, -1.0544e-05, -1.1352e-05,\n",
       "         -5.8565e-06, -2.3631e-05,  3.6654e-05,  1.3814e-05, -2.4259e-06,\n",
       "          1.8373e-04,  1.2855e-05, -1.1123e-05, -3.6633e-05,  7.4412e-07,\n",
       "         -1.7295e-05, -9.3874e-05, -1.4493e-05, -2.9436e-05,  1.7385e-05,\n",
       "          7.1331e-06,  2.8400e-05, -1.3385e-05,  4.3834e-05, -1.2811e-05,\n",
       "         -2.3047e-05, -4.2238e-05, -4.5843e-06, -1.6423e-03,  6.8493e-04,\n",
       "          5.0804e-03,  2.8933e-03, -2.6493e-03, -3.6007e-03, -3.3931e-03,\n",
       "          1.2591e-03, -2.9996e-03, -6.1851e-05, -2.5930e-03,  3.7112e-03,\n",
       "          1.0726e-03, -2.8211e-03, -2.7846e-03,  6.9045e-03,  2.6175e-03,\n",
       "         -1.5215e-03,  4.3614e-03,  2.4868e-03, -6.1117e-04, -5.4709e-05,\n",
       "          4.8023e-04,  4.4971e-04, -3.9946e-03, -1.8059e-03, -3.9826e-03,\n",
       "          1.4776e-03,  1.7243e-03, -2.0484e-03,  2.2355e-03,  2.0072e-03,\n",
       "         -2.4985e-03,  3.2732e-04, -3.2511e-03,  5.4223e-03, -7.7367e-04,\n",
       "         -2.3827e-03,  3.4862e-03, -2.6190e-03,  1.3701e-03,  1.6034e-04,\n",
       "          1.0873e-03,  4.6182e-04, -5.7025e-03, -2.4607e-03, -3.1808e-03,\n",
       "          3.3263e-04, -4.2333e-03, -2.5900e-04,  1.1172e-03, -2.7312e-03,\n",
       "          3.0223e-03, -5.1486e-04, -1.2294e-03,  2.6560e-03,  4.2521e-03,\n",
       "          2.2468e-03,  6.3802e-04,  7.2658e-04, -3.6479e-03, -1.9724e-03,\n",
       "          1.2853e-03,  3.9316e-03]),\n",
       " tensor([[-1.0221e-07, -6.1662e-07, -1.0387e-06,  ...,  6.7909e-07,\n",
       "          -4.4553e-07,  9.3693e-07],\n",
       "         [ 2.2450e-06, -3.5516e-06, -3.4882e-06,  ...,  3.6317e-06,\n",
       "           1.6436e-06,  3.1927e-06],\n",
       "         [-1.9137e-05,  1.4818e-05,  1.9320e-05,  ..., -1.5591e-05,\n",
       "          -6.7323e-06, -2.1698e-05],\n",
       "         ...,\n",
       "         [ 7.0096e-04, -5.2244e-04, -6.8040e-04,  ...,  5.9267e-04,\n",
       "           1.7190e-04,  6.9302e-04],\n",
       "         [ 2.5947e-05, -1.2054e-04, -8.4721e-05,  ...,  1.0514e-04,\n",
       "           6.0814e-05,  1.4041e-04],\n",
       "         [-1.9642e-04, -2.9009e-05,  6.8036e-05,  ..., -2.4624e-05,\n",
       "          -9.9754e-05, -8.9261e-05]]),\n",
       " tensor([[-9.8059e-07, -1.0674e-06,  1.1451e-06,  ...,  5.2572e-07,\n",
       "           2.2088e-06,  2.9830e-07],\n",
       "         [-3.3297e-06, -3.6664e-06,  3.0255e-06,  ...,  8.9837e-07,\n",
       "           6.9464e-06,  1.4032e-06],\n",
       "         [ 1.7136e-05,  1.9862e-05, -1.5539e-05,  ..., -5.4387e-06,\n",
       "          -3.5614e-05, -8.2859e-06],\n",
       "         ...,\n",
       "         [-2.8839e-04, -3.2491e-04,  2.6270e-04,  ...,  8.3686e-05,\n",
       "           6.0908e-04,  1.4254e-04],\n",
       "         [-5.8587e-05, -4.8428e-05,  4.6850e-05,  ...,  2.2550e-05,\n",
       "           1.0394e-04,  1.4738e-05],\n",
       "         [ 1.1567e-05,  9.5713e-06,  1.3507e-05,  ...,  4.9818e-06,\n",
       "          -9.5065e-06, -2.2382e-06]]),\n",
       " tensor([ 6.6870e-06,  4.7879e-05, -2.3943e-04,  9.3546e-05,  1.3585e-04,\n",
       "         -9.3339e-05, -1.8668e-05, -1.4070e-05,  1.7335e-05,  6.9621e-05,\n",
       "         -1.1900e-05, -2.3367e-05,  4.5996e-06, -2.3515e-05,  1.3416e-05,\n",
       "         -7.7497e-06, -1.0704e-05, -1.5473e-04,  9.1852e-05, -7.3565e-05,\n",
       "          4.1669e-05, -2.3599e-05, -2.4594e-05,  1.8069e-04,  1.0826e-04,\n",
       "          5.9846e-05,  1.9979e-05,  6.7129e-05, -3.1159e-05,  7.9966e-05,\n",
       "          6.0149e-05, -1.3706e-04,  8.5008e-05, -1.4502e-06, -2.3830e-04,\n",
       "          1.9452e-05, -1.1070e-04, -1.6946e-05,  3.8065e-05,  2.0066e-04,\n",
       "          3.2626e-05,  5.8909e-05, -1.0028e-04, -3.1945e-05, -2.3989e-04,\n",
       "         -3.2383e-05,  3.9780e-05, -4.0216e-05, -1.6978e-05, -9.0923e-06,\n",
       "          1.4048e-04,  1.0583e-04,  3.1815e-04,  1.1565e-04, -2.2951e-05,\n",
       "          1.6112e-05,  9.3330e-05,  1.3514e-04, -4.8643e-05,  2.0022e-05,\n",
       "          7.6569e-05, -1.0030e-04,  5.3332e-05, -3.3521e-05, -4.6397e-05,\n",
       "          3.4294e-05,  2.2139e-04, -1.8086e-04,  5.5689e-05,  1.5408e-05,\n",
       "          2.9496e-06, -1.4333e-04, -2.0406e-06,  1.3795e-05,  1.2298e-05,\n",
       "          1.2859e-05, -1.9370e-06, -6.9749e-06,  1.5439e-05, -4.8592e-05,\n",
       "         -5.5298e-05, -1.0462e-04,  1.1119e-05, -4.8747e-05, -4.4164e-05,\n",
       "          1.1297e-05,  9.1065e-05, -2.6466e-05,  3.1092e-05,  1.2401e-07,\n",
       "          4.5351e-05,  1.7506e-05,  9.1439e-05,  2.5869e-05, -1.6937e-05,\n",
       "          6.8892e-05,  9.1623e-05,  3.8796e-05,  1.8660e-04,  2.3936e-06,\n",
       "         -1.2024e-04,  6.7875e-05, -5.0242e-05,  1.0735e-04,  1.0135e-05,\n",
       "         -1.8011e-04,  4.4453e-04, -1.1442e-05,  1.2332e-04,  8.4066e-05,\n",
       "         -6.5093e-05,  5.3752e-05,  7.5607e-05, -5.3563e-05, -1.3890e-04,\n",
       "         -3.6241e-04, -2.0206e-04, -1.6671e-05, -2.3802e-06,  1.8627e-05,\n",
       "          3.7617e-06, -1.8044e-04, -4.1176e-05, -4.2339e-05, -9.6944e-05,\n",
       "         -1.2005e-05, -3.6312e-05,  3.7262e-05, -2.8203e-04,  2.5853e-03,\n",
       "         -8.2060e-03,  3.7442e-03, -9.0941e-03, -3.5918e-03, -1.6821e-03,\n",
       "         -1.1155e-02, -1.2988e-03,  8.4403e-04, -3.0224e-03,  8.8057e-04,\n",
       "         -2.4500e-04,  3.8033e-03,  3.4129e-03, -2.4927e-03,  2.6045e-03,\n",
       "          8.4136e-03, -3.6677e-03, -6.8758e-03, -1.2704e-03,  1.3354e-03,\n",
       "          3.2990e-03,  4.2124e-03,  7.0842e-03, -2.4949e-03, -1.0383e-03,\n",
       "          1.9542e-03,  4.0824e-03,  2.7264e-03, -4.6611e-03,  5.5528e-03,\n",
       "         -5.0671e-03,  5.2378e-04, -6.9590e-03, -1.7573e-03, -4.4312e-03,\n",
       "         -2.5811e-03, -3.2813e-03, -9.6866e-03,  2.4183e-03, -9.6574e-03,\n",
       "         -8.8101e-03,  7.5731e-04, -5.5315e-03, -4.6983e-03, -1.6942e-03,\n",
       "          3.0532e-03,  2.0510e-03, -2.2516e-03, -1.1354e-02,  8.7755e-03,\n",
       "         -8.9360e-03, -4.4990e-03, -9.4383e-04, -5.0254e-03,  4.1407e-03,\n",
       "         -4.9267e-03,  3.4476e-03,  5.3346e-03,  4.5284e-03,  8.6265e-03,\n",
       "          1.1262e-03, -9.7567e-04]),\n",
       " tensor([ 6.6870e-06,  4.7879e-05, -2.3943e-04,  9.3546e-05,  1.3585e-04,\n",
       "         -9.3339e-05, -1.8668e-05, -1.4070e-05,  1.7335e-05,  6.9621e-05,\n",
       "         -1.1900e-05, -2.3367e-05,  4.5996e-06, -2.3515e-05,  1.3416e-05,\n",
       "         -7.7497e-06, -1.0704e-05, -1.5473e-04,  9.1852e-05, -7.3565e-05,\n",
       "          4.1669e-05, -2.3599e-05, -2.4594e-05,  1.8069e-04,  1.0826e-04,\n",
       "          5.9846e-05,  1.9979e-05,  6.7129e-05, -3.1159e-05,  7.9966e-05,\n",
       "          6.0149e-05, -1.3706e-04,  8.5008e-05, -1.4502e-06, -2.3830e-04,\n",
       "          1.9452e-05, -1.1070e-04, -1.6946e-05,  3.8065e-05,  2.0066e-04,\n",
       "          3.2626e-05,  5.8909e-05, -1.0028e-04, -3.1945e-05, -2.3989e-04,\n",
       "         -3.2383e-05,  3.9780e-05, -4.0216e-05, -1.6978e-05, -9.0923e-06,\n",
       "          1.4048e-04,  1.0583e-04,  3.1815e-04,  1.1565e-04, -2.2951e-05,\n",
       "          1.6112e-05,  9.3330e-05,  1.3514e-04, -4.8643e-05,  2.0022e-05,\n",
       "          7.6569e-05, -1.0030e-04,  5.3332e-05, -3.3521e-05, -4.6397e-05,\n",
       "          3.4294e-05,  2.2139e-04, -1.8086e-04,  5.5689e-05,  1.5408e-05,\n",
       "          2.9496e-06, -1.4333e-04, -2.0406e-06,  1.3795e-05,  1.2298e-05,\n",
       "          1.2859e-05, -1.9370e-06, -6.9749e-06,  1.5439e-05, -4.8592e-05,\n",
       "         -5.5298e-05, -1.0462e-04,  1.1119e-05, -4.8747e-05, -4.4164e-05,\n",
       "          1.1297e-05,  9.1065e-05, -2.6466e-05,  3.1092e-05,  1.2401e-07,\n",
       "          4.5351e-05,  1.7506e-05,  9.1439e-05,  2.5869e-05, -1.6937e-05,\n",
       "          6.8892e-05,  9.1623e-05,  3.8796e-05,  1.8660e-04,  2.3936e-06,\n",
       "         -1.2024e-04,  6.7875e-05, -5.0242e-05,  1.0735e-04,  1.0135e-05,\n",
       "         -1.8011e-04,  4.4453e-04, -1.1442e-05,  1.2332e-04,  8.4066e-05,\n",
       "         -6.5093e-05,  5.3752e-05,  7.5607e-05, -5.3563e-05, -1.3890e-04,\n",
       "         -3.6241e-04, -2.0206e-04, -1.6671e-05, -2.3802e-06,  1.8627e-05,\n",
       "          3.7617e-06, -1.8044e-04, -4.1176e-05, -4.2339e-05, -9.6944e-05,\n",
       "         -1.2005e-05, -3.6312e-05,  3.7262e-05, -1.5076e-04,  1.1859e-03,\n",
       "         -3.9496e-03,  1.9003e-03, -4.5288e-03, -2.0305e-03, -9.2495e-04,\n",
       "         -5.7001e-03, -6.3395e-04,  4.3750e-04, -1.6216e-03,  4.6883e-04,\n",
       "         -1.3828e-04,  1.8789e-03,  1.6884e-03, -1.2438e-03,  1.4597e-03,\n",
       "          3.4952e-03, -1.7220e-03, -3.5553e-03, -5.8210e-04,  6.6199e-04,\n",
       "          1.4597e-03,  2.2253e-03,  3.6573e-03, -1.1480e-03, -5.1432e-04,\n",
       "          1.0613e-03,  2.0675e-03,  1.3671e-03, -2.4130e-03,  2.6090e-03,\n",
       "         -2.5610e-03,  2.8103e-04, -3.6979e-03, -9.5385e-04, -2.3258e-03,\n",
       "         -1.3457e-03, -1.6200e-03, -5.0816e-03,  1.1940e-03, -4.4985e-03,\n",
       "         -4.6597e-03,  3.9250e-04, -2.6522e-03, -2.2330e-03, -9.5092e-04,\n",
       "          1.3715e-03,  1.1590e-03, -1.1510e-03, -6.1682e-03,  4.6413e-03,\n",
       "         -4.6757e-03, -2.2789e-03, -5.3604e-04, -2.4367e-03,  1.9852e-03,\n",
       "         -2.3694e-03,  1.4559e-03,  2.6789e-03,  2.4816e-03,  4.0686e-03,\n",
       "          5.7546e-04, -5.0147e-04]),\n",
       " tensor([[-1.8655e-04],\n",
       "         [-4.2284e-05],\n",
       "         [ 1.1062e-04],\n",
       "         [-1.9161e-05],\n",
       "         [ 3.3844e-05],\n",
       "         [ 5.7418e-05],\n",
       "         [-9.4308e-06],\n",
       "         [ 1.1478e-04],\n",
       "         [-1.0916e-04],\n",
       "         [-5.9818e-05],\n",
       "         [ 1.1990e-04],\n",
       "         [-5.3719e-05],\n",
       "         [-3.0634e-04],\n",
       "         [-9.3969e-05],\n",
       "         [-8.0146e-05],\n",
       "         [-7.6933e-05],\n",
       "         [-7.6564e-05],\n",
       "         [ 9.8631e-05],\n",
       "         [ 2.3940e-04],\n",
       "         [ 1.5427e-04],\n",
       "         [-2.7094e-04],\n",
       "         [ 1.8926e-04],\n",
       "         [-5.6341e-05],\n",
       "         [-1.9620e-05],\n",
       "         [ 1.0203e-04],\n",
       "         [-2.0620e-04],\n",
       "         [-1.3049e-04],\n",
       "         [ 2.5163e-05],\n",
       "         [ 7.4161e-05],\n",
       "         [ 1.9618e-04],\n",
       "         [ 1.9976e-05],\n",
       "         [-2.0956e-04],\n",
       "         [-8.8433e-05],\n",
       "         [ 3.3110e-05],\n",
       "         [ 1.2716e-04],\n",
       "         [-5.6258e-05],\n",
       "         [ 9.0298e-06],\n",
       "         [-1.2978e-04],\n",
       "         [ 5.0493e-05],\n",
       "         [-7.8221e-05],\n",
       "         [ 4.9615e-05],\n",
       "         [-2.2460e-04],\n",
       "         [ 8.0407e-06],\n",
       "         [ 6.6968e-05],\n",
       "         [ 3.7999e-05],\n",
       "         [-2.0707e-04],\n",
       "         [ 1.4393e-04],\n",
       "         [-7.9947e-05],\n",
       "         [-6.4437e-05],\n",
       "         [-2.8046e-04],\n",
       "         [-6.0349e-05],\n",
       "         [-1.1534e-04],\n",
       "         [ 9.7642e-06],\n",
       "         [ 2.3521e-04],\n",
       "         [-9.5371e-05],\n",
       "         [ 6.9857e-05],\n",
       "         [-7.0324e-06],\n",
       "         [-2.3293e-04],\n",
       "         [-3.7176e-06],\n",
       "         [ 1.0099e-04],\n",
       "         [-3.3031e-05],\n",
       "         [ 1.2238e-05],\n",
       "         [-1.4659e-04],\n",
       "         [ 1.3849e-04],\n",
       "         [-2.0315e-05],\n",
       "         [-7.0090e-05],\n",
       "         [ 5.3253e-05],\n",
       "         [ 1.4639e-04],\n",
       "         [ 1.8871e-05],\n",
       "         [ 1.1068e-04],\n",
       "         [-9.9668e-05],\n",
       "         [ 6.5553e-05],\n",
       "         [ 2.4720e-05],\n",
       "         [-9.0130e-05],\n",
       "         [ 1.7364e-04],\n",
       "         [-5.4703e-05],\n",
       "         [-2.0688e-04],\n",
       "         [-1.4929e-05],\n",
       "         [-2.8361e-05],\n",
       "         [ 1.1296e-04],\n",
       "         [-1.4209e-05],\n",
       "         [ 3.8388e-05],\n",
       "         [ 7.0513e-05],\n",
       "         [ 1.5045e-04],\n",
       "         [ 2.2346e-05],\n",
       "         [-1.8140e-05],\n",
       "         [-2.8296e-04],\n",
       "         [-8.3717e-05],\n",
       "         [ 1.7071e-04],\n",
       "         [ 3.8395e-04],\n",
       "         [ 1.7235e-04],\n",
       "         [ 2.4822e-05],\n",
       "         [ 4.9088e-05],\n",
       "         [-1.5809e-04],\n",
       "         [-2.2253e-04],\n",
       "         [-1.7175e-04],\n",
       "         [-2.1646e-04],\n",
       "         [-8.6087e-06],\n",
       "         [-5.7136e-05],\n",
       "         [-4.9825e-06],\n",
       "         [-1.2542e-04],\n",
       "         [ 1.0285e-04],\n",
       "         [-2.1737e-05],\n",
       "         [-2.5829e-04],\n",
       "         [-8.8127e-05],\n",
       "         [-5.4137e-05],\n",
       "         [ 1.1917e-04],\n",
       "         [-1.5590e-04],\n",
       "         [-7.5259e-05],\n",
       "         [ 5.9865e-06],\n",
       "         [ 2.8953e-04],\n",
       "         [-2.3532e-05],\n",
       "         [ 1.0205e-04],\n",
       "         [-1.5459e-04],\n",
       "         [ 9.0575e-05],\n",
       "         [ 1.5814e-05],\n",
       "         [ 1.0770e-04],\n",
       "         [ 6.8586e-05],\n",
       "         [ 1.4098e-04],\n",
       "         [-1.2115e-04],\n",
       "         [-1.0450e-04],\n",
       "         [-1.0205e-04],\n",
       "         [ 9.6321e-05],\n",
       "         [-5.0971e-05],\n",
       "         [-9.5996e-05],\n",
       "         [ 8.7794e-05],\n",
       "         [ 2.8425e-04],\n",
       "         [ 9.5057e-05]]),\n",
       " tensor([[-6.1673e-06, -5.9079e-06,  5.6786e-06,  ...,  3.8758e-06,\n",
       "          -1.1519e-05, -7.0699e-06],\n",
       "         [-8.7498e-06, -8.5006e-06,  8.0537e-06,  ...,  5.5212e-06,\n",
       "          -1.6634e-05, -1.0097e-05],\n",
       "         [-2.1964e-05, -2.2111e-05,  2.1148e-05,  ...,  1.4312e-05,\n",
       "          -4.1873e-05, -2.5712e-05],\n",
       "         ...,\n",
       "         [ 1.2425e-05,  1.2467e-05, -1.1633e-05,  ..., -8.0253e-06,\n",
       "           2.3838e-05,  1.4423e-05],\n",
       "         [-2.6079e-06, -2.6958e-06,  2.6382e-06,  ...,  1.7456e-06,\n",
       "          -4.9331e-06, -3.0625e-06],\n",
       "         [-2.3442e-05, -2.3857e-05,  2.2486e-05,  ...,  1.5493e-05,\n",
       "          -4.5383e-05, -2.7634e-05]]),\n",
       " tensor([ 4.3333e-07, -2.3854e-07,  2.9008e-06,  8.6876e-07, -4.6437e-07,\n",
       "         -2.1327e-07, -6.3058e-07, -3.0147e-07, -3.6804e-07, -4.0028e-06,\n",
       "         -8.2307e-07,  2.8164e-07, -7.0142e-06,  8.0149e-07, -6.6593e-06,\n",
       "          3.6164e-06,  1.2775e-06,  4.9178e-06,  2.3790e-06,  1.2822e-06,\n",
       "         -8.2069e-08, -6.0061e-07,  4.1967e-07, -4.4913e-07,  1.4028e-06,\n",
       "         -1.1372e-06, -1.9386e-06,  4.3925e-07, -1.0872e-06, -3.7894e-06,\n",
       "         -2.7841e-06, -7.3680e-06, -2.0843e-06, -1.6852e-08,  1.2106e-08,\n",
       "          1.1916e-06, -1.3660e-06, -2.7380e-07,  2.9394e-08,  6.4610e-07,\n",
       "         -3.2611e-07,  1.6410e-07, -2.5323e-07,  3.0758e-07,  1.8160e-06,\n",
       "         -2.6809e-07, -2.8858e-06, -2.3913e-06, -1.1614e-06,  3.2551e-06,\n",
       "          4.6705e-08, -2.2840e-06, -1.4344e-06, -1.9794e-07,  4.0513e-06,\n",
       "          1.0305e-07,  2.3673e-06, -2.9784e-07, -2.4545e-06, -6.6911e-07,\n",
       "          9.3324e-07, -3.5852e-07, -1.7546e-06,  4.5580e-06,  1.7036e-06,\n",
       "          1.7534e-06,  1.8326e-06, -3.1767e-06,  4.1522e-07,  1.1095e-06,\n",
       "         -2.8278e-06,  9.4660e-07, -2.1566e-07, -1.5059e-07,  3.1968e-07,\n",
       "         -4.3700e-07, -3.0381e-06,  5.4784e-07,  1.2141e-07, -2.7962e-06,\n",
       "         -2.5853e-07, -1.9773e-06, -2.9822e-09, -1.4529e-06,  1.1083e-06,\n",
       "         -4.2199e-07,  1.1781e-07,  2.0457e-06, -3.5896e-06,  7.5835e-06,\n",
       "         -5.0112e-07, -6.3151e-08,  1.9948e-06,  2.5223e-07, -5.1521e-07,\n",
       "         -1.5476e-07,  9.5157e-06, -2.0809e-08, -9.0770e-09,  1.8831e-06,\n",
       "          4.9117e-07,  8.0796e-07,  4.9157e-08, -1.6317e-06,  3.7756e-06,\n",
       "          1.0219e-06, -1.4911e-07,  3.9913e-06, -1.7842e-06, -6.5714e-07,\n",
       "         -4.8780e-06, -1.0856e-06, -2.1319e-06, -8.5474e-07,  1.6235e-07,\n",
       "         -8.2531e-07, -9.6957e-10, -1.0775e-06,  6.2290e-06, -1.1700e-06,\n",
       "         -4.0366e-06,  3.3026e-06, -7.0991e-07,  1.5392e-06, -5.5119e-08,\n",
       "         -8.6259e-07,  1.0967e-06,  3.5271e-06])]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x.grad for x in list(model.encoder.parameters())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.training import trainer as _trainer\n",
    "from allennlp.training.trainer import *\n",
    "import math\n",
    "logger = _trainer.logger\n",
    "\n",
    "N_BATCHES_PER_UPDATE = config.batch_size // config.computational_batch_size\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    @gpu_mem_restore\n",
    "    def _train_epoch(self, epoch: int) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Trains one epoch and returns metrics. Copied from source\n",
    "        \"\"\"\n",
    "        logger.info(\"Epoch %d/%d\", epoch, self._num_epochs - 1)\n",
    "        peak_cpu_usage = peak_memory_mb()\n",
    "        logger.info(f\"Peak CPU memory usage MB: {peak_cpu_usage}\")\n",
    "        gpu_usage = []\n",
    "        for gpu, memory in gpu_memory_mb().items():\n",
    "            gpu_usage.append((gpu, memory))\n",
    "            logger.info(f\"GPU {gpu} memory usage MB: {memory}\")\n",
    "\n",
    "        train_loss = 0.0\n",
    "        # Set the model to \"train\" mode.\n",
    "        self.model.train()\n",
    "\n",
    "        # Get tqdm for the training batches\n",
    "        train_generator = self.iterator(self.train_data,\n",
    "                                        num_epochs=1,\n",
    "                                        shuffle=self.shuffle)\n",
    "        num_training_batches = self.iterator.get_num_batches(self.train_data)\n",
    "        self._last_log = time.time()\n",
    "        last_save_time = time.time()\n",
    "\n",
    "        batches_this_epoch = 0\n",
    "        if self._batch_num_total is None:\n",
    "            self._batch_num_total = 0\n",
    "\n",
    "        if self._histogram_interval is not None:\n",
    "            histogram_parameters = set(self.model.get_parameters_for_histogram_tensorboard_logging())\n",
    "\n",
    "        logger.info(\"Training\")\n",
    "        train_generator_tqdm = Tqdm.tqdm(train_generator,\n",
    "                                         total=num_training_batches)\n",
    "        cumulative_batch_size = 0\n",
    "        for batch in train_generator_tqdm:\n",
    "            batches_this_epoch += 1\n",
    "            self._batch_num_total += 1\n",
    "            batch_num_total = self._batch_num_total\n",
    "\n",
    "            self._log_histograms_this_batch = self._histogram_interval is not None and (\n",
    "                    batch_num_total % self._histogram_interval == 0)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            ###########\n",
    "            # Custom  #\n",
    "            ###########\n",
    "            loss = self.batch_loss(batch, for_training=True)\n",
    "            if torch.isnan(loss):\n",
    "                raise ValueError(\"nan loss encountered\")\n",
    "            train_loss += loss.item()\n",
    "            # wait to update\n",
    "            if (batches_this_epoch % N_BATCHES_PER_UPDATE) != 0: continue\n",
    "            ###############\n",
    "            # End Custom  #\n",
    "            ###############\n",
    "            \n",
    "            loss.backward()\n",
    "            batch_grad_norm = self.rescale_gradients()\n",
    "\n",
    "            # This does nothing if batch_num_total is None or you are using an\n",
    "            # LRScheduler which doesn't update per batch.\n",
    "            if self._learning_rate_scheduler:\n",
    "                self._learning_rate_scheduler.step_batch(batch_num_total)\n",
    "\n",
    "            if self._log_histograms_this_batch:\n",
    "                # get the magnitude of parameter updates for logging\n",
    "                # We need a copy of current parameters to compute magnitude of updates,\n",
    "                # and copy them to CPU so large models won't go OOM on the GPU.\n",
    "                param_updates = {name: param.detach().cpu().clone()\n",
    "                                 for name, param in self.model.named_parameters()}\n",
    "                self.optimizer.step()\n",
    "                for name, param in self.model.named_parameters():\n",
    "                    param_updates[name].sub_(param.detach().cpu())\n",
    "                    update_norm = torch.norm(param_updates[name].view(-1, ))\n",
    "                    param_norm = torch.norm(param.view(-1, )).cpu()\n",
    "                    self._tensorboard.add_train_scalar(\"gradient_update/\" + name,\n",
    "                                                       update_norm / (param_norm + 1e-7),\n",
    "                                                       batch_num_total)\n",
    "            else:\n",
    "                self.optimizer.step()\n",
    "\n",
    "            # Update the description with the latest metrics\n",
    "            metrics = self._get_metrics(train_loss, batches_this_epoch)\n",
    "            description = self._description_from_metrics(metrics)\n",
    "\n",
    "            train_generator_tqdm.set_description(description, refresh=False)\n",
    "\n",
    "            # Log parameter values to Tensorboard\n",
    "            if batch_num_total % self._summary_interval == 0:\n",
    "                if self._should_log_parameter_statistics:\n",
    "                    self._parameter_and_gradient_statistics_to_tensorboard(batch_num_total, batch_grad_norm)\n",
    "                if self._should_log_learning_rate:\n",
    "                    self._learning_rates_to_tensorboard(batch_num_total)\n",
    "                self._tensorboard.add_train_scalar(\"loss/loss_train\", metrics[\"loss\"], batch_num_total)\n",
    "                self._metrics_to_tensorboard(batch_num_total,\n",
    "                                             {\"epoch_metrics/\" + k: v for k, v in metrics.items()})\n",
    "\n",
    "            if self._log_histograms_this_batch:\n",
    "                self._histograms_to_tensorboard(batch_num_total, histogram_parameters)\n",
    "\n",
    "            if self._log_batch_size_period:\n",
    "                cur_batch = self._get_batch_size(batch)\n",
    "                cumulative_batch_size += cur_batch\n",
    "                if (batches_this_epoch - 1) % self._log_batch_size_period == 0:\n",
    "                    average = cumulative_batch_size/batches_this_epoch\n",
    "                    logger.info(f\"current batch size: {cur_batch} mean batch size: {average}\")\n",
    "                    self._tensorboard.add_train_scalar(\"current_batch_size\", cur_batch, batch_num_total)\n",
    "                    self._tensorboard.add_train_scalar(\"mean_batch_size\", average, batch_num_total)\n",
    "\n",
    "            # Save model if needed.\n",
    "            if self._model_save_interval is not None and (\n",
    "                    time.time() - last_save_time > self._model_save_interval\n",
    "            ):\n",
    "                last_save_time = time.time()\n",
    "                self._save_checkpoint(\n",
    "                        '{0}.{1}'.format(epoch, time_to_str(int(last_save_time))), [], is_best=False\n",
    "                )\n",
    "        metrics = self._get_metrics(train_loss, batches_this_epoch, reset=True)\n",
    "        metrics['cpu_memory_MB'] = peak_cpu_usage\n",
    "        for (gpu_num, memory) in gpu_usage:\n",
    "            metrics['gpu_'+str(gpu_num)+'_memory_MB'] = memory\n",
    "        return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=config.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_options = {\n",
    "    # TODO: Add appropriate learning rate scheduler\n",
    "    \"should_log_parameter_statistics\": True,\n",
    "    \"should_log_learning_rate\": True,\n",
    "    \"num_epochs\": config.epochs,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "SER_DIR = DATA_ROOT / \"ckpts\" / RUN_ID\n",
    "\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    iterator=iterator,\n",
    "    train_dataset=train_ds,\n",
    "    validation_dataset=val_ds,\n",
    "    serialization_dir=SER_DIR,\n",
    "    cuda_device=0 if USE_GPU else -1,\n",
    "    **training_options,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/26/2019 18:47:03 - INFO - allennlp.training.trainer -   Beginning training.\n",
      "01/26/2019 18:47:03 - INFO - allennlp.training.trainer -   Epoch 0/14\n",
      "01/26/2019 18:47:03 - INFO - allennlp.training.trainer -   Peak CPU memory usage MB: 2383.70816\n",
      "01/26/2019 18:47:03 - INFO - allennlp.training.trainer -   Training\n",
      "accuracy: 0.9624, loss: 0.1591 ||: 100%|██████████| 40/40 [01:24<00:00,  1.75s/it]\n",
      "01/26/2019 18:48:28 - INFO - allennlp.training.trainer -                     Training |  Validation\n",
      "01/26/2019 18:48:28 - INFO - allennlp.training.trainer -   cpu_memory_MB |  2383.708  |       N/A\n",
      "01/26/2019 18:48:28 - INFO - allennlp.training.trainer -   accuracy      |     0.962  |       N/A\n",
      "01/26/2019 18:48:28 - INFO - allennlp.training.trainer -   loss          |     0.159  |       N/A\n",
      "01/26/2019 18:48:28 - INFO - allennlp.training.trainer -   Best validation performance so far. Copying weights to '../data/jigsaw/ckpts/01_26_18:43:28/best.th'.\n",
      "01/26/2019 18:48:28 - INFO - allennlp.training.trainer -   Epoch duration: 00:01:24\n",
      "01/26/2019 18:48:28 - INFO - allennlp.training.trainer -   Estimated training time remaining: 0:19:49\n",
      "01/26/2019 18:48:28 - INFO - allennlp.training.trainer -   Epoch 1/14\n",
      "01/26/2019 18:48:28 - INFO - allennlp.training.trainer -   Peak CPU memory usage MB: 4297.691136\n",
      "01/26/2019 18:48:28 - INFO - allennlp.training.trainer -   Training\n",
      "accuracy: 0.9632, loss: 0.1555 ||: 100%|██████████| 40/40 [01:22<00:00,  1.13s/it]\n",
      "01/26/2019 18:49:51 - INFO - allennlp.training.trainer -                     Training |  Validation\n",
      "01/26/2019 18:49:51 - INFO - allennlp.training.trainer -   cpu_memory_MB |  4297.691  |       N/A\n",
      "01/26/2019 18:49:51 - INFO - allennlp.training.trainer -   accuracy      |     0.963  |       N/A\n",
      "01/26/2019 18:49:51 - INFO - allennlp.training.trainer -   loss          |     0.156  |       N/A\n",
      "01/26/2019 18:49:51 - INFO - allennlp.training.trainer -   Best validation performance so far. Copying weights to '../data/jigsaw/ckpts/01_26_18:43:28/best.th'.\n",
      "01/26/2019 18:49:51 - INFO - allennlp.training.trainer -   Epoch duration: 00:01:23\n",
      "01/26/2019 18:49:51 - INFO - allennlp.training.trainer -   Estimated training time remaining: 0:18:11\n",
      "01/26/2019 18:49:51 - INFO - allennlp.training.trainer -   Epoch 2/14\n",
      "01/26/2019 18:49:51 - INFO - allennlp.training.trainer -   Peak CPU memory usage MB: 5295.104\n",
      "01/26/2019 18:49:51 - INFO - allennlp.training.trainer -   Training\n",
      "accuracy: 0.9632, loss: 0.1422 ||: 100%|██████████| 40/40 [01:22<00:00,  2.54s/it]\n",
      "01/26/2019 18:51:14 - INFO - allennlp.training.trainer -                     Training |  Validation\n",
      "01/26/2019 18:51:14 - INFO - allennlp.training.trainer -   cpu_memory_MB |  5295.104  |       N/A\n",
      "01/26/2019 18:51:14 - INFO - allennlp.training.trainer -   accuracy      |     0.963  |       N/A\n",
      "01/26/2019 18:51:14 - INFO - allennlp.training.trainer -   loss          |     0.142  |       N/A\n",
      "01/26/2019 18:51:14 - INFO - allennlp.training.trainer -   Best validation performance so far. Copying weights to '../data/jigsaw/ckpts/01_26_18:43:28/best.th'.\n",
      "01/26/2019 18:51:14 - INFO - allennlp.training.trainer -   Epoch duration: 00:01:22\n",
      "01/26/2019 18:51:14 - INFO - allennlp.training.trainer -   Estimated training time remaining: 0:16:42\n",
      "01/26/2019 18:51:14 - INFO - allennlp.training.trainer -   Epoch 3/14\n",
      "01/26/2019 18:51:14 - INFO - allennlp.training.trainer -   Peak CPU memory usage MB: 5317.709824\n",
      "01/26/2019 18:51:14 - INFO - allennlp.training.trainer -   Training\n",
      "accuracy: 0.9661, loss: 0.1255 ||: 100%|██████████| 40/40 [01:18<00:00,  1.08s/it]\n",
      "01/26/2019 18:52:33 - INFO - allennlp.training.trainer -                     Training |  Validation\n",
      "01/26/2019 18:52:33 - INFO - allennlp.training.trainer -   cpu_memory_MB |  5317.710  |       N/A\n",
      "01/26/2019 18:52:33 - INFO - allennlp.training.trainer -   accuracy      |     0.966  |       N/A\n",
      "01/26/2019 18:52:33 - INFO - allennlp.training.trainer -   loss          |     0.126  |       N/A\n",
      "01/26/2019 18:52:33 - INFO - allennlp.training.trainer -   Best validation performance so far. Copying weights to '../data/jigsaw/ckpts/01_26_18:43:28/best.th'.\n",
      "01/26/2019 18:52:33 - INFO - allennlp.training.trainer -   Epoch duration: 00:01:18\n",
      "01/26/2019 18:52:33 - INFO - allennlp.training.trainer -   Estimated training time remaining: 0:15:06\n",
      "01/26/2019 18:52:33 - INFO - allennlp.training.trainer -   Epoch 4/14\n",
      "01/26/2019 18:52:33 - INFO - allennlp.training.trainer -   Peak CPU memory usage MB: 5317.709824\n",
      "01/26/2019 18:52:33 - INFO - allennlp.training.trainer -   Training\n",
      "accuracy: 0.9697, loss: 0.1111 ||: 100%|██████████| 40/40 [01:08<00:00,  1.17it/s]\n",
      "01/26/2019 18:53:41 - INFO - allennlp.training.trainer -                     Training |  Validation\n",
      "01/26/2019 18:53:41 - INFO - allennlp.training.trainer -   cpu_memory_MB |  5317.710  |       N/A\n",
      "01/26/2019 18:53:41 - INFO - allennlp.training.trainer -   accuracy      |     0.970  |       N/A\n",
      "01/26/2019 18:53:41 - INFO - allennlp.training.trainer -   loss          |     0.111  |       N/A\n",
      "01/26/2019 18:53:41 - INFO - allennlp.training.trainer -   Best validation performance so far. Copying weights to '../data/jigsaw/ckpts/01_26_18:43:28/best.th'.\n",
      "01/26/2019 18:53:41 - INFO - allennlp.training.trainer -   Epoch duration: 00:01:08\n",
      "01/26/2019 18:53:41 - INFO - allennlp.training.trainer -   Estimated training time remaining: 0:13:16\n",
      "01/26/2019 18:53:41 - INFO - allennlp.training.trainer -   Epoch 5/14\n",
      "01/26/2019 18:53:41 - INFO - allennlp.training.trainer -   Peak CPU memory usage MB: 5385.351168\n",
      "01/26/2019 18:53:41 - INFO - allennlp.training.trainer -   Training\n",
      "accuracy: 0.9720, loss: 0.1062 ||: 100%|██████████| 40/40 [01:10<00:00,  1.23s/it]\n",
      "01/26/2019 18:54:52 - INFO - allennlp.training.trainer -                     Training |  Validation\n",
      "01/26/2019 18:54:52 - INFO - allennlp.training.trainer -   cpu_memory_MB |  5385.351  |       N/A\n",
      "01/26/2019 18:54:52 - INFO - allennlp.training.trainer -   accuracy      |     0.972  |       N/A\n",
      "01/26/2019 18:54:52 - INFO - allennlp.training.trainer -   loss          |     0.106  |       N/A\n",
      "01/26/2019 18:54:52 - INFO - allennlp.training.trainer -   Best validation performance so far. Copying weights to '../data/jigsaw/ckpts/01_26_18:43:28/best.th'.\n",
      "01/26/2019 18:54:52 - INFO - allennlp.training.trainer -   Epoch duration: 00:01:10\n",
      "01/26/2019 18:54:52 - INFO - allennlp.training.trainer -   Estimated training time remaining: 0:11:42\n",
      "01/26/2019 18:54:52 - INFO - allennlp.training.trainer -   Epoch 6/14\n",
      "01/26/2019 18:54:52 - INFO - allennlp.training.trainer -   Peak CPU memory usage MB: 5579.460608\n",
      "01/26/2019 18:54:52 - INFO - allennlp.training.trainer -   Training\n",
      "accuracy: 0.9715, loss: 0.0947 ||: 100%|██████████| 40/40 [01:17<00:00,  1.06it/s]\n",
      "01/26/2019 18:56:09 - INFO - allennlp.training.trainer -                     Training |  Validation\n",
      "01/26/2019 18:56:09 - INFO - allennlp.training.trainer -   cpu_memory_MB |  5579.461  |       N/A\n",
      "01/26/2019 18:56:09 - INFO - allennlp.training.trainer -   accuracy      |     0.971  |       N/A\n",
      "01/26/2019 18:56:09 - INFO - allennlp.training.trainer -   loss          |     0.095  |       N/A\n",
      "01/26/2019 18:56:09 - INFO - allennlp.training.trainer -   Best validation performance so far. Copying weights to '../data/jigsaw/ckpts/01_26_18:43:28/best.th'.\n",
      "01/26/2019 18:56:09 - INFO - allennlp.training.trainer -   Epoch duration: 00:01:17\n",
      "01/26/2019 18:56:09 - INFO - allennlp.training.trainer -   Estimated training time remaining: 0:10:24\n",
      "01/26/2019 18:56:09 - INFO - allennlp.training.trainer -   Epoch 7/14\n",
      "01/26/2019 18:56:09 - INFO - allennlp.training.trainer -   Peak CPU memory usage MB: 5579.460608\n",
      "01/26/2019 18:56:09 - INFO - allennlp.training.trainer -   Training\n",
      "accuracy: 0.9739, loss: 0.0917 ||: 100%|██████████| 40/40 [01:16<00:00,  1.05s/it]\n",
      "01/26/2019 18:57:26 - INFO - allennlp.training.trainer -                     Training |  Validation\n",
      "01/26/2019 18:57:26 - INFO - allennlp.training.trainer -   cpu_memory_MB |  5579.461  |       N/A\n",
      "01/26/2019 18:57:26 - INFO - allennlp.training.trainer -   accuracy      |     0.974  |       N/A\n",
      "01/26/2019 18:57:26 - INFO - allennlp.training.trainer -   loss          |     0.092  |       N/A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/26/2019 18:57:26 - INFO - allennlp.training.trainer -   Best validation performance so far. Copying weights to '../data/jigsaw/ckpts/01_26_18:43:28/best.th'.\n",
      "01/26/2019 18:57:26 - INFO - allennlp.training.trainer -   Epoch duration: 00:01:16\n",
      "01/26/2019 18:57:26 - INFO - allennlp.training.trainer -   Estimated training time remaining: 0:09:04\n",
      "01/26/2019 18:57:26 - INFO - allennlp.training.trainer -   Epoch 8/14\n",
      "01/26/2019 18:57:26 - INFO - allennlp.training.trainer -   Peak CPU memory usage MB: 5579.460608\n",
      "01/26/2019 18:57:26 - INFO - allennlp.training.trainer -   Training\n",
      "accuracy: 0.9738, loss: 0.0869 ||: 100%|██████████| 40/40 [01:13<00:00,  3.65s/it]\n",
      "01/26/2019 18:58:40 - INFO - allennlp.training.trainer -                     Training |  Validation\n",
      "01/26/2019 18:58:40 - INFO - allennlp.training.trainer -   cpu_memory_MB |  5579.461  |       N/A\n",
      "01/26/2019 18:58:40 - INFO - allennlp.training.trainer -   accuracy      |     0.974  |       N/A\n",
      "01/26/2019 18:58:40 - INFO - allennlp.training.trainer -   loss          |     0.087  |       N/A\n",
      "01/26/2019 18:58:40 - INFO - allennlp.training.trainer -   Best validation performance so far. Copying weights to '../data/jigsaw/ckpts/01_26_18:43:28/best.th'.\n",
      "01/26/2019 18:58:40 - INFO - allennlp.training.trainer -   Epoch duration: 00:01:13\n",
      "01/26/2019 18:58:40 - INFO - allennlp.training.trainer -   Estimated training time remaining: 0:07:44\n",
      "01/26/2019 18:58:40 - INFO - allennlp.training.trainer -   Epoch 9/14\n",
      "01/26/2019 18:58:40 - INFO - allennlp.training.trainer -   Peak CPU memory usage MB: 5579.460608\n",
      "01/26/2019 18:58:40 - INFO - allennlp.training.trainer -   Training\n",
      "accuracy: 0.9745, loss: 0.0819 ||: 100%|██████████| 40/40 [01:08<00:00,  1.13it/s]\n",
      "01/26/2019 18:59:48 - INFO - allennlp.training.trainer -                     Training |  Validation\n",
      "01/26/2019 18:59:48 - INFO - allennlp.training.trainer -   cpu_memory_MB |  5579.461  |       N/A\n",
      "01/26/2019 18:59:48 - INFO - allennlp.training.trainer -   accuracy      |     0.975  |       N/A\n",
      "01/26/2019 18:59:48 - INFO - allennlp.training.trainer -   loss          |     0.082  |       N/A\n",
      "01/26/2019 18:59:48 - INFO - allennlp.training.trainer -   Best validation performance so far. Copying weights to '../data/jigsaw/ckpts/01_26_18:43:28/best.th'.\n",
      "01/26/2019 18:59:48 - INFO - allennlp.training.trainer -   Epoch duration: 00:01:08\n",
      "01/26/2019 18:59:48 - INFO - allennlp.training.trainer -   Estimated training time remaining: 0:06:22\n",
      "01/26/2019 18:59:48 - INFO - allennlp.training.trainer -   Epoch 10/14\n",
      "01/26/2019 18:59:48 - INFO - allennlp.training.trainer -   Peak CPU memory usage MB: 5579.460608\n",
      "01/26/2019 18:59:48 - INFO - allennlp.training.trainer -   Training\n",
      "accuracy: 0.9753, loss: 0.0790 ||: 100%|██████████| 40/40 [01:08<00:00,  1.21it/s]\n",
      "01/26/2019 19:00:57 - INFO - allennlp.training.trainer -                     Training |  Validation\n",
      "01/26/2019 19:00:57 - INFO - allennlp.training.trainer -   cpu_memory_MB |  5579.461  |       N/A\n",
      "01/26/2019 19:00:57 - INFO - allennlp.training.trainer -   accuracy      |     0.975  |       N/A\n",
      "01/26/2019 19:00:57 - INFO - allennlp.training.trainer -   loss          |     0.079  |       N/A\n",
      "01/26/2019 19:00:57 - INFO - allennlp.training.trainer -   Best validation performance so far. Copying weights to '../data/jigsaw/ckpts/01_26_18:43:28/best.th'.\n",
      "01/26/2019 19:00:57 - INFO - allennlp.training.trainer -   Epoch duration: 00:01:08\n",
      "01/26/2019 19:00:57 - INFO - allennlp.training.trainer -   Estimated training time remaining: 0:05:03\n",
      "01/26/2019 19:00:57 - INFO - allennlp.training.trainer -   Epoch 11/14\n",
      "01/26/2019 19:00:57 - INFO - allennlp.training.trainer -   Peak CPU memory usage MB: 5579.460608\n",
      "01/26/2019 19:00:57 - INFO - allennlp.training.trainer -   Training\n",
      "accuracy: 0.9766, loss: 0.0767 ||: 100%|██████████| 40/40 [01:11<00:00,  2.00s/it]\n",
      "01/26/2019 19:02:08 - INFO - allennlp.training.trainer -                     Training |  Validation\n",
      "01/26/2019 19:02:08 - INFO - allennlp.training.trainer -   cpu_memory_MB |  5579.461  |       N/A\n",
      "01/26/2019 19:02:08 - INFO - allennlp.training.trainer -   accuracy      |     0.977  |       N/A\n",
      "01/26/2019 19:02:08 - INFO - allennlp.training.trainer -   loss          |     0.077  |       N/A\n",
      "01/26/2019 19:02:08 - INFO - allennlp.training.trainer -   Best validation performance so far. Copying weights to '../data/jigsaw/ckpts/01_26_18:43:28/best.th'.\n",
      "01/26/2019 19:02:09 - INFO - allennlp.training.trainer -   Epoch duration: 00:01:11\n",
      "01/26/2019 19:02:09 - INFO - allennlp.training.trainer -   Estimated training time remaining: 0:03:46\n",
      "01/26/2019 19:02:09 - INFO - allennlp.training.trainer -   Epoch 12/14\n",
      "01/26/2019 19:02:09 - INFO - allennlp.training.trainer -   Peak CPU memory usage MB: 5741.789184\n",
      "01/26/2019 19:02:09 - INFO - allennlp.training.trainer -   Training\n",
      "accuracy: 0.9771, loss: 0.0720 ||: 100%|██████████| 40/40 [01:13<00:00,  2.02s/it]\n",
      "01/26/2019 19:03:22 - INFO - allennlp.training.trainer -                     Training |  Validation\n",
      "01/26/2019 19:03:22 - INFO - allennlp.training.trainer -   cpu_memory_MB |  5741.789  |       N/A\n",
      "01/26/2019 19:03:22 - INFO - allennlp.training.trainer -   accuracy      |     0.977  |       N/A\n",
      "01/26/2019 19:03:22 - INFO - allennlp.training.trainer -   loss          |     0.072  |       N/A\n",
      "01/26/2019 19:03:22 - INFO - allennlp.training.trainer -   Best validation performance so far. Copying weights to '../data/jigsaw/ckpts/01_26_18:43:28/best.th'.\n",
      "01/26/2019 19:03:22 - INFO - allennlp.training.trainer -   Epoch duration: 00:01:13\n",
      "01/26/2019 19:03:22 - INFO - allennlp.training.trainer -   Estimated training time remaining: 0:02:30\n",
      "01/26/2019 19:03:22 - INFO - allennlp.training.trainer -   Epoch 13/14\n",
      "01/26/2019 19:03:22 - INFO - allennlp.training.trainer -   Peak CPU memory usage MB: 5741.789184\n",
      "01/26/2019 19:03:22 - INFO - allennlp.training.trainer -   Training\n",
      "accuracy: 0.9789, loss: 0.0694 ||: 100%|██████████| 40/40 [01:13<00:00,  1.02s/it]\n",
      "01/26/2019 19:04:36 - INFO - allennlp.training.trainer -                     Training |  Validation\n",
      "01/26/2019 19:04:36 - INFO - allennlp.training.trainer -   cpu_memory_MB |  5741.789  |       N/A\n",
      "01/26/2019 19:04:36 - INFO - allennlp.training.trainer -   accuracy      |     0.979  |       N/A\n",
      "01/26/2019 19:04:36 - INFO - allennlp.training.trainer -   loss          |     0.069  |       N/A\n",
      "01/26/2019 19:04:36 - INFO - allennlp.training.trainer -   Best validation performance so far. Copying weights to '../data/jigsaw/ckpts/01_26_18:43:28/best.th'.\n",
      "01/26/2019 19:04:36 - INFO - allennlp.training.trainer -   Epoch duration: 00:01:13\n",
      "01/26/2019 19:04:36 - INFO - allennlp.training.trainer -   Estimated training time remaining: 0:01:15\n",
      "01/26/2019 19:04:36 - INFO - allennlp.training.trainer -   Epoch 14/14\n",
      "01/26/2019 19:04:36 - INFO - allennlp.training.trainer -   Peak CPU memory usage MB: 5741.789184\n",
      "01/26/2019 19:04:36 - INFO - allennlp.training.trainer -   Training\n",
      "accuracy: 0.9795, loss: 0.0618 ||: 100%|██████████| 40/40 [01:13<00:00,  1.49s/it]\n",
      "01/26/2019 19:05:50 - INFO - allennlp.training.trainer -                     Training |  Validation\n",
      "01/26/2019 19:05:50 - INFO - allennlp.training.trainer -   cpu_memory_MB |  5741.789  |       N/A\n",
      "01/26/2019 19:05:50 - INFO - allennlp.training.trainer -   accuracy      |     0.980  |       N/A\n",
      "01/26/2019 19:05:50 - INFO - allennlp.training.trainer -   loss          |     0.062  |       N/A\n",
      "01/26/2019 19:05:50 - INFO - allennlp.training.trainer -   Best validation performance so far. Copying weights to '../data/jigsaw/ckpts/01_26_18:43:28/best.th'.\n",
      "01/26/2019 19:05:50 - INFO - allennlp.training.trainer -   Epoch duration: 00:01:13\n"
     ]
    }
   ],
   "source": [
    "metrics = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'peak_cpu_memory_MB': 5741.789184,\n",
       " 'training_duration': '00:18:46',\n",
       " 'training_start_epoch': 0,\n",
       " 'training_epochs': 14,\n",
       " 'epoch': 14,\n",
       " 'training_accuracy': 0.9795,\n",
       " 'training_loss': 0.06179733141325414,\n",
       " 'training_cpu_memory_MB': 5741.789184,\n",
       " 'best_epoch': 14}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import expit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Predictor:\n",
    "    def __init__(self, model: Model, iterator: DataIterator) -> None:\n",
    "        self.model = model\n",
    "        self.iterator = iterator\n",
    "        \n",
    "    def _extract_preds(self, out_dict: dict) -> np.ndarray:\n",
    "        return out_dict[\"class_logits\"].detach().cpu().numpy()\n",
    "        \n",
    "    def _postprocess(self, predictions: List[np.ndarray]) -> np.ndarray:\n",
    "        return expit(np.concatenate(predictions, axis=0))\n",
    "        \n",
    "    def predict(self, ds: Iterable[Instance]) -> np.ndarray:\n",
    "        pred_generator = self.iterator(ds, num_epochs=1, shuffle=False)\n",
    "        self.model.eval()\n",
    "        pred_generator_tqdm = Tqdm.tqdm(pred_generator,\n",
    "                                        total=self.iterator.get_num_batches(ds))\n",
    "        preds = []\n",
    "        with torch.no_grad():\n",
    "            for batch in pred_generator_tqdm:\n",
    "                out_dict = self.model(**batch)\n",
    "                preds.append(self._extract_preds(out_dict))\n",
    "        return self._postprocess(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.data.iterators import BasicIterator\n",
    "seq_iterator = BasicIterator(batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/157 [00:00<?, ?it/s]\u001b[A\n",
      "  1%|          | 1/157 [00:00<01:27,  1.79it/s]\u001b[A\n",
      "  1%|▏         | 2/157 [00:01<01:45,  1.47it/s]\u001b[A\n",
      "  2%|▏         | 3/157 [00:02<01:44,  1.47it/s]\u001b[A\n",
      "  3%|▎         | 4/157 [00:02<01:44,  1.47it/s]\u001b[A\n",
      "  3%|▎         | 5/157 [00:03<01:27,  1.74it/s]\u001b[A\n",
      "  4%|▍         | 6/157 [00:03<01:36,  1.57it/s]\u001b[A\n",
      "  4%|▍         | 7/157 [00:04<01:21,  1.83it/s]\u001b[A\n",
      "  5%|▌         | 8/157 [00:04<01:23,  1.79it/s]\u001b[A\n",
      "  6%|▌         | 9/157 [00:05<01:23,  1.76it/s]\u001b[A\n",
      "  6%|▋         | 10/157 [00:06<01:38,  1.49it/s]\u001b[A\n",
      "  7%|▋         | 11/157 [00:07<01:39,  1.47it/s]\u001b[A\n",
      "  8%|▊         | 12/157 [00:07<01:30,  1.60it/s]\u001b[A\n",
      "  8%|▊         | 13/157 [00:08<01:33,  1.55it/s]\u001b[A\n",
      "  9%|▉         | 14/157 [00:09<01:39,  1.44it/s]\u001b[A\n",
      " 10%|▉         | 15/157 [00:09<01:32,  1.53it/s]\u001b[A\n",
      " 10%|█         | 16/157 [00:10<01:18,  1.80it/s]\u001b[A\n",
      " 11%|█         | 17/157 [00:10<01:22,  1.70it/s]\u001b[A\n",
      " 11%|█▏        | 18/157 [00:10<01:09,  1.99it/s]\u001b[A\n",
      " 12%|█▏        | 19/157 [00:11<01:16,  1.80it/s]\u001b[A\n",
      " 13%|█▎        | 20/157 [00:12<01:23,  1.63it/s]\u001b[A\n",
      " 13%|█▎        | 21/157 [00:13<01:25,  1.58it/s]\u001b[A\n",
      " 14%|█▍        | 22/157 [00:13<01:23,  1.62it/s]\u001b[A\n",
      " 15%|█▍        | 23/157 [00:13<01:10,  1.89it/s]\u001b[A\n",
      " 15%|█▌        | 24/157 [00:14<00:58,  2.26it/s]\u001b[A\n",
      " 16%|█▌        | 25/157 [00:14<00:51,  2.56it/s]\u001b[A\n",
      " 17%|█▋        | 26/157 [00:15<00:56,  2.32it/s]\u001b[A\n",
      " 17%|█▋        | 27/157 [00:15<01:02,  2.08it/s]\u001b[A\n",
      " 18%|█▊        | 28/157 [00:16<01:10,  1.84it/s]\u001b[A\n",
      " 18%|█▊        | 29/157 [00:16<01:13,  1.74it/s]\u001b[A\n",
      " 19%|█▉        | 30/157 [00:17<01:13,  1.73it/s]\u001b[A\n",
      " 20%|█▉        | 31/157 [00:17<01:03,  1.99it/s]\u001b[A\n",
      " 20%|██        | 32/157 [00:18<01:10,  1.77it/s]\u001b[A\n",
      " 21%|██        | 33/157 [00:18<00:57,  2.14it/s]\u001b[A\n",
      " 22%|██▏       | 34/157 [00:19<00:52,  2.35it/s]\u001b[A\n",
      " 22%|██▏       | 35/157 [00:19<00:49,  2.45it/s]\u001b[A\n",
      " 23%|██▎       | 36/157 [00:20<00:52,  2.30it/s]\u001b[A\n",
      " 24%|██▎       | 37/157 [00:20<01:03,  1.90it/s]\u001b[A\n",
      " 24%|██▍       | 38/157 [00:21<01:13,  1.62it/s]\u001b[A\n",
      " 25%|██▍       | 39/157 [00:21<01:05,  1.81it/s]\u001b[A\n",
      " 25%|██▌       | 40/157 [00:22<00:57,  2.03it/s]\u001b[A\n",
      " 26%|██▌       | 41/157 [00:22<00:51,  2.27it/s]\u001b[A\n",
      " 27%|██▋       | 42/157 [00:23<00:49,  2.33it/s]\u001b[A\n",
      " 27%|██▋       | 43/157 [00:23<00:55,  2.07it/s]\u001b[A\n",
      " 28%|██▊       | 44/157 [00:24<01:03,  1.77it/s]\u001b[A\n",
      " 29%|██▊       | 45/157 [00:24<00:56,  1.99it/s]\u001b[A\n",
      " 29%|██▉       | 46/157 [00:25<00:57,  1.95it/s]\u001b[A\n",
      " 30%|██▉       | 47/157 [00:25<00:47,  2.33it/s]\u001b[A\n",
      " 31%|███       | 48/157 [00:26<00:51,  2.13it/s]\u001b[A\n",
      " 31%|███       | 49/157 [00:26<00:54,  2.00it/s]\u001b[A\n",
      " 32%|███▏      | 50/157 [00:27<00:58,  1.84it/s]\u001b[A\n",
      " 32%|███▏      | 51/157 [00:27<01:00,  1.74it/s]\u001b[A\n",
      " 33%|███▎      | 52/157 [00:28<00:56,  1.87it/s]\u001b[A\n",
      " 34%|███▍      | 53/157 [00:29<00:59,  1.76it/s]\u001b[A\n",
      " 34%|███▍      | 54/157 [00:29<01:00,  1.70it/s]\u001b[A\n",
      " 35%|███▌      | 55/157 [00:30<00:51,  1.97it/s]\u001b[A\n",
      " 36%|███▌      | 56/157 [00:30<00:50,  1.99it/s]\u001b[A\n",
      " 36%|███▋      | 57/157 [00:31<01:00,  1.65it/s]\u001b[A\n",
      " 37%|███▋      | 58/157 [00:32<01:05,  1.51it/s]\u001b[A\n",
      " 38%|███▊      | 59/157 [00:32<01:04,  1.53it/s]\u001b[A\n",
      " 38%|███▊      | 60/157 [00:33<00:52,  1.84it/s]\u001b[A\n",
      " 39%|███▉      | 61/157 [00:33<00:56,  1.70it/s]\u001b[A\n",
      " 39%|███▉      | 62/157 [00:34<01:01,  1.55it/s]\u001b[A\n",
      " 40%|████      | 63/157 [00:35<01:06,  1.41it/s]\u001b[A\n",
      " 41%|████      | 64/157 [00:35<00:56,  1.66it/s]\u001b[A\n",
      " 41%|████▏     | 65/157 [00:36<00:55,  1.66it/s]\u001b[A\n",
      " 42%|████▏     | 66/157 [00:37<00:56,  1.62it/s]\u001b[A\n",
      " 43%|████▎     | 67/157 [00:37<01:02,  1.43it/s]\u001b[A\n",
      " 43%|████▎     | 68/157 [00:38<00:52,  1.70it/s]\u001b[A\n",
      " 44%|████▍     | 69/157 [00:38<00:42,  2.07it/s]\u001b[A\n",
      " 45%|████▍     | 70/157 [00:38<00:41,  2.10it/s]\u001b[A\n",
      " 45%|████▌     | 71/157 [00:39<00:41,  2.09it/s]\u001b[A\n",
      " 46%|████▌     | 72/157 [00:40<00:48,  1.77it/s]\u001b[A\n",
      " 46%|████▋     | 73/157 [00:40<00:40,  2.06it/s]\u001b[A\n",
      " 47%|████▋     | 74/157 [00:42<01:11,  1.17it/s]\u001b[A\n",
      " 48%|████▊     | 75/157 [00:42<00:58,  1.39it/s]\u001b[A\n",
      " 48%|████▊     | 76/157 [00:43<00:59,  1.37it/s]\u001b[A\n",
      " 49%|████▉     | 77/157 [00:44<00:57,  1.40it/s]\u001b[A\n",
      " 50%|████▉     | 78/157 [00:44<00:54,  1.45it/s]\u001b[A\n",
      " 50%|█████     | 79/157 [00:45<00:53,  1.45it/s]\u001b[A\n",
      " 51%|█████     | 80/157 [00:45<00:52,  1.48it/s]\u001b[A\n",
      " 52%|█████▏    | 81/157 [00:46<00:50,  1.50it/s]\u001b[A\n",
      " 52%|█████▏    | 82/157 [00:47<00:58,  1.28it/s]\u001b[A\n",
      " 53%|█████▎    | 83/157 [00:48<01:03,  1.17it/s]\u001b[A\n",
      " 54%|█████▎    | 84/157 [00:49<01:01,  1.19it/s]\u001b[A\n",
      " 54%|█████▍    | 85/157 [00:49<00:50,  1.42it/s]\u001b[A\n",
      " 55%|█████▍    | 86/157 [00:50<00:49,  1.42it/s]\u001b[A\n",
      " 55%|█████▌    | 87/157 [00:51<00:57,  1.22it/s]\u001b[A\n",
      " 56%|█████▌    | 88/157 [00:52<00:51,  1.34it/s]\u001b[A\n",
      " 57%|█████▋    | 89/157 [00:52<00:41,  1.62it/s]\u001b[A\n",
      " 57%|█████▋    | 90/157 [00:53<00:48,  1.37it/s]\u001b[A\n",
      " 58%|█████▊    | 91/157 [00:53<00:40,  1.64it/s]\u001b[A\n",
      " 59%|█████▊    | 92/157 [00:54<00:43,  1.50it/s]\u001b[A\n",
      " 59%|█████▉    | 93/157 [00:55<00:35,  1.80it/s]\u001b[A\n",
      " 60%|█████▉    | 94/157 [00:55<00:35,  1.77it/s]\u001b[A\n",
      " 61%|██████    | 95/157 [00:56<00:38,  1.60it/s]\u001b[A\n",
      " 61%|██████    | 96/157 [00:57<00:39,  1.55it/s]\u001b[A\n",
      " 62%|██████▏   | 97/157 [00:57<00:34,  1.75it/s]\u001b[A\n",
      " 62%|██████▏   | 98/157 [00:57<00:28,  2.04it/s]\u001b[A\n",
      " 63%|██████▎   | 99/157 [00:58<00:34,  1.70it/s]\u001b[A\n",
      " 64%|██████▎   | 100/157 [00:59<00:34,  1.67it/s]\u001b[A\n",
      " 64%|██████▍   | 101/157 [00:59<00:36,  1.54it/s]\u001b[A\n",
      " 65%|██████▍   | 102/157 [01:00<00:34,  1.60it/s]\u001b[A\n",
      " 66%|██████▌   | 103/157 [01:01<00:35,  1.54it/s]\u001b[A\n",
      " 66%|██████▌   | 104/157 [01:01<00:32,  1.61it/s]\u001b[A\n",
      " 67%|██████▋   | 105/157 [01:02<00:31,  1.64it/s]\u001b[A\n",
      " 68%|██████▊   | 106/157 [01:02<00:25,  1.98it/s]\u001b[A\n",
      " 68%|██████▊   | 107/157 [01:02<00:22,  2.24it/s]\u001b[A\n",
      " 69%|██████▉   | 108/157 [01:03<00:18,  2.68it/s]\u001b[A\n",
      " 69%|██████▉   | 109/157 [01:03<00:22,  2.17it/s]\u001b[A\n",
      " 70%|███████   | 110/157 [01:04<00:24,  1.92it/s]\u001b[A\n",
      " 71%|███████   | 111/157 [01:04<00:20,  2.23it/s]\u001b[A\n",
      " 71%|███████▏  | 112/157 [01:05<00:24,  1.84it/s]\u001b[A\n",
      " 72%|███████▏  | 113/157 [01:06<00:25,  1.72it/s]\u001b[A\n",
      " 73%|███████▎  | 114/157 [01:06<00:25,  1.66it/s]\u001b[A\n",
      " 73%|███████▎  | 115/157 [01:07<00:27,  1.55it/s]\u001b[A\n",
      " 74%|███████▍  | 116/157 [01:07<00:21,  1.88it/s]\u001b[A\n",
      " 75%|███████▍  | 117/157 [01:08<00:17,  2.25it/s]\u001b[A\n",
      " 75%|███████▌  | 118/157 [01:08<00:13,  2.83it/s]\u001b[A\n",
      " 76%|███████▌  | 119/157 [01:08<00:13,  2.91it/s]\u001b[A\n",
      " 76%|███████▋  | 120/157 [01:08<00:13,  2.73it/s]\u001b[A\n",
      " 77%|███████▋  | 121/157 [01:09<00:15,  2.36it/s]\u001b[A\n",
      " 78%|███████▊  | 122/157 [01:10<00:16,  2.18it/s]\u001b[A\n",
      " 78%|███████▊  | 123/157 [01:10<00:13,  2.46it/s]\u001b[A\n",
      " 79%|███████▉  | 124/157 [01:10<00:15,  2.08it/s]\u001b[A\n",
      " 80%|███████▉  | 125/157 [01:11<00:16,  1.91it/s]\u001b[A\n",
      " 80%|████████  | 126/157 [01:11<00:14,  2.18it/s]\u001b[A\n",
      " 81%|████████  | 127/157 [01:12<00:12,  2.40it/s]\u001b[A\n",
      " 82%|████████▏ | 128/157 [01:12<00:13,  2.09it/s]\u001b[A\n",
      " 82%|████████▏ | 129/157 [01:13<00:11,  2.42it/s]\u001b[A\n",
      " 83%|████████▎ | 130/157 [01:13<00:11,  2.38it/s]\u001b[A\n",
      " 83%|████████▎ | 131/157 [01:13<00:10,  2.56it/s]\u001b[A\n",
      " 84%|████████▍ | 132/157 [01:14<00:11,  2.15it/s]\u001b[A\n",
      " 85%|████████▍ | 133/157 [01:15<00:11,  2.02it/s]\u001b[A\n",
      " 85%|████████▌ | 134/157 [01:15<00:11,  2.03it/s]\u001b[A\n",
      " 86%|████████▌ | 135/157 [01:15<00:10,  2.16it/s]\u001b[A\n",
      " 87%|████████▋ | 136/157 [01:16<00:10,  2.06it/s]\u001b[A\n",
      " 87%|████████▋ | 137/157 [01:17<00:10,  1.94it/s]\u001b[A\n",
      " 88%|████████▊ | 138/157 [01:17<00:10,  1.77it/s]\u001b[A\n",
      " 89%|████████▊ | 139/157 [01:18<00:10,  1.65it/s]\u001b[A\n",
      " 89%|████████▉ | 140/157 [01:18<00:09,  1.73it/s]\u001b[A\n",
      " 90%|████████▉ | 141/157 [01:19<00:11,  1.43it/s]\u001b[A\n",
      " 90%|█████████ | 142/157 [01:20<00:09,  1.54it/s]\u001b[A\n",
      " 91%|█████████ | 143/157 [01:21<00:08,  1.62it/s]\u001b[A\n",
      " 92%|█████████▏| 144/157 [01:21<00:07,  1.67it/s]\u001b[A\n",
      " 92%|█████████▏| 145/157 [01:22<00:06,  1.73it/s]\u001b[A\n",
      " 93%|█████████▎| 146/157 [01:22<00:05,  2.15it/s]\u001b[A\n",
      " 94%|█████████▎| 147/157 [01:22<00:05,  1.92it/s]\u001b[A\n",
      " 94%|█████████▍| 148/157 [01:23<00:04,  2.05it/s]\u001b[A\n",
      " 95%|█████████▍| 149/157 [01:23<00:03,  2.03it/s]\u001b[A\n",
      " 96%|█████████▌| 150/157 [01:24<00:02,  2.47it/s]\u001b[A\n",
      " 96%|█████████▌| 151/157 [01:24<00:02,  2.05it/s]\u001b[A\n",
      " 97%|█████████▋| 152/157 [01:25<00:03,  1.65it/s]\u001b[A\n",
      " 97%|█████████▋| 153/157 [01:25<00:01,  2.00it/s]\u001b[A\n",
      " 98%|█████████▊| 154/157 [01:26<00:01,  1.78it/s]\u001b[A\n",
      " 99%|█████████▊| 155/157 [01:27<00:01,  1.86it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 156/157 [01:27<00:00,  1.80it/s]\u001b[A\n",
      "100%|██████████| 157/157 [01:27<00:00,  1.79it/s]\u001b[A"
     ]
    }
   ],
   "source": [
    "predictor = Predictor(model, seq_iterator)\n",
    "train_preds =predictor.predict(train_ds) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/157 [00:00<?, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "ename": "ConfigurationError",
     "evalue": "'You must call .index(vocabulary) on a field before determining padding lengths.'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConfigurationError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-74-f9a8c47cac21>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_ds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-63-ee8b257e29c2>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, ds)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpred_generator_tqdm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m                 \u001b[0mout_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m                 \u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extract_preds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/code-5-sIDrJN/lib/python3.7/site-packages/tqdm/_tqdm.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1000\u001b[0m \"\"\", fp_write=getattr(self.fp, 'write', sys.stderr.write))\n\u001b[1;32m   1001\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1002\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1003\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1004\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/code-5-sIDrJN/lib/python3.7/site-packages/allennlp/data/iterators/data_iterator.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, instances, num_epochs, shuffle)\u001b[0m\n\u001b[1;32m    147\u001b[0m                         \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_instances\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m                     \u001b[0mpadding_lengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_padding_lengths\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m                     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Batch padding lengths: %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadding_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m                     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Batch size: %d\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minstances\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/code-5-sIDrJN/lib/python3.7/site-packages/allennlp/data/dataset.py\u001b[0m in \u001b[0;36mget_padding_lengths\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mpadding_lengths\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         all_instance_lengths: List[Dict[str, Dict[str, int]]] = [instance.get_padding_lengths()\n\u001b[0;32m---> 58\u001b[0;31m                                                                  for instance in self.instances]\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mall_instance_lengths\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mpadding_lengths\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/code-5-sIDrJN/lib/python3.7/site-packages/allennlp/data/dataset.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mpadding_lengths\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         all_instance_lengths: List[Dict[str, Dict[str, int]]] = [instance.get_padding_lengths()\n\u001b[0;32m---> 58\u001b[0;31m                                                                  for instance in self.instances]\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mall_instance_lengths\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mpadding_lengths\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/code-5-sIDrJN/lib/python3.7/site-packages/allennlp/data/instance.py\u001b[0m in \u001b[0;36mget_padding_lengths\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfield_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfield\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfields\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m             \u001b[0mlengths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfield_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfield\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_padding_lengths\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/code-5-sIDrJN/lib/python3.7/site-packages/allennlp/data/fields/text_field.py\u001b[0m in \u001b[0;36mget_padding_lengths\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_indexed_tokens\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m             raise ConfigurationError(\"You must call .index(vocabulary) on a \"\n\u001b[0m\u001b[1;32m     87\u001b[0m                                      \"field before determining padding lengths.\")\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mConfigurationError\u001b[0m: 'You must call .index(vocabulary) on a field before determining padding lengths.'"
     ]
    }
   ],
   "source": [
    "test_preds = predictor.predict(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = pd.read_csv(DATA_ROOT / \"test_proced.csv\")[label_cols].values\n",
    "if config.testing:\n",
    "    test_labels = test_labels[:10000, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, f1_score, accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========toxic=========\n",
      "{'auc': 0.9911234642830135, 'f1': 0.7863554757630161, 'acc': 0.9643, 'tnr': 0.8986, 'fpr': 0.0043, 'fnr': 0.0314, 'tpr': 0.0657, 'precision': 0.9385714285714286, 'recall': 0.6766220391349125}\n",
      "========severe_toxic=========\n",
      "{'auc': 0.9881406162638692, 'f1': 0.0196078431372549, 'acc': 0.99, 'tnr': 0.9899, 'fpr': 0.0, 'fnr': 0.01, 'tpr': 0.0001, 'precision': 1.0, 'recall': 0.009900990099009901}\n",
      "========obscene=========\n",
      "{'auc': 0.9951877612413269, 'f1': 0.746820809248555, 'acc': 0.9781, 'tnr': 0.9458, 'fpr': 0.0015, 'fnr': 0.0204, 'tpr': 0.0323, 'precision': 0.9556213017751479, 'recall': 0.6129032258064516}\n",
      "========threat=========\n",
      "{'auc': 0.9444865024277084, 'f1': 0.0, 'acc': 0.9967, 'tnr': 0.9967, 'fpr': 0.0, 'fnr': 0.0033, 'tpr': 0.0, 'precision': nan, 'recall': 0.0}\n",
      "========insult=========\n",
      "{'auc': 0.9903444745317468, 'f1': 0.6601941747572816, 'acc': 0.972, 'tnr': 0.9448, 'fpr': 0.0058, 'fnr': 0.0222, 'tpr': 0.0272, 'precision': 0.8242424242424242, 'recall': 0.5506072874493927}\n",
      "========identity_hate=========\n",
      "{'auc': 0.9777920244338155, 'f1': 0.0, 'acc': 0.9916, 'tnr': 0.9916, 'fpr': 0.0, 'fnr': 0.0084, 'tpr': 0.0, 'precision': nan, 'recall': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vyvyen/.local/share/virtualenvs/code-5-sIDrJN/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/vyvyen/.local/share/virtualenvs/code-5-sIDrJN/lib/python3.7/site-packages/ipykernel_launcher.py:9: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  if __name__ == '__main__':\n",
      "/Users/vyvyen/.local/share/virtualenvs/code-5-sIDrJN/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/vyvyen/.local/share/virtualenvs/code-5-sIDrJN/lib/python3.7/site-packages/ipykernel_launcher.py:9: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  if __name__ == '__main__':\n",
      "/Users/vyvyen/.local/share/virtualenvs/code-5-sIDrJN/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/vyvyen/.local/share/virtualenvs/code-5-sIDrJN/lib/python3.7/site-packages/ipykernel_launcher.py:9: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  if __name__ == '__main__':\n",
      "/Users/vyvyen/.local/share/virtualenvs/code-5-sIDrJN/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/vyvyen/.local/share/virtualenvs/code-5-sIDrJN/lib/python3.7/site-packages/ipykernel_launcher.py:9: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "def to_metric_dict(t: np.ndarray, y: np.ndarray, thres=0.5):\n",
    "    tn, fp, fn, tp = confusion_matrix(t, y >= thres).ravel()\n",
    "    return {\"auc\": roc_auc_score(t, y),\n",
    "            \"f1\": f1_score(t, y >= thres),\n",
    "            \"acc\": accuracy_score(t, y >= thres),\n",
    "            \"tnr\": tn / len(t), \"fpr\": fp / len(t),\n",
    "            \"fnr\": fn / len(t), \"tpr\": tp / len(t),\n",
    "            \"precision\": tp / (tp + fp), \"recall\": tp / (tp + fn),\n",
    "          }\n",
    "\n",
    "train_label_metrics = {}\n",
    "label_metrics = {}\n",
    "for i, lbl in enumerate(label_cols):\n",
    "    train_label_metrics[lbl] = to_metric_dict(train_labels[:, i], train_preds[:, i])\n",
    "    label_metrics[lbl] = to_metric_dict(test_labels[:, i], test_preds[:, i])\n",
    "    print(f\"========{lbl}=========\")\n",
    "    print(label_metrics[lbl])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'toxic': {'auc': 0.9911234642830135,\n",
       "  'f1': 0.7863554757630161,\n",
       "  'acc': 0.9643,\n",
       "  'tnr': 0.8986,\n",
       "  'fpr': 0.0043,\n",
       "  'fnr': 0.0314,\n",
       "  'tpr': 0.0657,\n",
       "  'precision': 0.9385714285714286,\n",
       "  'recall': 0.6766220391349125},\n",
       " 'severe_toxic': {'auc': 0.9881406162638692,\n",
       "  'f1': 0.0196078431372549,\n",
       "  'acc': 0.99,\n",
       "  'tnr': 0.9899,\n",
       "  'fpr': 0.0,\n",
       "  'fnr': 0.01,\n",
       "  'tpr': 0.0001,\n",
       "  'precision': 1.0,\n",
       "  'recall': 0.009900990099009901},\n",
       " 'obscene': {'auc': 0.9951877612413269,\n",
       "  'f1': 0.746820809248555,\n",
       "  'acc': 0.9781,\n",
       "  'tnr': 0.9458,\n",
       "  'fpr': 0.0015,\n",
       "  'fnr': 0.0204,\n",
       "  'tpr': 0.0323,\n",
       "  'precision': 0.9556213017751479,\n",
       "  'recall': 0.6129032258064516},\n",
       " 'threat': {'auc': 0.9444865024277084,\n",
       "  'f1': 0.0,\n",
       "  'acc': 0.9967,\n",
       "  'tnr': 0.9967,\n",
       "  'fpr': 0.0,\n",
       "  'fnr': 0.0033,\n",
       "  'tpr': 0.0,\n",
       "  'precision': nan,\n",
       "  'recall': 0.0},\n",
       " 'insult': {'auc': 0.9903444745317468,\n",
       "  'f1': 0.6601941747572816,\n",
       "  'acc': 0.972,\n",
       "  'tnr': 0.9448,\n",
       "  'fpr': 0.0058,\n",
       "  'fnr': 0.0222,\n",
       "  'tpr': 0.0272,\n",
       "  'precision': 0.8242424242424242,\n",
       "  'recall': 0.5506072874493927},\n",
       " 'identity_hate': {'auc': 0.9777920244338155,\n",
       "  'f1': 0.0,\n",
       "  'acc': 0.9916,\n",
       "  'tnr': 0.9916,\n",
       "  'fpr': 0.0,\n",
       "  'fnr': 0.0084,\n",
       "  'tpr': 0.0,\n",
       "  'precision': nan,\n",
       "  'recall': 0.0}}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_label_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_metrics[\"global\"] = {}\n",
    "for mtrc in label_metrics[\"toxic\"].keys():\n",
    "    label_metrics[\"global\"][mtrc] = np.mean([label_metrics[col][mtrc] for col in label_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'auc': 0.9811791405302467,\n",
       " 'f1': 0.368829717151018,\n",
       " 'acc': 0.9821166666666666,\n",
       " 'tnr': 0.9612333333333334,\n",
       " 'fpr': 0.0019333333333333331,\n",
       " 'fnr': 0.015950000000000002,\n",
       " 'tpr': 0.020883333333333334,\n",
       " 'precision': nan,\n",
       " 'recall': 0.30833892374829447}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_metrics[\"global\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Record results and save weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../lib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import record_experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Record summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not config.testing:\n",
    "    experiment_log = dict(config)\n",
    "    experiment_log.update(metrics)\n",
    "    experiment_log.update(label_metrics)\n",
    "    record_experiments.record(experiment_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output tensorboard outputs and training logs to s3\n",
    "\n",
    "(Remove weights since they take up too much space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm {SER_DIR / \"*.th\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls {SER_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 sync {SER_DIR} s3://nnfornlp/ckpts/{RUN_ID}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
