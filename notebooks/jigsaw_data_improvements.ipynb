{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "dependencies"
    ]
   },
   "outputs": [],
   "source": [
    "depends_on = [\n",
    "    \"preproc_jigsaw\",\n",
    "    \"jigsaw_create_augmented_data\",\n",
    "    \"create_fasttext_matrix\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings for seamlessly running on colab\n",
    "import os\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive')\n",
    "    os.environ[\"IS_COLAB\"] = \"True\"\n",
    "except ImportError:\n",
    "    os.environ[\"IS_COLAB\"] = \"False\"    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"SLACK_TOKEN\" not in os.environ:\n",
    "    os.environ[\"SLACK_TOKEN\"] = \"\" # TODO: insert here for slack notifications\n",
    "if \"SLACK_ID\" not in os.environ:\n",
    "    os.environ[\"SLACK_ID\"] = \"\" # TODO: insert here for slack notifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "if [ \"$IS_COLAB\" = \"True\" ]; then\n",
    "    pip install git+https://github.com/facebookresearch/fastText.git\n",
    "    pip install torch\n",
    "    pip install torchvision\n",
    "    pip install allennlp\n",
    "    pip install dnspython\n",
    "    pip install jupyter_slack\n",
    "    pip install git+https://github.com/keitakurita/Better_LSTM_PyTorch.git\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import *\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from functools import partial\n",
    "from overrides import overrides\n",
    "import warnings\n",
    "\n",
    "from allennlp.data import Instance\n",
    "from allennlp.data.token_indexers import TokenIndexer, SingleIdTokenIndexer\n",
    "from allennlp.data.tokenizers import Token\n",
    "from allennlp.nn import util as nn_util\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger(__name__)  # pylint: disable=invalid-name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from contextlib import contextmanager\n",
    "\n",
    "class Config(dict):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        for k, v in kwargs.items():\n",
    "            setattr(self, k, v)\n",
    "    \n",
    "    def set(self, key, val):\n",
    "        self[key] = val\n",
    "        setattr(self, key, val)\n",
    "\n",
    "@contextmanager\n",
    "def timer(name):\n",
    "    t0 = time.time()\n",
    "    yield\n",
    "    print(f'[{name}] done in {time.time() - t0:.0f} s')\n",
    "    \n",
    "import functools\n",
    "import traceback\n",
    "import sys\n",
    "\n",
    "def get_ref_free_exc_info():\n",
    "    \"Free traceback from references to locals/globals to avoid circular reference leading to gc.collect() unable to reclaim memory\"\n",
    "    type, val, tb = sys.exc_info()\n",
    "    traceback.clear_frames(tb)\n",
    "    return (type, val, tb)\n",
    "\n",
    "def gpu_mem_restore(func):\n",
    "    \"Reclaim GPU RAM if CUDA out of memory happened, or execution was interrupted\"\n",
    "    @functools.wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        try:\n",
    "            return func(*args, **kwargs)\n",
    "        except:\n",
    "            type, val, tb = get_ref_free_exc_info() # must!\n",
    "            raise type(val).with_traceback(tb) from None\n",
    "    return wrapper\n",
    "\n",
    "def ifnone(a: Any, alt: Any): return alt if a is None else a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = TypeVar(\"T\")\n",
    "TensorDict = Dict[str, Union[torch.Tensor, Dict[str, torch.Tensor]]]  # pylint: disable=invalid-name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# for papermill\n",
    "testing = True # set to False when running experiments\n",
    "debugging = False\n",
    "seed = 1\n",
    "use_bt = True\n",
    "computational_batch_size = 128\n",
    "batch_size = 128\n",
    "lr = 5e-3\n",
    "lr_schedule = \"slanted_triangular\"\n",
    "epochs = 2 if not testing else 1\n",
    "hidden_sz = 128\n",
    "dataset = \"jigsaw\"\n",
    "n_classes = 6\n",
    "max_seq_len = 512\n",
    "download_data = False\n",
    "ft_model_path = \"../data/jigsaw/ft_model.txt\"\n",
    "max_vocab_size = 400000\n",
    "dropouti = 0.2\n",
    "dropoutw = 0.0\n",
    "dropoute = 0.1\n",
    "dropoutr = 0.2 # TODO: Implement\n",
    "val_ratio = 0.0\n",
    "use_augmented = False\n",
    "freeze_embeddings = True\n",
    "mixup_ratio = 0.0\n",
    "discrete_mixup_ratio = 0.0\n",
    "attention_bias = True\n",
    "use_attention_aux = True\n",
    "weight_decay = 0.\n",
    "bias_init = True\n",
    "neg_splits = 1\n",
    "num_layers = 2\n",
    "rnn_type = \"lstm\"\n",
    "rnn_residual = True\n",
    "pooling_type = \"attention\" # attention or multipool or augmented_multipool\n",
    "model_type = \"standard\"\n",
    "cache_elmo_embeddings = True\n",
    "use_word_level_features = True\n",
    "use_sentence_level_features = True\n",
    "bucket = True\n",
    "compute_thres_on_test = True\n",
    "find_lr = False\n",
    "permute_sentences = False\n",
    "run_id = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Can we make this play better with papermill?\n",
    "config = Config(\n",
    "    testing=testing,\n",
    "    debugging=debugging,\n",
    "    seed=seed,\n",
    "    use_bt=use_bt,\n",
    "    computational_batch_size=computational_batch_size,\n",
    "    batch_size=batch_size,\n",
    "    lr=lr,\n",
    "    lr_schedule=lr_schedule,\n",
    "    epochs=epochs,\n",
    "    hidden_sz=hidden_sz,\n",
    "    dataset=dataset,\n",
    "    n_classes=n_classes,\n",
    "    max_seq_len=max_seq_len, # necessary to limit memory usage\n",
    "    ft_model_path=ft_model_path,\n",
    "    max_vocab_size=max_vocab_size,\n",
    "    dropouti=dropouti,\n",
    "    dropoutw=dropoutw,\n",
    "    dropoute=dropoute,\n",
    "    dropoutr=dropoutr,\n",
    "    val_ratio=val_ratio,\n",
    "    use_augmented=use_augmented,\n",
    "    freeze_embeddings=freeze_embeddings,\n",
    "    attention_bias=attention_bias,\n",
    "    use_attention_aux=use_attention_aux,\n",
    "    weight_decay=weight_decay,\n",
    "    bias_init=bias_init,\n",
    "    neg_splits=neg_splits,\n",
    "    num_layers=num_layers,\n",
    "    rnn_type=rnn_type,\n",
    "    rnn_residual=rnn_residual,\n",
    "    pooling_type=pooling_type,\n",
    "    model_type=model_type,\n",
    "    cache_elmo_embeddings=cache_elmo_embeddings,\n",
    "    use_word_level_features=use_word_level_features,\n",
    "    use_sentence_level_features=use_sentence_level_features,\n",
    "    mixup_ratio=mixup_ratio,\n",
    "    discrete_mixup_ratio=discrete_mixup_ratio,\n",
    "    bucket=bucket,\n",
    "    compute_thres_on_test=compute_thres_on_test,\n",
    "    permute_sentences=permute_sentences,\n",
    "    find_lr=find_lr,\n",
    "    run_id=run_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.common.checks import ConfigurationError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.model_type != \"standard\" and \"bert\" not in config.model_type and \"elmo\" not in config.model_type:\n",
    "    raise ConfigurationError(f\"Invalid model type {config.model_type} given\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.mixup_ratio > 0. and config.bucket:\n",
    "    raise ConfigurationError(f\"Mixup should be combined with complete random shuffling of the input\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"bert\" in config.model_type and config.computational_batch_size > 16:\n",
    "    raise ConfigurationError(\"Batch size too large for BERT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "now = datetime.datetime.now()\n",
    "RUN_ID = config.run_id if config.run_id is not None else now.strftime(\"%m_%d_%H:%M:%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_GPU = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.environ[\"IS_COLAB\"] != \"True\":\n",
    "    DATA_ROOT = Path(\"../data\") / config.dataset\n",
    "else:\n",
    "    DATA_ROOT = Path(\"./gdrive/My Drive/Colab_Workspace/Colab Notebooks/data\") / config.dataset\n",
    "    config.ft_model_path = str(DATA_ROOT / \"ft_model.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p {DATA_ROOT}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "if download_data:\n",
    "    if config.val_ratio > 0.0:\n",
    "        fnames = [\"train_wo_val.csv\", \"test_proced.csv\", \"val.csv\", \"ft_model.txt\"]\n",
    "    else:\n",
    "        fnames = [\"train.csv\", \"test_proced.csv\", \"ft_model.txt\"]\n",
    "    if config.use_augmented or config.discrete_mixup_ratio > 0.0: fnames.append(\"train_extra.csv\")\n",
    "    for fname in fnames:\n",
    "        if not (DATA_ROOT / fname).exists():\n",
    "            print(subprocess.Popen([f\"aws s3 cp s3://nnfornlp/raw_data/jigsaw/{fname} {str(DATA_ROOT)}\"],\n",
    "                                   shell=True, stdout=subprocess.PIPE).stdout.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls {DATA_ROOT}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set random seed manually to replicate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(config.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.data.dataset_readers import DatasetReader, StanfordSentimentTreeBankDatasetReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader_registry = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def register(name: str):\n",
    "    def dec(x: Callable):\n",
    "        reader_registry[name] = x\n",
    "        return x\n",
    "    return dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_cols = [\"toxic\", \"severe_toxic\", \"obscene\",\n",
    "              \"threat\", \"insult\", \"identity_hate\"]\n",
    "\n",
    "from enum import IntEnum\n",
    "ColIdx = IntEnum('ColIdx', [(x.upper(), i) for i, x in enumerate(label_cols)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "from allennlp.data.fields import TextField, SequenceLabelField, LabelField, MetadataField, ArrayField\n",
    "import string\n",
    "alphabet = set(string.ascii_lowercase)\n",
    "\n",
    "sentence_level_features: List[Callable[[List[str]], float]] = [\n",
    "    lambda x: (np.log1p(len(x)) - 3.628) / 1.065, # stat computed on train set\n",
    "]\n",
    "\n",
    "word_level_features: List[Callable[[str], float]] = [\n",
    "    lambda x: 1 if (x.lower() == x) else 0,\n",
    "    lambda x: len([c for c in x.lower() if c not in alphabet]) / len(x),\n",
    "]\n",
    "\n",
    "def proc(x: str) -> str:\n",
    "    if config.model_type == \"standard\" or \"uncased\" in config.model_type:\n",
    "        return x.lower()\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "@register(\"jigsaw\")\n",
    "class JigsawDatasetReader(DatasetReader):\n",
    "    def __init__(self, tokenizer: Callable[[str], List[str]]=lambda x: x.split(),\n",
    "                 token_indexers: Dict[str, TokenIndexer] = None, # TODO: Handle mapping from BERT\n",
    "                 max_seq_len: Optional[int]=config.max_seq_len) -> None:\n",
    "        super().__init__(lazy=False)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.token_indexers = token_indexers or {\"tokens\": SingleIdTokenIndexer()}\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "    @overrides\n",
    "    def text_to_instance(self, tokens: List[str], id: str,\n",
    "                         labels: np.ndarray) -> Instance:\n",
    "        sentence_field = TextField([Token(proc(x)) for x in tokens],\n",
    "                                   self.token_indexers)\n",
    "        fields = {\"tokens\": sentence_field}\n",
    "        \n",
    "        wl_feats = np.array([[func(w) for func in word_level_features] for w in tokens])\n",
    "        fields[\"word_level_features\"] = ArrayField(array=wl_feats)\n",
    "        \n",
    "        sl_feats = np.array([func(tokens) for func in sentence_level_features])\n",
    "        fields[\"sentence_level_features\"] = ArrayField(array=sl_feats)\n",
    "        \n",
    "        id_field = MetadataField(id)\n",
    "        fields[\"id\"] = id_field\n",
    "        \n",
    "        meta_field = MetadataField({\"lengths\": np.array([len(t) for t in tokens])})\n",
    "        fields[\"meta\"] = meta_field\n",
    "        \n",
    "        label_field = ArrayField(array=labels)\n",
    "        fields[\"label\"] = label_field\n",
    "\n",
    "        return Instance(fields)\n",
    "    \n",
    "    @overrides\n",
    "    def _read(self, file_path: str) -> Iterator[Instance]:\n",
    "        with open(file_path) as f:\n",
    "            reader = csv.reader(f)\n",
    "            next(reader)\n",
    "            for i, line in enumerate(reader):\n",
    "                _, id_, text, *labels = line\n",
    "                yield self.text_to_instance(\n",
    "                    self.tokenizer(text),\n",
    "                    id_, np.array([int(x) for x in labels]),\n",
    "                )\n",
    "                if config.testing and i == 1000: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare token handlers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from functools import wraps\n",
    "\n",
    "def maybeshuffle(_tokenize):\n",
    "    def func(*args, **kwargs):\n",
    "        arr = _tokenize(*args, **kwargs)\n",
    "        if config.permute_sentences:\n",
    "            random.shuffle(arr)\n",
    "        return arr\n",
    "    return func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.data.tokenizers.word_splitter import SpacyWordSplitter\n",
    "from allennlp.data.token_indexers import WordpieceIndexer, SingleIdTokenIndexer\n",
    "\n",
    "_spacy_tok = SpacyWordSplitter(language='en_core_web_sm', pos_tags=False).split_words\n",
    "\n",
    "if config.model_type == \"standard\" or (\"elmo\" in config.model_type and config.cache_elmo_embeddings):\n",
    "    from allennlp.data.token_indexers import SingleIdTokenIndexer\n",
    "    token_indexer = SingleIdTokenIndexer(\n",
    "        lowercase_tokens=\"elmo\" not in config.model_type,\n",
    "    )\n",
    "    @maybeshuffle\n",
    "    def tokenizer(x: str):\n",
    "        return [w.text for w in\n",
    "                _spacy_tok(x)[:config.max_seq_len]]\n",
    "elif \"elmo\" in config.model_type:\n",
    "    from allennlp.data.token_indexers.elmo_indexer import ELMoTokenCharactersIndexer\n",
    "    token_indexer = ELMoTokenCharactersIndexer()\n",
    "    @maybeshuffle\n",
    "    def tokenizer(x: str):\n",
    "        # add start and end of sentence tokens\n",
    "        return [\"<S>\"] + [w.text for w in\n",
    "                _spacy_tok(x)[:config.max_seq_len - 2]] + [\"</S>\"]\n",
    "elif \"bert\" in config.model_type:\n",
    "    def flatten(x: List[List[T]]) -> List[T]:\n",
    "        return [item for sublist in x for item in sublist]\n",
    "\n",
    "    from allennlp.data.token_indexers import PretrainedBertIndexer\n",
    "    token_indexer = PretrainedBertIndexer(\n",
    "        pretrained_model=config.model_type,\n",
    "        max_pieces=config.max_seq_len,\n",
    "        do_lowercase=True,\n",
    "     )\n",
    "    # apparently we need to truncate the sequence here, which is a stupid design decision\n",
    "    @maybeshuffle\n",
    "    def tokenizer(s: str):\n",
    "        if \"uncased\" in config.model_type: s = s.lower()\n",
    "        return flatten([\n",
    "                token_indexer.wordpiece_tokenizer(w)\n",
    "            for w in s.split()])[:config.max_seq_len - 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = JigsawDatasetReader(\n",
    "    tokenizer=tokenizer,\n",
    "    token_indexers={\"tokens\": token_indexer}\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.val_ratio > 0.0:\n",
    "    train_ds, val_ds, test_ds = (reader.read(DATA_ROOT / fname) for fname in [\"train_wo_val.csv\",\n",
    "                                                                              \"val.csv\",\n",
    "                                                                              \"test_proced.csv\"])\n",
    "else:\n",
    "    train_ds, test_ds = (reader.read(DATA_ROOT / fname) for fname in [\n",
    "        \"train_with_bt.csv\" if config.use_bt else \"train.csv\",\n",
    "      \"test_proced.csv\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.use_augmented or config.discrete_mixup_ratio > 0.0:\n",
    "    # TODO: Handle data leak for validation!\n",
    "    train_aug_ds = reader.read(DATA_ROOT / \"train_extra.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars(train_ds[0].fields[\"tokens\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.val_ratio > 0.0:\n",
    "    train_labels = pd.read_csv(DATA_ROOT / \"train_wo_val.csv\")[label_cols].values\n",
    "else:\n",
    "    train_labels = pd.read_csv(DATA_ROOT / \"train.csv\")[label_cols].values\n",
    "if config.testing: train_labels = train_labels[:len(train_ds), :]\n",
    "if config.use_augmented:\n",
    "    train_aux_labels = pd.read_csv(DATA_ROOT / \"train_extra.csv\")[label_cols].values\n",
    "    if config.testing: train_aux_labels = train_aux_labels[:len(train_ds), :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.data.vocabulary import Vocabulary\n",
    "if \"bert\" in config.model_type:\n",
    "    vocab = Vocabulary()\n",
    "elif config.model_type == \"standard\" or config.cache_elmo_embeddings:\n",
    "    full_ds = train_ds + test_ds\n",
    "    if config.val_ratio > 0.0: full_ds = full_ds + val_ds\n",
    "    vocab = Vocabulary.from_instances(full_ds, max_vocab_size=config.max_vocab_size)\n",
    "else:\n",
    "    vocab = Vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.save_to_files(DATA_ROOT / \"vocab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.data.iterators import BucketIterator, DataIterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "class Sampler:\n",
    "    def sample(self, ds: List[Instance]) -> List[Instance]:\n",
    "        return ds\n",
    "    def sample_size(self, ds: List[Instance]) -> int:\n",
    "        return len(ds)\n",
    "\n",
    "class BiasedSampler(Sampler):\n",
    "    def __init__(self, mask: np.ndarray, n_splits: int):\n",
    "        self.mask = mask\n",
    "        self.n_splits = n_splits\n",
    "        self.pos = np.where(self.mask)[0]\n",
    "        self.neg = np.where(~self.mask)[0]\n",
    "        self._n_splits_iterated = 0\n",
    "        \n",
    "    def sample(self, ds: List[Instance]) -> List[Instance]:\n",
    "        if self._n_splits_iterated % self.n_splits == 0:\n",
    "            self.folds = KFold(n_splits=self.n_splits).split(self.neg)\n",
    "        _, neg_idxs = next(self.folds)\n",
    "        \n",
    "        p = np.random.permutation(len(self.pos) + len(neg_idxs))\n",
    "        smpl = np.r_[self.pos, self.neg[neg_idxs]][p]\n",
    "        \n",
    "        self._n_splits_iterated += 1\n",
    "        return [ds[i] for i in smpl]\n",
    "    \n",
    "    def sample_size(self, ds: List[Instance]) -> int:\n",
    "        \"\"\"Returns number of samples that would be returned upon a call to sample\"\"\"\n",
    "        # there might be a slight difference depending on the epoch, but it's okay\n",
    "        return len(self.pos) + len(self.neg) // self.n_splits "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScoredSampler:\n",
    "    def __init__(self, mask: np.ndarray, ratio: float):\n",
    "        self.mask = mask\n",
    "        self.ratio = ratio\n",
    "        self.n_samples = int(len(self.tgt) * self.ratio)\n",
    "        self.score = mask.astype(\"int\")\n",
    "    \n",
    "    def set_score(self, score: np.ndarray):\n",
    "        assert len(score) == len(self.tgt)\n",
    "        self.score = score\n",
    "    \n",
    "    def sample(self, ds: List[Instance]):\n",
    "        \"\"\"Sample top n targets sorted by score descending\"\"\"\n",
    "        smpl = np.arange(len(self.mask))[np.argsort(-self.score)][:self.n_samples]\n",
    "        return [ds[i] for i in smpl]\n",
    "    \n",
    "    def sample_size(self, ds: List[Instance]) -> int:\n",
    "        \"\"\"Returns number of samples that would be returned upon a call to sample\"\"\"\n",
    "        return self.n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import deque\n",
    "from overrides import overrides\n",
    "\n",
    "from allennlp.common.checks import ConfigurationError\n",
    "from allennlp.common.util import lazy_groups_of, add_noise_to_dict_values\n",
    "from allennlp.data.dataset import Batch\n",
    "from allennlp.data.instance import Instance\n",
    "from allennlp.data.iterators import DataIterator, BucketIterator, BasicIterator\n",
    "from allennlp.data.vocabulary import Vocabulary\n",
    "\n",
    "class SamplingIteratorMixin:\n",
    "    \"\"\"Uses Python's MRO to add sampling.\n",
    "    DANGER: This is pushing the limits of OOP and might lead to bugs\n",
    "    \"\"\"\n",
    "    def __init__(self, *args, sampler: Sampler=None, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.sampler = ifnone(sampler, Sampler())\n",
    "        \n",
    "    def get_num_batches(self, instances: List[Instance]):\n",
    "        return math.ceil(self.sampler.sample_size(instances) / self._batch_size)\n",
    "\n",
    "    def _create_batches(self, instances: Iterable[Instance], shuffle: bool) -> Iterable[Batch]:\n",
    "        yield from super()._create_batches(self.sampler.sample(instances), shuffle)\n",
    "\n",
    "# Caution: Inheritance must be in order: SamplingIteratorMixin, BucketIterator\n",
    "class CustomBucketIterator(SamplingIteratorMixin, BucketIterator): pass\n",
    "class CustomBasicIterator(SamplingIteratorMixin, BasicIterator): pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Allow for customization\n",
    "if config.neg_splits > 1:\n",
    "    if config.use_augmented:\n",
    "        full_trn_labels = np.concatenate([train_labels, train_aux_labels], axis=0)\n",
    "    else:\n",
    "        full_trn_labels = train_labels\n",
    "    sampler = BiasedSampler(full_trn_labels.sum(1) >= 1,\n",
    "                            config.neg_splits)\n",
    "else:\n",
    "    sampler = Sampler()\n",
    "if config.bucket:\n",
    "    iterator = CustomBucketIterator(\n",
    "        batch_size=config.batch_size, \n",
    "        biggest_batch_first=config.testing,\n",
    "        sorting_keys=[(\"tokens\", \"num_tokens\")],\n",
    "        sampler=sampler,\n",
    "    )\n",
    "else:\n",
    "    # CAUTION: BasicIterator shuffles the dataset internally\n",
    "    # TODO: Either fix this bug or ensure evalutation can handle shuffle\n",
    "    # in the dataset order\n",
    "    iterator = CustomBasicIterator(\n",
    "        batch_size=config.batch_size, \n",
    "        sampler=sampler,\n",
    "    )\n",
    "iterator.index_with(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(iterator(train_ds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch[\"tokens\"][\"tokens\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch[\"tokens\"][\"tokens\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.modules.token_embedders import Embedding\n",
    "from allennlp.modules.token_embedders.bert_token_embedder import BertEmbedder, PretrainedBertEmbedder\n",
    "from allennlp.modules.seq2vec_encoders import Seq2VecEncoder, PytorchSeq2VecWrapper\n",
    "from allennlp.modules.stacked_bidirectional_lstm import StackedBidirectionalLstm\n",
    "from allennlp.nn.util import get_text_field_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(Seq2VecEncoder):\n",
    "    def __init__(self, inp_sz, aug_sz=None,\n",
    "                 hidden_sz=None, out_sz=None, dim=1, eps=1e-9,\n",
    "                 return_attention=False, use_bias=True):\n",
    "        super().__init__()\n",
    "        self.inp_sz, self.dim, self.eps = inp_sz, dim, eps\n",
    "        self.out_sz = ifnone(out_sz, self.inp_sz)\n",
    "        self.return_attention = return_attention\n",
    "        self.l1 = nn.Linear(inp_sz, ifnone(inp_sz * 2, hidden_sz))\n",
    "        nn.init.xavier_uniform_(self.l1.weight.data)\n",
    "        nn.init.zeros_(self.l1.bias.data)\n",
    "        \n",
    "        vw = torch.zeros(ifnone(inp_sz * 2, hidden_sz), 1)\n",
    "        nn.init.xavier_uniform_(vw)        \n",
    "        self.vw = nn.Parameter(vw)\n",
    "        self.use_bias = use_bias\n",
    "        if self.use_bias: self.b = nn.Parameter(torch.zeros(1))\n",
    "    \n",
    "    @overrides\n",
    "    def get_input_dim(self) -> int:\n",
    "        return self.inp_sz\n",
    "    \n",
    "    @overrides\n",
    "    def get_output_dim(self) -> int:\n",
    "        return self.out_sz\n",
    "        \n",
    "    def forward(self, x, aug=None, mask=None):\n",
    "        e = torch.tanh(self.l1(x))\n",
    "        e = torch.einsum(\"bij,jk->bi\", [e, self.vw]) \n",
    "        if self.use_bias: e = e + self.b\n",
    "        a = torch.exp(e)\n",
    "        \n",
    "        if mask is not None: a = a.masked_fill(mask == 0, 0)\n",
    "\n",
    "        a = a / (torch.sum(a, dim=self.dim, keepdim=True) + self.eps)\n",
    "\n",
    "        weighted_input = x * a.unsqueeze(-1)\n",
    "        if self.return_attention:\n",
    "            return torch.sum(weighted_input, dim=1), a\n",
    "        else:\n",
    "            return torch.sum(weighted_input, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.modules.seq2seq_encoders import PytorchSeq2SeqWrapper, Seq2SeqEncoder\n",
    "from better_lstm import LSTM, VariationalDropout\n",
    "\n",
    "class MultiPooling(Seq2VecEncoder):\n",
    "    \"\"\"Does max and mean pooling over the temporal dimension\"\"\"\n",
    "    def __init__(self, input_sz: int):\n",
    "        super().__init__()\n",
    "        self.input_sz = input_sz\n",
    "        \n",
    "    @overrides\n",
    "    def get_input_dim(self) -> int:\n",
    "        return self.input_sz\n",
    "    \n",
    "    @overrides\n",
    "    def get_output_dim(self) -> int:\n",
    "        return self.input_sz * 2\n",
    "        \n",
    "    def forward(self, x, mask=None, aug=None):\n",
    "        max_, _ = torch.max(x, dim=1)\n",
    "        mean_ = torch.mean(x, dim=1)\n",
    "        return torch.cat([max_, mean_], dim=-1)\n",
    "\n",
    "class AugmentedMultiPool(MultiPooling):\n",
    "    def __init__(self, input_sz, aug_sz):\n",
    "        super().__init__(input_sz)\n",
    "        self.attn = Attention(input_sz, hidden_sz=input_sz, \n",
    "                              out_sz=input_sz)\n",
    "    @overrides\n",
    "    def get_output_dim(self) -> int:\n",
    "        return self.input_sz * 3\n",
    "    \n",
    "    def forward(self, x, mask=None, aug=None):\n",
    "        pooled = super().forward(x, mask=mask, aug=aug)\n",
    "        attn = self.attn(x, mask=mask, aug=None)\n",
    "        return torch.cat([pooled, attn], dim=-1)\n",
    "    \n",
    "class BiRNN(Seq2SeqEncoder):\n",
    "    def __init__(self, rnn_type, n_layers, embed_sz, hidden_sz, dropoutw=0.):\n",
    "        super().__init__()\n",
    "        self.rnn_type = rnn_type\n",
    "        self.n_layers = n_layers\n",
    "        self.embed_sz = embed_sz\n",
    "        self.hidden_sz = hidden_sz\n",
    "        in_szs = [embed_sz] + [hidden_sz * 2] * (n_layers - 1)\n",
    "        if rnn_type == \"lstm\":\n",
    "            rnns = [LSTM(in_sz, hidden_sz, batch_first=True, num_layers=1,\n",
    "                         bidirectional=True, dropoutw=dropoutw)\n",
    "                    for in_sz in in_szs]\n",
    "        else:\n",
    "            if dropoutw > 0.0:\n",
    "                warnings.warn(\"Weight dropout not currently supported with GRUs\")\n",
    "            rnns = [nn.GRU(in_sz, hidden_sz, batch_first=True, num_layers=1, \n",
    "                           bidirectional=True)\n",
    "                    for in_sz in in_szs]\n",
    "            for gru in rnns:\n",
    "                for name, param in gru.named_parameters():\n",
    "                    if \"weight_hh\" in name:\n",
    "                        nn.init.orthogonal_(param.data)\n",
    "                    elif \"weight_ih\" in name:\n",
    "                        nn.init.xavier_uniform_(param.data)\n",
    "                    elif \"bias\" in name:\n",
    "                        nn.init.zeros_(param.data)\n",
    "        self.rnns = nn.ModuleList([PytorchSeq2SeqWrapper(rnn) for rnn in rnns])\n",
    "        if config.use_attention_aux:\n",
    "            self.ln = nn.Linear(embed_sz, 64) # handle attention auxillary input here\n",
    "\n",
    "    @overrides\n",
    "    def get_input_dim(self) -> int:\n",
    "        return self.embed_sz\n",
    "    \n",
    "    @overrides\n",
    "    def get_output_dim(self) -> int:\n",
    "        if config.rnn_residual:\n",
    "            out_sz = self.hidden_sz * 2 * self.n_layers\n",
    "        else:\n",
    "            out_sz = self.hidden_sz * 2\n",
    "        if config.use_attention_aux: out_sz += 64\n",
    "        return out_sz\n",
    "    \n",
    "    def forward(self, embeds, mask=None):\n",
    "        x = embeds\n",
    "        outputs = []\n",
    "        for rnn in self.rnns:\n",
    "            x = rnn(x, mask=mask)\n",
    "            if config.rnn_residual:\n",
    "                outputs.append(x)\n",
    "        if config.rnn_residual:\n",
    "            x = torch.cat(outputs, dim=-1)\n",
    "        else:\n",
    "            x = outputs[-1]\n",
    "        if config.use_attention_aux:\n",
    "            x = torch.cat([torch.tanh(self.ln(embeds)), x], dim=-1)\n",
    "        return x\n",
    "    \n",
    "class BiRNNEncoder(Seq2VecEncoder):\n",
    "    def __init__(self, rnn: Seq2SeqEncoder,\n",
    "                 pooler: Seq2VecEncoder,\n",
    "                 dropouti=0.0, dropoutr=0.0):\n",
    "        super().__init__()\n",
    "        self.dropouti = VariationalDropout(dropouti, batch_first=True)\n",
    "        self.rnn = rnn\n",
    "        self.dropouto = VariationalDropout(dropoutr, batch_first=True)\n",
    "        self.pool = pooler\n",
    "        \n",
    "    @overrides\n",
    "    def get_input_dim(self) -> int:\n",
    "        return self.rnn.get_input_dim()\n",
    "    \n",
    "    @overrides\n",
    "    def get_output_dim(self) -> int:\n",
    "        out_dim = self.pool.get_output_dim()\n",
    "        if config.use_sentence_level_features:\n",
    "            out_dim += len(sentence_level_features)\n",
    "        return out_dim\n",
    "    \n",
    "    def _init_hidden_state(self, bs:int):\n",
    "        if self.rnn.rnn_type == \"lstm\":\n",
    "            return torch.zeros(bs, self.hidden_sz), torch.zeros(bs, self.hidden_sz)\n",
    "        else:\n",
    "            return torch.zeros(bs, self.hidden_sz)\n",
    "    \n",
    "    @overrides\n",
    "    def forward(self, x: torch.Tensor, sentence_feats: torch.Tensor,\n",
    "                mask: Optional[torch.Tensor]=None) -> torch.Tensor:\n",
    "        seq = self.rnn(x, mask)\n",
    "        seq = self.dropouto(seq)\n",
    "        vec = self.pool(seq, aug=x, mask=mask)\n",
    "        if config.use_sentence_level_features:\n",
    "            return torch.cat([sentence_feats, vec], dim=-1)\n",
    "        else:\n",
    "            return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.training.metrics import CategoricalAccuracy, BooleanAccuracy, Metric\n",
    "\n",
    "def prod(x: Iterable):\n",
    "    acc = 1\n",
    "    for v in x: acc *= v\n",
    "    return acc\n",
    "\n",
    "class MultilabelAccuracy(Metric):\n",
    "    def __init__(self, thres=0.5):\n",
    "        self.thres = 0.5\n",
    "        self.correct_count = 0\n",
    "        self.total_count = 0\n",
    "    \n",
    "    def __call__(self, logits: torch.FloatTensor, \n",
    "                 t: torch.LongTensor) -> float:\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        t = t.detach().cpu().numpy()\n",
    "        cc = ((logits >= self.thres) == t).sum()\n",
    "        tc = prod(logits.shape)\n",
    "        self.correct_count += cc\n",
    "        self.total_count += tc\n",
    "        return cc / tc\n",
    "    \n",
    "    def get_metric(self, reset: bool=False):\n",
    "        acc = self.correct_count / self.total_count\n",
    "        if reset:\n",
    "            self.reset()\n",
    "        return acc\n",
    "    \n",
    "    @overrides\n",
    "    def reset(self):\n",
    "        self.correct_count = 0\n",
    "        self.total_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.nn.util import move_to_device, has_tensor\n",
    "\n",
    "def permute(obj, p: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Given a structure (possibly) containing Tensors on the CPU,\n",
    "    permute all the Tensors\n",
    "    \"\"\"\n",
    "    if not has_tensor(obj):\n",
    "        return obj\n",
    "    elif isinstance(obj, torch.Tensor):\n",
    "        return obj[p]\n",
    "    elif isinstance(obj, dict):\n",
    "        return {key: permute(value, p) for key, value in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [permute(item, p) for item in obj]\n",
    "    elif isinstance(obj, tuple):\n",
    "        return tuple([permute(item, p) for item in obj])\n",
    "    else:\n",
    "        return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.models import Model\n",
    "from allennlp.modules.text_field_embedders import TextFieldEmbedder, BasicTextFieldEmbedder\n",
    "from torch.distributions.beta import Beta\n",
    "\n",
    "class BaselineModel(Model):\n",
    "    def __init__(self, word_embeddings: TextFieldEmbedder,\n",
    "                 encoder: Seq2VecEncoder,\n",
    "                 out_sz: int=config.n_classes,\n",
    "                 multilabel: bool=True, \n",
    "                 dropouto=0.1,\n",
    "                 mixup_alpha: int=0.2):\n",
    "        super().__init__(vocab)\n",
    "        self.word_embeddings = word_embeddings\n",
    "        self.encoder = encoder\n",
    "        feature_sz = self.encoder.get_output_dim()\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(feature_sz, 50),\n",
    "            nn.ELU(),\n",
    "            nn.Dropout(dropouto),\n",
    "            nn.Linear(50, out_sz),\n",
    "        )        \n",
    "        self.multilabel = multilabel\n",
    "        self.lambda_sampler = Beta(torch.tensor([mixup_alpha]), torch.tensor([mixup_alpha]))\n",
    "        # TODO: Handle multiclass case\n",
    "        if self.multilabel:\n",
    "            self.accuracy = MultilabelAccuracy()\n",
    "            self.per_label_acc = {c: MultilabelAccuracy() for c in label_cols}\n",
    "            self.loss = nn.BCEWithLogitsLoss()\n",
    "        else:\n",
    "            self.loss = nn.CrossEntropyLoss()\n",
    "            self.accuracy = CategoricalAccuracy()\n",
    "        self.is_test_mode = False\n",
    "            \n",
    "    def test_mode(self, val=True):\n",
    "        self.is_test_mode = val\n",
    "        \n",
    "    def get_embeddings(self, toks: Dict[str, torch.Tensor],\n",
    "                       word_feats: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Encapsulates addition of word level features\"\"\"\n",
    "        embeddings = self.word_embeddings(toks)\n",
    "        if config.use_word_level_features:\n",
    "            embeddings = torch.cat([word_feats, embeddings], dim=-1)\n",
    "        return embeddings\n",
    "\n",
    "    def forward(self, tokens: Dict[str, torch.Tensor],\n",
    "                label: torch.Tensor,\n",
    "                word_level_features: torch.Tensor,\n",
    "                sentence_level_features: torch.Tensor,\n",
    "                **meta) -> torch.Tensor:\n",
    "        if self.is_test_mode: tokens[\"tokens\"] *= 0\n",
    "        \n",
    "        mask = get_text_field_mask(tokens)\n",
    "        embeddings = self.get_embeddings(tokens, word_level_features)\n",
    "        state = self.encoder(embeddings, \n",
    "                             sentence_feats=sentence_level_features, \n",
    "                             mask=mask)\n",
    "        class_logits = self.projection(state)\n",
    "        \n",
    "        output = {\"class_logits\": class_logits}\n",
    "\n",
    "        output[\"accuracy\"] = self.accuracy(class_logits, label)\n",
    "        output[\"loss\"] = self.loss(class_logits, label)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def mixup(self, tokens: Dict[str, torch.Tensor],\n",
    "              label: torch.Tensor,\n",
    "              word_level_features: torch.Tensor,\n",
    "              sentence_level_features: torch.Tensor,\n",
    "              **meta) -> TensorDict:\n",
    "        # generate new tokens and labels\n",
    "        bs = label.size(0)\n",
    "        shuf = torch.randperm(bs).to(label.device)\n",
    "        tokens2 = permute(tokens, shuf)\n",
    "        labels1, labels2 = label, permute(label, shuf)\n",
    "        # TODO: Think of how to handle this masking intelligently\n",
    "        mask1, mask2 = (get_text_field_mask(t) for t in (tokens, tokens2))\n",
    "        embs1, embs2 = (self.get_embeddings(t, word_level_features) for t in (tokens, tokens2))\n",
    "        # interpolate\n",
    "        ratios = self.lambda_sampler.sample((bs, 1)).to(label.device)\n",
    "        embs = ratios * embs1 + (1-ratios) * embs2\n",
    "        label = ratios.squeeze(2) * labels1 + (1-ratios.squeeze(2)) * labels2\n",
    "        \n",
    "        # remaining process is the same\n",
    "        # TODO: Handle stat feats\n",
    "        state = self.encoder(embs, sentence_level_features, mask1 * mask2) # TODO: Handle masking\n",
    "        class_logits = self.projection(state)\n",
    "        \n",
    "        output = {\"loss\": self.loss(class_logits, label)}\n",
    "        return output\n",
    "    \n",
    "    def get_metrics(self, reset: bool = False) -> Dict[str, float]:\n",
    "        return {\"accuracy\": self.accuracy.get_metric(reset)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.set(\"vocab_size\", min(vocab.get_vocab_size(), config.max_vocab_size))\n",
    "if config.model_type == \"standard\":\n",
    "    config.set(\"embedding_dim\", 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import warnings\n",
    "\n",
    "def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "def get_fasttext_embeddings(model_path: str, vocab: Vocabulary):\n",
    "    prog_bar = tqdm(open(model_path, encoding=\"utf8\", errors='ignore'))\n",
    "    prog_bar.set_description(\"Loading embeddings\")\n",
    "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in prog_bar\n",
    "                             if len(o)>100)\n",
    "    all_embs = np.stack(embeddings_index.values())\n",
    "\n",
    "    embeddings = np.zeros((config.vocab_size + 5, 300))\n",
    "    n_missing_tokens = 0\n",
    "    prog_bar = tqdm(vocab.get_index_to_token_vocabulary().items())\n",
    "    prog_bar.set_description(\"Creating matrix\")\n",
    "    for idx, token in prog_bar:\n",
    "        if idx == 0: continue # keep padding as all zeros\n",
    "        if idx == 1: continue # Treat unknown words as dropped words\n",
    "        if token == \"[MASK]\":\n",
    "            embeddings[idx, :] = np.random.randn(300) * 0.5\n",
    "        if token not in embeddings_index:\n",
    "            n_missing_tokens += 1\n",
    "            if n_missing_tokens < 10:\n",
    "                warnings.warn(f\"Token {token} not in embeddings: did you change preprocessing?\")\n",
    "            if n_missing_tokens == 10:\n",
    "                warnings.warn(f\"More than {n_missing_tokens} missing, supressing warnings\")\n",
    "        else:\n",
    "            embeddings[idx, :] = embeddings_index[token]\n",
    "    \n",
    "    if n_missing_tokens > 0:\n",
    "        warnings.warn(f\"{n_missing_tokens} in total are missing from embedding text file\")\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with timer(\"Loading embeddings\"):\n",
    "    if config.model_type == \"standard\":\n",
    "        embedding_weights = get_fasttext_embeddings(config.ft_model_path, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomEmbedding(Embedding):\n",
    "    # TODO: Fix (make this decently efficient: currently allocating two embeddings)\n",
    "    def __init__(self, num_embeddings, embedding_dim,\n",
    "                 padding_index=None, max_norm=None, trainable=True,\n",
    "                 weight=None, dropout=0., scale=None):\n",
    "        super().__init__(num_embeddings, embedding_dim, weight=weight,\n",
    "                         padding_index=padding_index, max_norm=max_norm,\n",
    "                         trainable=trainable)\n",
    "        self.dropout = dropout\n",
    "        self.scale = scale\n",
    "        self.padding_idx = padding_index\n",
    "\n",
    "    def forward(self, words):\n",
    "        weight = self.weight\n",
    "        if self.dropout > 0.0 and self.training:\n",
    "            mask = weight.data.new().resize_((weight.size(0), 1)).bernoulli_(1 - self.dropout).expand_as(weight) / (1 - self.dropout)\n",
    "            masked_embed_weight = mask * weight\n",
    "        else:\n",
    "            masked_embed_weight = weight\n",
    "        if self.scale:\n",
    "            masked_embed_weight = scale.expand_as(masked_embed_weight) * masked_embed_weight\n",
    "\n",
    "        padding_idx = self.padding_idx\n",
    "        if padding_idx is None:\n",
    "            padding_idx = -1\n",
    "\n",
    "        X = torch.nn.functional.embedding(words, masked_embed_weight,\n",
    "            padding_idx, self.max_norm, self.norm_type,\n",
    "            self.scale_grad_by_freq, self.sparse\n",
    "          )\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.modules.text_field_embedders.text_field_embedder import TextFieldEmbedder\n",
    "from allennlp.modules.time_distributed import TimeDistributed\n",
    "\n",
    "# TODO: Implement\n",
    "class ElmoTextFieldEmbedder(TextFieldEmbedder):\n",
    "    # AllenNLP support for caching sucks by default\n",
    "    # so we have to write our own embedder to bypass this problem\n",
    "    def __init__(self,\n",
    "                 token_embedders: Dict[str, Any],\n",
    "                 embedder_to_indexer_map: Dict[str, List[str]] = None,\n",
    "                 allow_unmatched_keys: bool = False) -> None:\n",
    "        super().__init__()\n",
    "        self._token_embedders = token_embedders\n",
    "        self._embedder_to_indexer_map = embedder_to_indexer_map\n",
    "        for key, embedder in token_embedders.items():\n",
    "            name = 'token_embedder_%s' % key\n",
    "            self.add_module(name, embedder)\n",
    "        self._allow_unmatched_keys = allow_unmatched_keys\n",
    "    \n",
    "    @overrides\n",
    "    def get_output_dim(self) -> int:\n",
    "        output_dim = 0\n",
    "        for embedder in self._token_embedders.values():\n",
    "            output_dim += embedder.get_output_dim()\n",
    "        return output_dim\n",
    "    \n",
    "    def forward(self, text_field_input: Dict[str, torch.Tensor],\n",
    "                num_wrapping_dims: int = 0) -> torch.Tensor:\n",
    "        if self._token_embedders.keys() != text_field_input.keys():\n",
    "            if not self._allow_unmatched_keys:\n",
    "                message = \"Mismatched token keys: %s and %s\" % (str(self._token_embedders.keys()),\n",
    "                                                                str(text_field_input.keys()))\n",
    "                raise ConfigurationError(message)\n",
    "        embedded_representations = []\n",
    "        keys = sorted(self._token_embedders.keys())\n",
    "        for key in keys:\n",
    "            # If we pre-specified a mapping explictly, use that.\n",
    "            if self._embedder_to_indexer_map is not None:\n",
    "                tensors = [text_field_input[indexer_key] for\n",
    "                           indexer_key in self._embedder_to_indexer_map[key]]\n",
    "            else:\n",
    "                # otherwise, we assume the mapping between indexers and embedders\n",
    "                # is bijective and just use the key directly.\n",
    "                tensors = [text_field_input[key]]\n",
    "            # Note: need to use getattr here so that the pytorch voodoo\n",
    "            # with submodules works with multiple GPUs.\n",
    "            embedder = getattr(self, 'token_embedder_{}'.format(key))\n",
    "            for _ in range(num_wrapping_dims):\n",
    "                embedder = TimeDistributed(embedder)\n",
    "            # Force embedder to use word inputs\n",
    "            if key == \"tokens\":\n",
    "                token_vectors = embedder(tensors[0], \n",
    "                                         word_inputs=tensors[0] if config.cache_elmo_embeddings else None)\n",
    "            else:\n",
    "                token_vectors = embedder(*tensors)\n",
    "            embedded_representations.append(token_vectors)\n",
    "        return torch.cat(embedded_representations, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_pretrained_bert.modeling import BertModel\n",
    "from allennlp.modules.token_embedders.token_embedder import TokenEmbedder\n",
    "\n",
    "class CustomBertEmbedder(TokenEmbedder):\n",
    "    \"\"\"\n",
    "    A ``TokenEmbedder`` that produces BERT embeddings for your tokens.\n",
    "    Should be paired with a ``BertIndexer``, which produces wordpiece ids.\n",
    "    Sums last 4 hidden layers for now (might use scalar mix in the future)\n",
    "    \"\"\"\n",
    "    def __init__(self, pretrained_model: str,\n",
    "                 use_scalar_mix: bool = False,\n",
    "                 fine_tune: bool = False,\n",
    "                 n_hidden_layers: int = 4) -> None:\n",
    "        super().__init__()\n",
    "        if use_scalar_mix and fine_tune:\n",
    "            raise ConfigurationError(\"Choose mix or fine tuning\")\n",
    "        \n",
    "        self.bert_model = BertModel.from_pretrained(pretrained_model)\n",
    "        for param in self.bert_model.parameters():\n",
    "            param.requires_grad = fine_tune\n",
    "        self.output_dim = self.bert_model.config.hidden_size\n",
    "        self.n_hidden_layers = n_hidden_layers\n",
    "        if use_scalar_mix:\n",
    "            self._scalar_mix = ScalarMix(n_hidden_layers,\n",
    "                                         do_layer_norm=False)\n",
    "        else:\n",
    "            self._scalar_mix = None\n",
    "\n",
    "    def get_output_dim(self) -> int:\n",
    "        return self.output_dim\n",
    "\n",
    "    def forward(self,\n",
    "                input_ids: torch.LongTensor,\n",
    "                offsets: torch.LongTensor = None,\n",
    "                token_type_ids: torch.LongTensor = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_ids : ``torch.LongTensor``\n",
    "            The (batch_size, ..., max_sequence_length) tensor of wordpiece ids.\n",
    "        offsets : ``torch.LongTensor``, optional\n",
    "            The BERT embeddings are one per wordpiece. However it's possible/likely\n",
    "            you might want one per original token. In that case, ``offsets``\n",
    "            represents the indices of the desired wordpiece for each original token.\n",
    "            Depending on how your token indexer is configured, this could be the\n",
    "            position of the last wordpiece for each token, or it could be the position\n",
    "            of the first wordpiece for each token.\n",
    "\n",
    "            For example, if you had the sentence \"Definitely not\", and if the corresponding\n",
    "            wordpieces were [\"Def\", \"##in\", \"##ite\", \"##ly\", \"not\"], then the input_ids\n",
    "            would be 5 wordpiece ids, and the \"last wordpiece\" offsets would be [3, 4].\n",
    "            If offsets are provided, the returned tensor will contain only the wordpiece\n",
    "            embeddings at those positions, and (in particular) will contain one embedding\n",
    "            per token. If offsets are not provided, the entire tensor of wordpiece embeddings\n",
    "            will be returned.\n",
    "        token_type_ids : ``torch.LongTensor``, optional\n",
    "            If an input consists of two sentences (as in the BERT paper),\n",
    "            tokens from the first sentence should have type 0 and tokens from\n",
    "            the second sentence should have type 1.  If you don't provide this\n",
    "            (the default BertIndexer doesn't) then it's assumed to be all 0s.\n",
    "        \"\"\"\n",
    "        # pylint: disable=arguments-differ\n",
    "        if token_type_ids is None:\n",
    "            token_type_ids = torch.zeros_like(input_ids)\n",
    "\n",
    "        input_mask = (input_ids != 0).long()\n",
    "\n",
    "        # input_ids may have extra dimensions, so we reshape down to 2-d\n",
    "        # before calling the BERT model and then reshape back at the end.\n",
    "        all_encoder_layers, _ = self.bert_model(input_ids=nn_util.combine_initial_dims(input_ids),\n",
    "                                                token_type_ids=nn_util.combine_initial_dims(token_type_ids),\n",
    "                                                attention_mask=nn_util.combine_initial_dims(input_mask))\n",
    "        if self._scalar_mix is not None:\n",
    "            mix = self._scalar_mix(all_encoder_layers[-self.n_hidden_layers:], input_mask)\n",
    "        else:\n",
    "            mix = torch.stack(all_encoder_layers[-self.n_hidden_layers:]).mean(dim=0)\n",
    "\n",
    "        # At this point, mix is (batch_size * d1 * ... * dn, sequence_length, embedding_dim)\n",
    "\n",
    "        if offsets is None:\n",
    "            # Resize to (batch_size, d1, ..., dn, sequence_length, embedding_dim)\n",
    "            return nn_util.uncombine_initial_dims(mix, input_ids.size())\n",
    "        else:\n",
    "            # offsets is (batch_size, d1, ..., dn, orig_sequence_length)\n",
    "            offsets2d = nn_util.combine_initial_dims(offsets)\n",
    "            # now offsets is (batch_size * d1 * ... * dn, orig_sequence_length)\n",
    "            range_vector = nn_util.get_range_vector(offsets2d.size(0),\n",
    "                                                 device=nn_util.get_device_of(mix)).unsqueeze(1)\n",
    "            # selected embeddings is also (batch_size * d1 * ... * dn, orig_sequence_length)\n",
    "            selected_embeddings = mix[range_vector, offsets2d]\n",
    "\n",
    "            return util.uncombine_initial_dims(selected_embeddings, offsets.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.modules.text_field_embedders import BasicTextFieldEmbedder\n",
    "\n",
    "if config.model_type == \"standard\":\n",
    "    token_embedding = CustomEmbedding(num_embeddings=config.vocab_size + 5,\n",
    "                                      embedding_dim=config.embedding_dim,\n",
    "                                      trainable=not config.freeze_embeddings,\n",
    "                                      weight=torch.tensor(embedding_weights, dtype=torch.float),\n",
    "                                      dropout=config.dropoute, padding_index=0)\n",
    "    word_embeddings = BasicTextFieldEmbedder({\"tokens\": token_embedding})\n",
    "elif \"elmo\" in config.model_type:\n",
    "    from allennlp.modules.token_embedders import ElmoTokenEmbedder\n",
    "    from allennlp.modules.elmo import Elmo\n",
    "\n",
    "    options_file = 'https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x1024_128_2048cnn_1xhighway/elmo_2x1024_128_2048cnn_1xhighway_options.json'\n",
    "    weight_file = 'https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x1024_128_2048cnn_1xhighway/elmo_2x1024_128_2048cnn_1xhighway_weights.hdf5'\n",
    "    \n",
    "    all_words_ordered = [w for i, w in sorted([(i, w) for w, i in vocab.get_token_to_index_vocabulary().items()])]\n",
    "    elmo_embedder = ElmoTokenEmbedder(\n",
    "        options_file, weight_file, dropout=config.dropoute,\n",
    "        vocab_to_cache=all_words_ordered if config.cache_elmo_embeddings else None\n",
    "    )\n",
    "    # TODO: Find a way to skip character encodings\n",
    "    word_embeddings = ElmoTextFieldEmbedder({\"tokens\": elmo_embedder})\n",
    "elif \"bert\" in config.model_type:\n",
    "    from allennlp.modules.token_embedders.bert_token_embedder import PretrainedBertEmbedder\n",
    "    bert_embedder = CustomBertEmbedder(\n",
    "            pretrained_model=config.model_type,\n",
    "            fine_tune=False, use_scalar_mix=False,\n",
    "    )\n",
    "    word_embeddings: TextFieldEmbedder = BasicTextFieldEmbedder({\"tokens\": bert_embedder},\n",
    "                                                                 # we'll be ignoring masks so we'll need to set this to True\n",
    "                                                                allow_unmatched_keys = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.pooling_type != \"bert_pool\":\n",
    "    embed_sz = word_embeddings.get_output_dim()\n",
    "    if config.use_word_level_features: \n",
    "        embed_sz += len(word_level_features)\n",
    "    rnn = BiRNN(rnn_type=rnn_type, n_layers=config.num_layers, \n",
    "                embed_sz=embed_sz, hidden_sz=config.hidden_sz, \n",
    "                dropoutw=config.dropoutw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.pooling_type != \"bert_pool\":\n",
    "    if config.pooling_type == \"attention\":\n",
    "        pooler = Attention(rnn.get_output_dim(), hidden_sz=rnn.get_output_dim(),\n",
    "                           out_sz=rnn.get_output_dim(), dim=1, \n",
    "                           use_bias=config.attention_bias)\n",
    "    elif config.pooling_type == \"multipool\":\n",
    "        pooler = MultiPooling(rnn.get_output_dim())\n",
    "    elif config.pooling_type == \"augmented_multipool\":\n",
    "        pooler = AugmentedMultiPool(rnn.get_output_dim(), \n",
    "                                    aug_sz=embed_sz)\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid pooling type {config.pooling_type}\")\n",
    "\n",
    "    encoder = BiRNNEncoder(\n",
    "            rnn,\n",
    "            pooler,\n",
    "            dropouti=config.dropouti,\n",
    "            dropoutr=config.dropoutr,\n",
    "        )\n",
    "else:\n",
    "    BERT_DIM = word_embeddings.get_output_dim()\n",
    "\n",
    "    class BertSentencePooler(Seq2VecEncoder):\n",
    "        def forward(self, embs: torch.tensor, \n",
    "                    mask: torch.tensor=None,\n",
    "                    **kwargs,\n",
    "                   ) -> torch.tensor:\n",
    "            # extract first token tensor\n",
    "            return embs[:, 0]\n",
    "\n",
    "        @overrides\n",
    "        def get_output_dim(self) -> int:\n",
    "            return BERT_DIM\n",
    "\n",
    "    encoder = BertSentencePooler(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BaselineModel(\n",
    "    word_embeddings, \n",
    "    encoder, \n",
    "    out_sz=config.n_classes,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize bias according to prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.bias_init:\n",
    "    class_bias = torch.zeros(len(label_cols))\n",
    "    for i, _ in enumerate(label_cols):\n",
    "        p = train_labels[:, i].mean()\n",
    "        class_bias[i] = np.log(p / (1-p))\n",
    "\n",
    "    model.projection[-1].bias.data = class_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_GPU: model.cuda()\n",
    "else: model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "init_state_dict = deepcopy(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic sanity checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = nn_util.move_to_device(batch, 0 if USE_GPU else -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = batch[\"tokens\"]\n",
    "labels = batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = get_text_field_mask(tokens)\n",
    "wlfs = batch[\"word_level_features\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embs = model.word_embeddings.token_embedder_tokens(tokens[\"tokens\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = model.word_embeddings(tokens)\n",
    "if config.use_word_level_features:\n",
    "    embeddings = torch.cat([wlfs, embeddings], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.pooling_type != \"bert_pool\":\n",
    "    encoded = model.encoder.rnn(embeddings, mask=mask)\n",
    "else:\n",
    "    encoded = model.encoder(embeddings, mask=mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(**batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = model(**batch)[\"loss\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch[\"label\"].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch[\"tokens\"][\"tokens\"].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.mixup(**batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Callback(): # borrowed from fastai\n",
    "    \"Base class for callbacks that want to record values, dynamically change learner params, etc.\"\n",
    "    _order=0\n",
    "\n",
    "    def set_trainer(self, trainer):\n",
    "        self.trainer = trainer\n",
    "\n",
    "    def on_train_begin(self, **kwargs:Any)->None:\n",
    "        \"To initialize constants in the callback.\"\n",
    "        pass\n",
    "    def on_epoch_begin(self, **kwargs:Any)->None:\n",
    "        \"At the beginning of each epoch.\"\n",
    "        pass\n",
    "    def on_batch_begin(self, **kwargs:Any)->None:\n",
    "        \"Set HP before the step is done. Returns xb, yb (which can allow us to modify the input at that step if needed).\"\n",
    "        pass\n",
    "    def on_batch_loss(self, batch, for_training=True) -> torch.Tensor:\n",
    "        pass\n",
    "    def on_loss_begin(self, **kwargs:Any)->None:\n",
    "        \"Called after forward pass but before loss has been computed. Returns the output (which can allow us to modify it).\"\n",
    "        pass\n",
    "    def on_backward_begin(self, loss: torch.Tensor, **kwargs)->None:\n",
    "        \"\"\"Called after the forward pass and the loss has been computed, but before backprop.\n",
    "           Returns the loss (which can allow us to modify it, for instance for reg functions)\"\"\"\n",
    "        pass\n",
    "    def on_backward_end(self, loss: torch.Tensor, **kwargs:Any)->None:\n",
    "        \"Called after backprop but before optimizer step. Useful for true weight decay in AdamW.\"\n",
    "        pass\n",
    "    def on_step_end(self, loss: torch.Tensor, **kwargs:Any)->None:\n",
    "        \"Called after the step of the optimizer but before the gradients are zeroed.\"\n",
    "        pass\n",
    "    def on_batch_end(self, **kwargs:Any)->None:\n",
    "        \"Called at the end of the batch.\"\n",
    "        pass\n",
    "    def on_epoch_end(self, **kwargs:Any)->bool:\n",
    "        \"Called at the end of an epoch.\"\n",
    "        return False\n",
    "    def on_train_end(self, **kwargs:Any)->None:\n",
    "        \"Useful for cleaning up things and saving files/models.\"\n",
    "        pass\n",
    "    \n",
    "    def get_state(self, minimal:bool=True):\n",
    "        to_remove = ['exclude', 'not_min'] + getattr(self, 'exclude', []).copy()\n",
    "        if minimal: to_remove += getattr(self, 'not_min', []).copy()\n",
    "        return {k:v for k,v in self.__dict__.items() if k not in to_remove}\n",
    "    \n",
    "    def  __repr__(self): \n",
    "        attrs = func_args(self.__init__)\n",
    "        to_remove = getattr(self, 'exclude', [])\n",
    "        list_repr = [self.__class__.__name__] + [f'{k}: {getattr(self, k)}' for k in attrs if k != 'self' and k not in to_remove]\n",
    "        return '\\n'.join(list_repr) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "def ifnone(x, alt): return alt if x is None else x\n",
    "\n",
    "class CallbackHandler:\n",
    "    def __init__(self, callbacks):\n",
    "        self.callbacks = ifnone(callbacks, [])\n",
    "        self.callbacks = sorted(self.callbacks, key=lambda o: getattr(o, '_order', 0))\n",
    "    \n",
    "    def call(self, attr, *args, **kwargs):\n",
    "        for cb in self.callbacks: getattr(cb, attr)(*args, **kwargs)\n",
    "    \n",
    "    def set_trainer(self, trainer):\n",
    "        self.trainer = trainer\n",
    "        self.call(\"set_trainer\", trainer)\n",
    "        \n",
    "    def on_train_begin(self): self.call(\"on_train_begin\")\n",
    "    def on_batch_begin(self): self.call(\"on_batch_begin\")\n",
    "    def on_batch_loss(self, batch, for_training=True):\n",
    "        loss = None\n",
    "        for cb in self.callbacks:\n",
    "            cb_loss = cb.on_batch_loss(batch, for_training=for_training)\n",
    "            if cb_loss is not None:\n",
    "                loss = cb_loss if loss is None else loss + cb_loss\n",
    "        return loss\n",
    "    def on_backward_begin(self, loss: torch.Tensor): self.call(\"on_backward_begin\", loss)\n",
    "    def on_backward_end(self, loss: torch.Tensor): self.call('on_backward_end', loss)\n",
    "    def on_step_end(self, loss: torch.Tensor): self.call('on_step_end', loss)\n",
    "    def on_batch_end(self): self.call(\"on_batch_end\")\n",
    "    def on_epoch_end(self): self.call(\"on_epoch_end\", val_loss)\n",
    "    def on_train_end(self): self.call(\"on_train_end\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StopTraining(Exception): pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NanWeightMonitor(Callback):\n",
    "    def on_backward_end(self, loss):\n",
    "        for name, param in self.trainer.model.named_parameters():\n",
    "            if torch.isnan(param.data).any() or torch.isinf(param.data).any():\n",
    "                raise StopTraining(f\"Nan/Inf weights in param {name}: \\n {param}\")\n",
    "try:\n",
    "    import jupyter_slack\n",
    "    can_notify = True\n",
    "except:\n",
    "    jupyter_slack = None\n",
    "    can_notify = False\n",
    "\n",
    "class SlackNotification(Callback):\n",
    "    def __init__(self, silent):\n",
    "        self.silent = silent\n",
    "    def on_train_end(self):\n",
    "        if not self.silent and can_notify:\n",
    "            try:\n",
    "                jupyter_slack.notify_self(f\"Finished training with state {self.trainer._state}\")\n",
    "            except: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TensorboardCallback(Callback):\n",
    "    \"\"\"For now, delegate all processing to the trainer's own methods\"\"\"\n",
    "    def on_batch_begin(self):\n",
    "        self._log_histograms_this_batch = \\\n",
    "        self.trainer._histogram_interval is not None and (\n",
    "            self.trainer._batch_num_total % self.trainer._histogram_interval == 0)\n",
    "    \n",
    "    def on_backward_end(self, loss):\n",
    "        if self._log_histograms_this_batch:\n",
    "            # get the magnitude of parameter updates for logging\n",
    "            # We need a copy of current parameters to compute magnitude of updates,\n",
    "            # and copy them to CPU so large models won't go OOM on the GPU.\n",
    "            self.param_updates = {\n",
    "                name: param.detach().cpu().clone()\n",
    "                for name, param in self.trainer.model.named_parameters()\n",
    "            }\n",
    "    \n",
    "    def on_step_end(self, loss):\n",
    "        if self._log_histograms_this_batch:\n",
    "            for name, param in self.trainer.model.named_parameters():\n",
    "                self.param_updates[name].sub_(param.detach().cpu())\n",
    "                update_norm = torch.norm(self.param_updates[name].view(-1, ))\n",
    "                param_norm = torch.norm(param.view(-1, )).cpu()\n",
    "                self.trainer._tensorboard.add_train_scalar(\n",
    "                    \"gradient_update/\" + name,\n",
    "                     update_norm / (param_norm + 1e-7),\n",
    "                     batch_num_total\n",
    "                )\n",
    "            self.param_updates = {} # release memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mixup(Callback):\n",
    "    \"\"\"Does mixup in embedding space\n",
    "    TODO: Figure out how to best handle masking...\n",
    "    \"\"\"\n",
    "    def __init__(self, weight: float, \n",
    "                 batch_iterator: Optional[Iterable[TensorDict]]=None):\n",
    "        self.weight = weight\n",
    "        self.batch_iterator = batch_iterator\n",
    "        \n",
    "    def on_batch_loss(self, batch: TensorDict, for_training=True):\n",
    "        if for_training and self.weight > 0.:\n",
    "            # use mixup iterator if exists\n",
    "            if self.batch_iterator is not None: batch = next(self.batch_iterator)\n",
    "            mixup_output_dict = self.model.mixup(**batch)\n",
    "            return mixup_output_dict[\"loss\"] * self.weight\n",
    "        else:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscreteMixup(Callback):\n",
    "    \"\"\"Mixes up and concatenates sentences within a batch\"\"\"\n",
    "    def __init__(self, weight: float, \n",
    "                 batch_iterator: Optional[Iterable[TensorDict]]=None):\n",
    "        self.weight = weight\n",
    "        self.batch_iterator = batch_iterator\n",
    "    \n",
    "    def on_batch_loss(self, batch: TensorDict, for_training=True):\n",
    "        if for_training and self.weight > 0.:\n",
    "            # use mixup iterator if exists\n",
    "            if self.batch_iterator is not None: \n",
    "                batch = next(self.batch_iterator)\n",
    "                \n",
    "            # create permutation\n",
    "            tokens, label = batch[\"tokens\"], batch[\"label\"]\n",
    "            bs = label.size(0)\n",
    "            shuf = torch.randperm(bs).to(label.device)\n",
    "            tokens2 = permute(tokens, shuf)\n",
    "            labels2 = permute(label, shuf)\n",
    "            \n",
    "            # join the sentences\n",
    "            n_tokens1 = get_text_field_mask(tokens).sum(1)\n",
    "            n_tokens2 = get_text_field_mask(tokens2).sum(1)\n",
    "            maxlen = min(config.max_seq_len, (n_tokens1 + n_tokens2).sum())\n",
    "            # TODO: Is there a faster way?\n",
    "            new_tokens = torch.zeros(bs, maxlen, \n",
    "                                     dtype=torch.long).to(label.device)\n",
    "            for i, (t1, t2) in enumerate(zip(tokens[\"tokens\"], tokens2[\"tokens\"])):\n",
    "                l1, l2 = n_tokens1[i].item(), n_tokens2[i].item()\n",
    "                new_tokens[i, :l1] = t1 # TODO: Fairly divide the capacity\n",
    "                new_tokens[i, l1:min(maxlen, l1+l2)] = \\\n",
    "                    t2[:min(maxlen-l1, l2)]\n",
    "            \n",
    "            # compute loss on new batch\n",
    "            new_batch = {k: v for k, v in batch.items()}\n",
    "            new_batch[\"tokens\"] = {\"tokens\": new_tokens}\n",
    "            new_batch[\"label\"] = new_label\n",
    "            return model(**new_batch)[\"loss\"] * self.weight\n",
    "        else:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Optimizer\n",
    "class AdamW(Optimizer):\n",
    "    \"\"\"\n",
    "    Adam with weight decay decoupled, borrowed from PyTorch Pretrained BERT\n",
    "    \"\"\"\n",
    "    def __init__(self, params, lr=0.01, b1=0.9, b2=0.999, e=1e-6, weight_decay=0.01,\n",
    "                 max_grad_norm=1.0):\n",
    "        defaults = dict(lr=lr, b1=b1, b2=b2, e=e, weight_decay=weight_decay,\n",
    "                        max_grad_norm=max_grad_norm)\n",
    "        super().__init__(params, defaults)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        \"\"\"Performs a single optimization step.\n",
    "        Arguments:\n",
    "            closure (callable, optional): A closure that reevaluates the model\n",
    "                and returns the loss.\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data\n",
    "                if grad.is_sparse:\n",
    "                    raise RuntimeError('Adam does not support sparse gradients, please consider SparseAdam instead')\n",
    "\n",
    "                state = self.state[p]\n",
    "\n",
    "                # State initialization\n",
    "                if len(state) == 0:\n",
    "                    state['step'] = 0\n",
    "                    # Exponential moving average of gradient values\n",
    "                    state['next_m'] = torch.zeros_like(p.data)\n",
    "                    # Exponential moving average of squared gradient values\n",
    "                    state['next_v'] = torch.zeros_like(p.data)\n",
    "\n",
    "                next_m, next_v = state['next_m'], state['next_v']\n",
    "                beta1, beta2 = group['b1'], group['b2']\n",
    "\n",
    "                # Decay the first and second moment running average coefficient\n",
    "                # In-place operations to update the averages at the same time\n",
    "                next_m.mul_(beta1).add_(1 - beta1, grad)\n",
    "                next_v.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
    "                update = next_m / (next_v.sqrt() + group['e'])\n",
    "\n",
    "                if group['weight_decay'] > 0.0:\n",
    "                    update += group['weight_decay'] * p.data\n",
    "                update_with_lr = group['lr'] * update\n",
    "                p.data.add_(-update_with_lr)\n",
    "\n",
    "                state['step'] += 1\n",
    "\n",
    "                # step_size = lr_scheduled * math.sqrt(bias_correction2) / bias_correction1\n",
    "                # No bias correction\n",
    "                # bias_correction1 = 1 - beta1 ** state['step']\n",
    "                # bias_correction2 = 1 - beta2 ** state['step']\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.training import trainer as _trainer\n",
    "from allennlp.training.trainer import *\n",
    "import math\n",
    "logger = _trainer.logger\n",
    "\n",
    "N_BATCHES_PER_UPDATE = config.batch_size // config.computational_batch_size\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def __init__(self, *args,\n",
    "                 callbacks=[],\n",
    "                 **kwargs):\n",
    "        \"\"\"Applies mixup to mixup_ratio samples in each batch\"\"\"\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.callbacks = callbacks\n",
    "        if not isinstance(callbacks, CallbackHandler):\n",
    "            self.callbacks = CallbackHandler(callbacks)\n",
    "        self.callbacks.set_trainer(self)\n",
    "        self._state = {}\n",
    "        \n",
    "    @overrides\n",
    "    def batch_loss(self, batch: TensorDict, for_training=True) -> torch.Tensor:\n",
    "        batch = nn_util.move_to_device(batch, self._cuda_devices[0])\n",
    "        output_dict = self.model(**batch)\n",
    "        try:\n",
    "            loss = output_dict[\"loss\"]\n",
    "            if for_training:\n",
    "                loss += self.model.get_regularization_penalty()\n",
    "        except KeyError:\n",
    "            if for_training:\n",
    "                raise RuntimeError(\"The model you are trying to optimize does not contain a\"\n",
    "                                   \" 'loss' key in the output of model.forward(inputs).\")\n",
    "            return None\n",
    "        \n",
    "        cb_loss = self.callbacks.on_batch_loss(batch, for_training=for_training)\n",
    "        if cb_loss is not None: loss += cb_loss\n",
    "        return loss\n",
    "        \n",
    "    @gpu_mem_restore\n",
    "    def _train_epoch(self, epoch: int) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Trains one epoch and returns metrics. Copied from source\n",
    "        \"\"\"\n",
    "        logger.info(\"Epoch %d/%d\", epoch, self._num_epochs - 1)\n",
    "        peak_cpu_usage = peak_memory_mb()\n",
    "        logger.info(f\"Peak CPU memory usage MB: {peak_cpu_usage}\")\n",
    "        gpu_usage = []\n",
    "        for gpu, memory in gpu_memory_mb().items():\n",
    "            gpu_usage.append((gpu, memory))\n",
    "            logger.info(f\"GPU {gpu} memory usage MB: {memory}\")\n",
    "\n",
    "        train_loss = 0.0\n",
    "        # Set the model to \"train\" mode.\n",
    "        self.model.train()\n",
    "\n",
    "        # Get tqdm for the training batches\n",
    "        train_generator = self.iterator(self.train_data,\n",
    "                                        num_epochs=1,\n",
    "                                        shuffle=self.shuffle)\n",
    "        num_training_batches = self.iterator.get_num_batches(self.train_data)\n",
    "        self._last_log = time.time()\n",
    "        last_save_time = time.time()\n",
    "\n",
    "        batches_this_epoch = 0\n",
    "        if self._batch_num_total is None:\n",
    "            self._batch_num_total = 0\n",
    "\n",
    "        if self._histogram_interval is not None:\n",
    "            histogram_parameters = set(self.model.get_parameters_for_histogram_tensorboard_logging())\n",
    "\n",
    "        logger.info(\"Training\")\n",
    "        train_generator_tqdm = Tqdm.tqdm(train_generator,\n",
    "                                         total=num_training_batches)\n",
    "        cumulative_batch_size = 0\n",
    "        for batch in train_generator_tqdm:\n",
    "            batches_this_epoch += 1\n",
    "            self._batch_num_total += 1\n",
    "            batch_num_total = self._batch_num_total\n",
    "\n",
    "            self.callbacks.on_batch_begin()\n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            ###########\n",
    "            # Custom  #\n",
    "            ###########\n",
    "            loss = self.batch_loss(batch, for_training=True)\n",
    "            if torch.isnan(loss): # TODO: Move to callback\n",
    "                raise StopTraining(\"nan loss encountered\")\n",
    "            train_loss += loss.item()\n",
    "            # wait to update\n",
    "            if (batches_this_epoch % N_BATCHES_PER_UPDATE) != 0: continue\n",
    "            ###############\n",
    "            # End Custom  #\n",
    "            ###############\n",
    "            self.callbacks.on_backward_begin(loss)\n",
    "            loss.backward()\n",
    "            self.callbacks.on_backward_end(loss)\n",
    "            batch_grad_norm = self.rescale_gradients()\n",
    "            \n",
    "            # This does nothing if batch_num_total is None or you are using an\n",
    "            # LRScheduler which doesn't update per batch.\n",
    "            if self._learning_rate_scheduler:\n",
    "                self._learning_rate_scheduler.step_batch(batch_num_total)\n",
    "\n",
    "            # TODO: Move to callback\n",
    "            self.optimizer.step()\n",
    "            self.callbacks.on_step_end(loss)\n",
    "\n",
    "            # Update the description with the latest metrics\n",
    "            metrics = self._get_metrics(train_loss, batches_this_epoch)\n",
    "            self.metrics = metrics # temporarily for delegating processing to callbacks\n",
    "            description = self._description_from_metrics(metrics)\n",
    "\n",
    "            train_generator_tqdm.set_description(description, refresh=False)\n",
    "\n",
    "            # TODO: Move all this Tensorboard processing to callbacks\n",
    "            # Log parameter values to Tensorboard\n",
    "            if self._batch_num_total % self._summary_interval == 0:\n",
    "                if self._should_log_parameter_statistics:\n",
    "                    self._parameter_and_gradient_statistics_to_tensorboard(batch_num_total, batch_grad_norm)\n",
    "                if self._should_log_learning_rate:\n",
    "                    self._learning_rates_to_tensorboard(batch_num_total)\n",
    "                self._tensorboard.add_train_scalar(\"loss/loss_train\", metrics[\"loss\"], batch_num_total)\n",
    "                self._metrics_to_tensorboard(batch_num_total,\n",
    "                                             {\"epoch_metrics/\" + k: v for k, v in metrics.items()})\n",
    "\n",
    "            if self._log_histograms_this_batch:\n",
    "                self._histograms_to_tensorboard(batch_num_total, histogram_parameters)\n",
    "\n",
    "            if self._log_batch_size_period:\n",
    "                cur_batch = self._get_batch_size(batch)\n",
    "                cumulative_batch_size += cur_batch\n",
    "                if (batches_this_epoch - 1) % self._log_batch_size_period == 0:\n",
    "                    average = cumulative_batch_size/batches_this_epoch\n",
    "                    logger.info(f\"current batch size: {cur_batch} mean batch size: {average}\")\n",
    "                    self._tensorboard.add_train_scalar(\"current_batch_size\", cur_batch, batch_num_total)\n",
    "                    self._tensorboard.add_train_scalar(\"mean_batch_size\", average, batch_num_total)\n",
    "\n",
    "            # Save model if needed.\n",
    "            if self._model_save_interval is not None and (\n",
    "                    time.time() - last_save_time > self._model_save_interval\n",
    "            ):\n",
    "                last_save_time = time.time()\n",
    "                self._save_checkpoint(\n",
    "                        '{0}.{1}'.format(epoch, time_to_str(int(last_save_time))), [], is_best=False\n",
    "                )\n",
    "            # Save model if needed. (TODO: Move to callback)\n",
    "            if self._model_save_interval is not None and (\n",
    "                    time.time() - last_save_time > self._model_save_interval\n",
    "            ):\n",
    "                last_save_time = time.time()\n",
    "                self._save_checkpoint(\n",
    "                        '{0}.{1}'.format(epoch, time_to_str(int(last_save_time))), [], is_best=False\n",
    "                )\n",
    "            self.callbacks.on_batch_end()\n",
    "    \n",
    "        # END FOR\n",
    "        metrics = self._get_metrics(train_loss, batches_this_epoch, reset=True)\n",
    "        metrics['cpu_memory_MB'] = peak_cpu_usage\n",
    "        for (gpu_num, memory) in gpu_usage:\n",
    "            metrics['gpu_'+str(gpu_num)+'_memory_MB'] = memory\n",
    "        return metrics\n",
    "    \n",
    "    @overrides\n",
    "    def train(self):\n",
    "        self._state[\"Train success\"] = True\n",
    "        try:\n",
    "            self.callbacks.on_train_begin()\n",
    "            try:\n",
    "                epoch_counter, validation_metric_per_epoch = self._restore_checkpoint()\n",
    "            except RuntimeError:\n",
    "                traceback.print_exc()\n",
    "                raise ConfigurationError(\"Could not recover training from the checkpoint.  Did you mean to output to \"\n",
    "                                         \"a different serialization directory or delete the existing serialization \"\n",
    "                                         \"directory?\")\n",
    "\n",
    "            self._enable_gradient_clipping()\n",
    "            self._enable_activation_logging()\n",
    "\n",
    "            logger.info(\"Beginning training.\")\n",
    "\n",
    "            train_metrics: Dict[str, float] = {}\n",
    "            val_metrics: Dict[str, float] = {}\n",
    "            metrics: Dict[str, Any] = {}\n",
    "            epochs_trained = 0\n",
    "            training_start_time = time.time()\n",
    "\n",
    "            for epoch in range(epoch_counter, self._num_epochs):\n",
    "                epoch_start_time = time.time()\n",
    "                train_metrics = self._train_epoch(epoch)\n",
    "\n",
    "                if self._validation_data is not None:\n",
    "                    with torch.no_grad():\n",
    "                        # We have a validation set, so compute all the metrics on it.\n",
    "                        val_loss, num_batches = self._validation_loss()\n",
    "                        val_metrics = self._get_metrics(val_loss, num_batches, reset=True)\n",
    "\n",
    "                        # Check validation metric for early stopping\n",
    "                        this_epoch_val_metric = val_metrics[self._validation_metric]\n",
    "\n",
    "                        # Check validation metric to see if it's the best so far\n",
    "                        is_best_so_far = self._is_best_so_far(this_epoch_val_metric, validation_metric_per_epoch)\n",
    "                        validation_metric_per_epoch.append(this_epoch_val_metric)\n",
    "                        if self._should_stop_early(validation_metric_per_epoch):\n",
    "                            logger.info(\"Ran out of patience.  Stopping training.\")\n",
    "                            break\n",
    "\n",
    "                else:\n",
    "                    # No validation set, so just assume it's the best so far.\n",
    "                    is_best_so_far = True\n",
    "                    val_metrics = {}\n",
    "                    this_epoch_val_metric = None\n",
    "\n",
    "                self._metrics_to_tensorboard(epoch, train_metrics, val_metrics=val_metrics)\n",
    "                self._metrics_to_console(train_metrics, val_metrics)\n",
    "\n",
    "                # Create overall metrics dict\n",
    "                training_elapsed_time = time.time() - training_start_time\n",
    "                metrics[\"training_duration\"] = time.strftime(\"%H:%M:%S\", time.gmtime(training_elapsed_time))\n",
    "                metrics[\"training_start_epoch\"] = epoch_counter\n",
    "                metrics[\"training_epochs\"] = epochs_trained\n",
    "                metrics[\"epoch\"] = epoch\n",
    "\n",
    "                for key, value in train_metrics.items():\n",
    "                    metrics[\"training_\" + key] = value\n",
    "                for key, value in val_metrics.items():\n",
    "                    metrics[\"validation_\" + key] = value\n",
    "\n",
    "                if is_best_so_far:\n",
    "                    # Update all the best_ metrics.\n",
    "                    # (Otherwise they just stay the same as they were.)\n",
    "                    metrics['best_epoch'] = epoch\n",
    "                    for key, value in val_metrics.items():\n",
    "                        metrics[\"best_validation_\" + key] = value\n",
    "\n",
    "                if self._serialization_dir:\n",
    "                    dump_metrics(os.path.join(self._serialization_dir, f'metrics_epoch_{epoch}.json'), metrics)\n",
    "\n",
    "                if self._learning_rate_scheduler:\n",
    "                    # The LRScheduler API is agnostic to whether your schedule requires a validation metric -\n",
    "                    # if it doesn't, the validation metric passed here is ignored.\n",
    "                    self._learning_rate_scheduler.step(epoch)\n",
    "\n",
    "                self._save_checkpoint(epoch, validation_metric_per_epoch, is_best=is_best_so_far)\n",
    "\n",
    "                epoch_elapsed_time = time.time() - epoch_start_time\n",
    "                logger.info(\"Epoch duration: %s\", time.strftime(\"%H:%M:%S\", time.gmtime(epoch_elapsed_time)))\n",
    "\n",
    "                if epoch < self._num_epochs - 1:\n",
    "                    training_elapsed_time = time.time() - training_start_time\n",
    "                    estimated_time_remaining = training_elapsed_time * \\\n",
    "                        ((self._num_epochs - epoch_counter) / float(epoch - epoch_counter + 1) - 1)\n",
    "                    formatted_time = str(datetime.timedelta(seconds=int(estimated_time_remaining)))\n",
    "                    logger.info(\"Estimated training time remaining: %s\", formatted_time)\n",
    "\n",
    "                epochs_trained += 1\n",
    "\n",
    "            return metrics\n",
    "        except StopTraining as e:\n",
    "            self._state[\"Train success\"] = False\n",
    "            logger.error(f\"Training stopped due to exception: \\n{e}\")\n",
    "        finally: self.callbacks.on_train_end()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test performance when input is all 0s\n",
    "- If our initialization works decently, the loss should barely/not move and accuracy should stay constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.debugging:\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config.lr)\n",
    "    model.test_mode()\n",
    "    trainer = CustomTrainer(\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        iterator=iterator,\n",
    "        train_dataset=train_ds[:256],\n",
    "        cuda_device=0 if USE_GPU else -1,\n",
    "        num_epochs=5,\n",
    "    )\n",
    "    metrics = trainer.train()\n",
    "    model.load_state_dict(init_state_dict)\n",
    "    model.test_mode(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test performance on a small batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.debugging:\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config.lr)\n",
    "    state_dict = deepcopy(model.state_dict())\n",
    "    trainer = CustomTrainer(\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        iterator=iterator,\n",
    "        train_dataset=train_ds[:256],\n",
    "        cuda_device=0 if USE_GPU else -1,\n",
    "        num_epochs=50,\n",
    "    )\n",
    "    metrics = trainer.train()\n",
    "    model.load_state_dict(init_state_dict)\n",
    "    metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import math\n",
    "\n",
    "class LRFinder(Callback):\n",
    "    def __init__(self):\n",
    "        self.losses = []\n",
    "        self.lrs = []\n",
    "        self.best_loss = 1e9\n",
    "\n",
    "    def on_step_end(self, loss, **kwargs):\n",
    "        # Log the learning rate\n",
    "        self.losses.append(loss.item())\n",
    "        self.lrs.append(self.trainer.optimizer.state_dict()['param_groups'][0][\"lr\"])\n",
    "\n",
    "    def plot_loss(self, n_skip_beginning=10, n_skip_end=5):\n",
    "        plt.ylabel(\"loss\")\n",
    "        plt.xlabel(\"learning rate (log scale)\")\n",
    "        plt.plot(self.lrs[n_skip_beginning:-n_skip_end], self.losses[n_skip_beginning:-n_skip_end])\n",
    "        plt.xscale('log')\n",
    "\n",
    "    def plot_loss_change(self, sma=1, n_skip_beginning=10, n_skip_end=5, y_lim=(-0.01, 0.01)):\n",
    "        assert sma >= 1\n",
    "        derivatives = [0] * sma\n",
    "        for i in range(sma, len(self.lrs)):\n",
    "            derivative = (self.losses[i] - self.losses[i - sma]) / sma\n",
    "            derivatives.append(derivative)\n",
    "\n",
    "        plt.ylabel(\"rate of loss change\")\n",
    "        plt.xlabel(\"learning rate (log scale)\")\n",
    "        plt.plot(self.lrs[n_skip_beginning:-n_skip_end], derivatives[n_skip_beginning:-n_skip_end])\n",
    "        plt.xscale('log')\n",
    "        plt.ylim(y_lim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.find_lr:\n",
    "    from copy import deepcopy\n",
    "    \n",
    "    class ExponentialIncrease(torch.optim.lr_scheduler._LRScheduler):\n",
    "        def __init__(self, optimizer: torch.optim.Optimizer, n_iters: int,\n",
    "                     lr_start=1e-6, lr_end=2.0) -> None:\n",
    "            self.n_iters = n_iters\n",
    "            self.steps = 0\n",
    "            self.lr_start = lr_start\n",
    "            self.gamma = (lr_end / lr_start) ** (1 / n_iters)\n",
    "            super().__init__(optimizer)\n",
    "        def step(self, epoch=None): pass\n",
    "        def step_batch(self, epoch=None):\n",
    "            self.steps += 1\n",
    "            if epoch is None: epoch = self.last_epoch + 1\n",
    "            self.last_epoch = epoch\n",
    "            for param_group, learning_rate in zip(self.optimizer.param_groups, self.get_lr()):\n",
    "                param_group['lr'] = learning_rate\n",
    "        def get_lr(self):\n",
    "            return [self.lr_start * (self.gamma ** self.steps) for _ in self.base_lrs]\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-6)\n",
    "    lr_finder = LRFinder()\n",
    "    trainer = CustomTrainer(\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        iterator=iterator,\n",
    "        train_dataset=train_ds,\n",
    "        learning_rate_scheduler=ExponentialIncrease(optimizer, \n",
    "                                                    iterator.get_num_batches(train_ds)),\n",
    "        cuda_device=0 if USE_GPU else -1,\n",
    "        num_epochs=1,\n",
    "        callbacks=[lr_finder],\n",
    "    )\n",
    "    trainer.train()\n",
    "    model.load_state_dict(init_state_dict)\n",
    "    del model_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.find_lr:\n",
    "    lr_finder.plot_loss(n_skip_beginning=0, n_skip_end=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actual Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(init_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), \n",
    "                       lr=config.lr, weight_decay=config.weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _prod(args):\n",
    "    acc = 1\n",
    "    for a in args: acc *= a\n",
    "    return acc\n",
    "num_trainable_params = sum([_prod(p.shape) for p in model.parameters() if p.requires_grad])\n",
    "num_trainable_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.training.learning_rate_schedulers import SlantedTriangular, CosineWithRestarts\n",
    "if config.lr_schedule == \"slanted_triangular\":\n",
    "    lr_sched = SlantedTriangular(optimizer, \n",
    "                                 num_epochs=config.epochs, \n",
    "                                 num_steps_per_epoch=iterator.get_num_batches(train_ds))\n",
    "elif config.lr_scheduler == \"cosine_annealing\":\n",
    "    lr_sched = optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer, T_max=iterator.get_num_batches(train_ds) * config.epochs,\n",
    "    )\n",
    "elif config.lr_scheduler is None:\n",
    "    lr_sched = None\n",
    "else:\n",
    "    raise ConfigurationError(f\"Invalid lr schedule {config.lr_scheduler} passed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_options = {\n",
    "    # TODO: Add appropriate learning rate scheduler\n",
    "    \"should_log_parameter_statistics\": True,\n",
    "    \"should_log_learning_rate\": True,\n",
    "    \"num_epochs\": config.epochs,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (os.environ[\"IS_COLAB\"] != \"True\" and not config.testing):\n",
    "    SER_DIR = DATA_ROOT / \"ckpts\" / RUN_ID\n",
    "else:\n",
    "    SER_DIR = None\n",
    "\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    iterator=iterator,\n",
    "    train_dataset=train_ds + train_aug_ds if config.use_augmented else train_ds,\n",
    "    validation_dataset=val_ds if config.val_ratio > 0.0 else None,\n",
    "    serialization_dir=SER_DIR,\n",
    "    cuda_device=0 if USE_GPU else -1,\n",
    "    callbacks=[NanWeightMonitor(), \n",
    "               TensorboardCallback(),\n",
    "               Mixup(weight=config.mixup_ratio),\n",
    "               SlackNotification(silent=config.testing)],\n",
    "    learning_rate_scheduler=lr_sched,\n",
    "    **training_options,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import expit\n",
    "from collections import defaultdict\n",
    "\n",
    "def dict_append(d: Dict[str, List], upd: Dict[str, Any]) -> Dict[str, List]:\n",
    "    for k, v in upd.items(): d[k].append(v)\n",
    "\n",
    "def tonp(tsr): return tsr.detach().cpu().numpy()\n",
    "        \n",
    "class Predictor:\n",
    "    def __init__(self, model: Model, iterator: DataIterator,\n",
    "                 cuda_device: int=-1) -> None:\n",
    "        self.model = model\n",
    "        self.iterator = iterator\n",
    "        self.cuda_device = cuda_device\n",
    "        \n",
    "    def _extract_data(self, batch) -> Dict[str, np.ndarray]:\n",
    "        out_dict = self.model(**batch)\n",
    "        lens = tonp(get_text_field_mask(batch[\"tokens\"]).sum(1))\n",
    "        return {\n",
    "                \"preds\": expit(tonp(out_dict[\"class_logits\"])),\n",
    "                \"oov_ratio\": tonp((batch[\"tokens\"][\"tokens\"] == 1).sum(1)) / lens,\n",
    "                \"lens\": lens,\n",
    "               }\n",
    "        \n",
    "    def _postprocess(self, predictions: Dict[str, np.ndarray]) -> np.ndarray:\n",
    "        return {k: np.concatenate(v, axis=0) for k, v in predictions.items()}\n",
    "    \n",
    "    @gpu_mem_restore\n",
    "    def predict(self, ds: Iterable[Instance]) -> np.ndarray:\n",
    "        pred_generator = self.iterator(ds, num_epochs=1, shuffle=False)\n",
    "        self.model.eval()\n",
    "        pred_generator_tqdm = Tqdm.tqdm(pred_generator,\n",
    "                                        total=self.iterator.get_num_batches(ds))\n",
    "        preds = defaultdict(list)\n",
    "        with torch.no_grad():\n",
    "            for batch in pred_generator_tqdm:\n",
    "                batch = nn_util.move_to_device(batch, self.cuda_device)\n",
    "                dict_append(preds, self._extract_data(batch))\n",
    "        return self._postprocess(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.data.iterators import BasicIterator\n",
    "seq_iterator = BasicIterator(batch_size=64)\n",
    "seq_iterator.index_with(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Horrible solution to the shuffling problem with BasicIterator\n",
    "# TODO: Solve more elegantly?\n",
    "if not config.bucket:\n",
    "    del train_ds; import gc; gc.collect()\n",
    "    if config.val_ratio > 0.0:\n",
    "        train_ds = reader.read(DATA_ROOT / \"train_wo_val.csv\")\n",
    "    else:\n",
    "        train_ds = reader.read(DATA_ROOT / \"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = Predictor(model, seq_iterator, cuda_device=0 if USE_GPU else -1)\n",
    "train_meta = predictor.predict(train_ds) \n",
    "train_preds = train_meta.pop(\"preds\")\n",
    "test_meta = predictor.predict(test_ds)\n",
    "test_preds = test_meta.pop(\"preds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst_df = pd.read_csv(DATA_ROOT / \"test_proced.csv\")\n",
    "test_labels = tst_df[label_cols].values\n",
    "test_texts = tst_df[\"comment_text\"].values\n",
    "if config.testing:\n",
    "    test_labels = test_labels[:len(test_ds), :]\n",
    "    test_texts = test_texts[:len(test_ds)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, f1_score, accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluator:\n",
    "    def __init__(self, thres=0.5):\n",
    "        if isinstance(thres, float):\n",
    "            self.thres = np.ones(len(label_cols)) * thres\n",
    "        else:\n",
    "            self.thres = thres\n",
    "    \n",
    "    def _to_metric_dict(self, t: np.ndarray, y: np.ndarray, thres: float) -> Dict:\n",
    "        tn, fp, fn, tp = confusion_matrix(t, y >= thres).ravel()\n",
    "        return {\"auc\": roc_auc_score(t, y),\n",
    "                \"f1\": f1_score(t, y >= thres),\n",
    "                \"acc\": accuracy_score(t, y >= thres),\n",
    "                \"tnr\": tn / len(t), \"fpr\": fp / len(t),\n",
    "                \"fnr\": fn / len(t), \"tpr\": tp / len(t),\n",
    "                \"precision\": tp / (tp + fp), \"recall\": tp / (tp + fn),\n",
    "        }\n",
    "\n",
    "    def _stats_per_quadrant(self, tgt, preds, \n",
    "                            metadata: Dict[str, np.ndarray],\n",
    "                            texts: np.ndarray=None):\n",
    "        out_data = {}\n",
    "        for i, lbl in enumerate(label_cols):\n",
    "            # get indicies of each quadrant`\n",
    "            preds_bin = preds[:, i] >= self.thres[i]\n",
    "            quads = {\n",
    "                \"tp\": np.where((tgt[:, i] == 1) & preds_bin)[0],\n",
    "                \"fp\": np.where((tgt[:, i] == 0) & preds_bin)[0],\n",
    "                \"tn\": np.where((tgt[:, i] == 0) & ~preds_bin)[0],\n",
    "                \"fn\": np.where((tgt[:, i] == 1) & ~preds_bin)[0],\n",
    "            }\n",
    "            \n",
    "            # get stats for metadata\n",
    "            out_data[lbl] = {}\n",
    "            for quad, qidxs in quads.items():\n",
    "                quad_data = {}\n",
    "                for k, full_data in metadata.items():\n",
    "                    data = full_data[qidxs]\n",
    "                    for metric in [\"mean\", \"std\", \"min\", \"max\"]:\n",
    "                        if len(data) > 0:\n",
    "                            quad_data[f\"{k}_{metric}\"] = getattr(data, metric)()\n",
    "                        else:\n",
    "                            quad_data[f\"{k}_{metric}\"] = np.nan\n",
    "\n",
    "                out_data[lbl][quad] = quad_data\n",
    "            \n",
    "            # do error analysis\n",
    "            if texts is not None:\n",
    "                for quad, qidxs in quads.items():\n",
    "                    quad_preds = preds[qidxs, i]\n",
    "                    if len(quad_preds) == 0: continue\n",
    "                    if quad in [\"tp\", \"fp\"]:\n",
    "                        out_data[lbl][quad][\"most_confident\"] = texts[quad_preds.argmax()]\n",
    "                        out_data[lbl][quad][\"most_confident_prob\"] = quad_preds.max()\n",
    "                        out_data[lbl][quad][\"least_confident\"] = texts[quad_preds.argmin()]\n",
    "                        out_data[lbl][quad][\"least_confident_prob\"] = quad_preds.min()\n",
    "                    else:\n",
    "                        out_data[lbl][quad][\"most_confident\"] = texts[quad_preds.argmin()]\n",
    "                        out_data[lbl][quad][\"most_confident_prob\"] = quad_preds.min()\n",
    "                        out_data[lbl][quad][\"least_confident\"] = texts[quad_preds.argmax()]\n",
    "                        out_data[lbl][quad][\"least_confident_prob\"] = quad_preds.max()\n",
    "        return out_data        \n",
    "    \n",
    "    @gpu_mem_restore\n",
    "    def evaluate(self, tgt: np.ndarray, preds: np.ndarray,\n",
    "                 trn_tgt: np.ndarray, trn_preds: np.ndarray,\n",
    "                 metadata: Dict[str, np.ndarray]={}, \n",
    "                 texts: np.ndarray=None) -> Dict:\n",
    "        \"\"\"\n",
    "        Metadata: Data about the inputs (e.g. length, OOV ratio)\n",
    "        \"\"\"\n",
    "        train_label_metrics = {}\n",
    "        label_metrics = {}\n",
    "                \n",
    "        # get per-label stats\n",
    "        for i, lbl in enumerate(label_cols):\n",
    "            train_label_metrics[lbl] = self._to_metric_dict(trn_tgt[:, i],\n",
    "                                                            trn_preds[:, i],\n",
    "                                                            self.thres[i])\n",
    "            label_metrics[lbl] = self._to_metric_dict(tgt[:, i], preds[:, i],\n",
    "                                                      self.thres[i])\n",
    "            print(f\"========{lbl}=========\")\n",
    "            print(label_metrics[lbl])\n",
    "        \n",
    "        # get global stats\n",
    "        for mtrc in label_metrics[\"toxic\"].keys():\n",
    "            label_metrics[f\"global_{mtrc}\"] = \\\n",
    "                np.mean([label_metrics[col][mtrc] for col in label_cols])\n",
    "            \n",
    "        # get per-label-quadrant stats\n",
    "        quad_stats = self._stats_per_quadrant(tgt, preds, metadata=metadata, texts=texts)\n",
    "        if len(quad_stats) > 0:\n",
    "            for c in label_cols:\n",
    "                label_metrics[c][\"quad_stats\"] = quad_stats[c]\n",
    "\n",
    "        label_metrics[\"train\"] = train_label_metrics,\n",
    "        return label_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute best threshold based on training data\n",
    "if config.compute_thres_on_test:\n",
    "    lbls, pds = test_labels, test_preds\n",
    "else:\n",
    "    lbls, pds = train_labels, train_preds\n",
    "    \n",
    "thres = np.zeros(len(label_cols))\n",
    "best_scores = np.zeros(len(label_cols))\n",
    "for i, col in enumerate(label_cols):\n",
    "    best_score = -1\n",
    "    best_thres = -1\n",
    "    for x in np.linspace(0, 1.0, num=999):\n",
    "        scr = f1_score(lbls[:, i], pds[:, i] >= x)\n",
    "        if scr > best_score:\n",
    "            best_thres = x\n",
    "            best_score = scr\n",
    "    thres[i] = best_thres\n",
    "    best_scores[i] = best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = Evaluator(thres=thres)\n",
    "label_metrics = evaluator.evaluate(\n",
    "    test_labels, test_preds,\n",
    "    train_labels, train_preds,\n",
    "    metadata=test_meta, texts=test_texts,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_metrics[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Record results and save weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.environ[\"IS_COLAB\"] != \"True\":\n",
    "    import sys\n",
    "    sys.path.append(\"../lib\")\n",
    "    from record_experiments import record\n",
    "else:\n",
    "    PASSWORD = \"mongo11747\" # FILL IN IF COLAB\n",
    "\n",
    "    from typing import *\n",
    "    import pymongo\n",
    "    from bson.objectid import ObjectId\n",
    "    import os\n",
    "    import logging\n",
    "\n",
    "    # Logger\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logger.setLevel(logging.INFO)\n",
    "    ch = logging.StreamHandler()\n",
    "    ch.setLevel(logging.DEBUG)\n",
    "    formatter = logging.Formatter('[%(levelname)s] %(asctime)s - %(name)s %(message)s')\n",
    "    ch.setFormatter(formatter)\n",
    "    logger.addHandler(ch)\n",
    "\n",
    "    conn_str = f\"mongodb+srv://root:{PASSWORD}@cluster0-ptgoc.mongodb.net/test?retryWrites=true\"\n",
    "\n",
    "    client = pymongo.MongoClient(conn_str)\n",
    "    db = client.experiments\n",
    "    collection = db.logs\n",
    "\n",
    "    def _cln(v: Any) -> Any:\n",
    "        \"\"\"Ensure variables are serializable\"\"\"\n",
    "        if isinstance(v, (np.float, np.float16, np.float32, np.float64, np.float128)):\n",
    "            return float(v)\n",
    "        elif isinstance(v, (np.int, np.int0, np.int8, np.int16, np.int32, np.int64)):\n",
    "            return int(v)\n",
    "        elif isinstance(v, dict):\n",
    "            return {k: _cln(v_) for k, v_ in v.items()}\n",
    "        else:\n",
    "            return v\n",
    "\n",
    "    def record(log: dict):\n",
    "        res = collection.insert_one({str(k): _cln(v) for k, v in log.items()})\n",
    "        logger.info(f\"Inserted results at id {res.inserted_id}\")\n",
    "        return res\n",
    "\n",
    "    def find(id_: Optional[str]=None, query: Optional[dict]=None):\n",
    "        if query is None: query = {\"_id\": ObjectId(id_)}\n",
    "        res = collection.find_one(query)\n",
    "        return res\n",
    "\n",
    "    def delete(id_: Optional[str]=None, query: Optional[dict]=None):\n",
    "        if query is None: query = {\"_id\": ObjectId(id_)}\n",
    "        res = collection.delete_many(query)\n",
    "        logger.info(f\"Deleted {res.deleted_count} entries\")\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Record summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from pytz import timezone\n",
    "\n",
    "if not config.testing:\n",
    "    experiment_log = dict(config)\n",
    "    tz = timezone('EST')\n",
    "    experiment_log[\"execution_date\"] = datetime.now(tz).strftime(\"%Y-%m-%d %H:%M %Z\")\n",
    "    experiment_log.update(metrics)\n",
    "    experiment_log.update(label_metrics)\n",
    "    record(experiment_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
