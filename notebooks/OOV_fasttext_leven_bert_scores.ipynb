{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from pytorch_pretrained_bert.tokenization import BertTokenizer\n",
    "from pytorch_pretrained_bert.modeling import BertForMaskedLM\n",
    "\n",
    "#torch_device=torch.device('cuda')\n",
    "torch_device=torch.device('cpu')\n",
    "\n",
    "bert_model_mlm = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "bert_model_mlm.eval()\n",
    "bert_model_mlm.to(torch_device)\n",
    "\n",
    "for param in bert_model_mlm.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "bert_id2tok = dict()\n",
    "for tok, tok_id in bert_tokenizer.vocab.items():\n",
    "    bert_id2tok[tok_id] = tok\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "spacy_nlp=spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters\n",
    "\n",
    "MAX_BERT_LEN=256\n",
    "MAX_COSINE_DIST=0.3\n",
    "BERT_VOCAB_QTY=30000\n",
    "\n",
    "num_threads=8\n",
    "K=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "ft_compiled_path = \"../data/jigsaw/ft_model_bert_basic_tok.npy\" # Embeddings generated from the vocabulary\n",
    "fasttext_embeds = np.load(ft_compiled_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.data.tokenizers.word_splitter import SpacyWordSplitter\n",
    "from pytorch_pretrained_bert.tokenization import BasicTokenizer\n",
    "from allennlp.data.token_indexers import WordpieceIndexer, SingleIdTokenIndexer\n",
    "import re\n",
    "\n",
    "#_spacy_tok = SpacyWordSplitter(language='en_core_web_sm', pos_tags=False).split_words\n",
    "_bert_tok = BasicTokenizer(do_lower_case=True)\n",
    "\n",
    "spacy_tokenizer = SpacyWordSplitter(language='en_core_web_sm', pos_tags=False)\n",
    "\n",
    "from allennlp.data.token_indexers import SingleIdTokenIndexer\n",
    "token_indexer = SingleIdTokenIndexer(\n",
    "    lowercase_tokens=True,\n",
    ")\n",
    "\n",
    "from itertools import groupby\n",
    "\n",
    "def remove_url(s):\n",
    "    return re.sub(r\"http\\S+\", \"\", s)\n",
    "\n",
    "def remove_extra_chars(s, max_qty=2):\n",
    "    res = [c * min(max_qty, len(list(group_iter))) for c, group_iter in groupby(s)] \n",
    "    return ''.join(res)\n",
    "\n",
    "def tokenizer(x: str):\n",
    "    return [remove_extra_chars(w) for w in _bert_tok.tokenize(remove_url(x))]\n",
    "    #return [w.text for w in _spacy_tok(x.lower())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this', 'is', 'a', 'test', 'don', \"'\", 't', 'do', 'this', 'a', 'thome', '!']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"This is a test don't do this a thome!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"n't\" in bert_tokenizer.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30522"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bert_tokenizer.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[unused1]'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_id2tok[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14021"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_tokenizer.vocab['sh']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns arrays of arrays if there's an OOV word or an empty array instead\n",
    "# Each array element is a tuple: \n",
    "# position of OOV word (with respect to the original tokenizer), sent for BERT tokenizer\n",
    "def get_bert_masked_inputs(toks, bert_tokenizer):\n",
    "    res = []\n",
    "    \n",
    "    oov_pos = []\n",
    "    bert_vocab = bert_tokenizer.vocab\n",
    "    \n",
    "    for i in range(len(toks)):\n",
    "        if toks[i] not in bert_vocab:\n",
    "            oov_pos.append(i)\n",
    "            \n",
    "\n",
    "    for pos in oov_pos:\n",
    "        res.append( (pos, '[CLS] %s [MASK] %s [SEP]' % \n",
    "                     (' '.join(toks[0:pos]), ' '.join(toks[pos+1:])) ) )\n",
    "        \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_bert_masked_inputs(tokenizer('This is a *strangge* sentence.'), bert_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'det',\n",
       " '##ete',\n",
       " 'what',\n",
       " 'the',\n",
       " '[MASK]',\n",
       " 'are',\n",
       " 'you',\n",
       " 'doing',\n",
       " 'here',\n",
       " '?',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toks = bert_tokenizer.tokenize('[CLS] detete what the [MASK] are you doing here ? [SEP]')\n",
    "toks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[, CLS, ], what, the, [, MASK, ], are, you, do, n't, here, ?, [, SEP, ]]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toks = spacy_tokenizer.split_words(\"[CLS] what the [MASK] are you don't here ? [SEP]\")\n",
    "toks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[', 'CLS', ']', 'what', 'the', '[', 'MASK', ']', 'are', 'you', 'do', \"n't\", 'here', 'sh#t', 'fcuk', '?', '[', 'SEP', ']']\n"
     ]
    }
   ],
   "source": [
    "doc = spacy_nlp(\"[CLS] what the [MASK] are you don't here sh#t fcuk? [SEP]\")\n",
    "print([token.text for token in doc])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##\n",
      "##\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "qty=0\n",
    "for w in spacy_nlp.vocab:\n",
    "    if w.text.find('\\n') >=0:\n",
    "        print('##%s##' % w.text)\n",
    "        qty +=1\n",
    "print(qty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57852"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(spacy_nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['don',\n",
       " \"'\",\n",
       " 't',\n",
       " 'couldn',\n",
       " \"'\",\n",
       " 't',\n",
       " 'can',\n",
       " \"'\",\n",
       " 't',\n",
       " 'you',\n",
       " \"'\",\n",
       " 're',\n",
       " 'i',\n",
       " \"'\",\n",
       " 'm',\n",
       " 'sheet']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"don't  couldn't can't you're I'm sheeeet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['don',\n",
       " \"'\",\n",
       " 't',\n",
       " 'couldn',\n",
       " \"'\",\n",
       " 't',\n",
       " 'can',\n",
       " \"'\",\n",
       " 't',\n",
       " 'you',\n",
       " \"'\",\n",
       " 're',\n",
       " 'i',\n",
       " \"'\",\n",
       " 'm',\n",
       " 'fc',\n",
       " '##uk']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_tokenizer.tokenize(\"don't  couldn't can't you're I'm fcuk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tess', '##t']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_tokenizer.tokenize(\"tesst\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['you',\n",
       " \"'\",\n",
       " 're',\n",
       " 'right',\n",
       " '.',\n",
       " 'it',\n",
       " \"'\",\n",
       " 's',\n",
       " 'a',\n",
       " 'miracle',\n",
       " '!',\n",
       " 'you',\n",
       " \"'\",\n",
       " 'd',\n",
       " 'been',\n",
       " 'deceived',\n",
       " '!']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"You ' re right. It ' s a miracle! You'd been deceived!\") # 've', 're', 's', 'd', 'll'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "# pos_oov is OOV index with respect to the original (not BERT) tokenizer!!!\n",
    "UtterData = namedtuple('SentData', ['batch_sent_id', 'sent_pos_oov', 'bert_pos_oov', 'tok_ids', 'oov_token'])\n",
    "\n",
    "def get_batch_data(torch_device, bert_tokenizer, sent_list, max_len=MAX_BERT_LEN):\n",
    "    \n",
    "    batch_data_raw = []\n",
    "    batch_max_seq_qty = 0\n",
    "    batch_sent_id = -1\n",
    "    for sent_toks in sent_list:\n",
    "        batch_sent_id += 1\n",
    "        for sent_pos_oov, text in get_bert_masked_inputs(sent_toks, bert_tokenizer):\n",
    "            # To accurately get what is the position of [MASK] according\n",
    "            # to BERT tokenizer, we need to re-tokenize the sentence using\n",
    "            # the BERT tokenizer\n",
    "            all_bert_toks = bert_tokenizer.tokenize(text)\n",
    "            bert_toks = all_bert_toks[0:max_len] # 512 is the max. Bert seq. length\n",
    "\n",
    "            tok_ids = bert_tokenizer.convert_tokens_to_ids(bert_toks)\n",
    "            bert_pos_oov = None\n",
    "            for i in range(len(bert_toks)):\n",
    "                if bert_toks[i] == '[MASK]':\n",
    "                    bert_pos_oov = i\n",
    "                    break\n",
    "            assert(bert_pos_oov is not None or len(all_bert_toks) > max_len)\n",
    "            if bert_pos_oov is not None:\n",
    "                tok_qty = len(tok_ids)\n",
    "                batch_max_seq_qty = max(batch_max_seq_qty, tok_qty)\n",
    "                batch_data_raw.append( \n",
    "                    UtterData(batch_sent_id=batch_sent_id, \n",
    "                              sent_pos_oov=sent_pos_oov, \n",
    "                              bert_pos_oov=bert_pos_oov,\n",
    "                              tok_ids=tok_ids, \n",
    "                              oov_token=sent_toks[sent_pos_oov]))\n",
    "            \n",
    "    batch_qty = len(batch_data_raw)\n",
    "    tok_ids_batch = np.zeros( (batch_qty, batch_max_seq_qty), dtype=np.int64) # zero is a padding symbol\n",
    "    tok_mask_batch = np.zeros( (batch_qty, batch_max_seq_qty), dtype=np.int64)\n",
    "    for k in range(batch_qty):\n",
    "        tok_ids = batch_data_raw[k].tok_ids\n",
    "        tok_qty = len(tok_ids)\n",
    "        tok_ids_batch[k, 0:tok_qty] = tok_ids\n",
    "        tok_mask_batch[k, 0:tok_qty] = np.ones(tok_qty)\n",
    "                   \n",
    "    tok_ids_batch = torch.from_numpy(tok_ids_batch).to(device=torch_device) \n",
    "    \n",
    "    return batch_data_raw, tok_ids_batch, tok_mask_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from collections import namedtuple\n",
    "import sys\n",
    "\n",
    "BertPredProbs = namedtuple('BertPred', ['batch_sent_id', 'sent_pos_oov', 'bert_pos_oov', 'logits'])\n",
    "\n",
    "def get_bert_preds_for_words_batch(torch_device, bert_model_mlm, \n",
    "                                   batch_data_raw, tok_ids_batch, tok_mask_batch, # comes from get_batch_data\n",
    "                                   word_ids # a list of IDS for which we generate logits\n",
    "                                   ):\n",
    "\n",
    "    seg_ids = torch.zeros_like(tok_ids_batch, device=torch_device)\n",
    "    \n",
    "    batch_qty = len(batch_data_raw)\n",
    "    \n",
    "    # Main BERT model see modeling.py in https://github.com/huggingface/pytorch-pretrained-BERT\n",
    "    bert = bert_model_mlm.bert \n",
    "    # cls is an instance of BertOnlyMLMHead (see https://github.com/huggingface/pytorch-pretrained-BERT)\n",
    "    cls = bert_model_mlm.cls\n",
    "    # predictions are of the type BertLMPredictionHead (see https://github.com/huggingface/pytorch-pretrained-BERT)\n",
    "    predictions = cls.predictions\n",
    "    transform = predictions.transform\n",
    "   \n",
    "    # We don't use the complete decoding matrix, but only selected rows\n",
    "    word_ids = torch.from_numpy(np.array(word_ids, dtype=np.int64)).to(device=torch_device)\n",
    "    tok_mask_batch = torch.from_numpy(np.array(tok_mask_batch, dtype=np.int64)).to(device=torch_device)\n",
    "                                \n",
    "    weight = predictions.decoder.weight[word_ids,:]\n",
    "    bias = predictions.bias[word_ids]\n",
    "\n",
    "    # Transformations from the main BERT model\n",
    "    sequence_output, _= bert(tok_ids_batch, seg_ids, attention_mask=tok_mask_batch, output_all_encoded_layers=False)\n",
    "    # Transformations from the BertLMPredictionHead model with the restricted last layer\n",
    "    hidden_states = transform(sequence_output)    \n",
    "    logits = torch.nn.functional.linear(hidden_states, weight) + bias                            \n",
    "                                        \n",
    "    logits=logits.detach().cpu().numpy()\n",
    "    \n",
    "    res = []\n",
    "    \n",
    "    for k in range(batch_qty):\n",
    "        e = batch_data_raw[k]\n",
    "        bert_pos_oov = e.bert_pos_oov         \n",
    "        res.append( BertPredProbs(batch_sent_id = batch_data_raw[k].batch_sent_id,\n",
    "                                  bert_pos_oov = bert_pos_oov,\n",
    "                                  sent_pos_oov = e.sent_pos_oov,\n",
    "                                  logits = logits[k, bert_pos_oov]\n",
    "                            ) \n",
    "                  )\n",
    "        \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3109,\n",
       " 6616,\n",
       " 17752,\n",
       " 6548,\n",
       " 4485,\n",
       " 2725,\n",
       " 24341,\n",
       " 4147,\n",
       " 2437,\n",
       " 3241,\n",
       " 2035,\n",
       " 15540,\n",
       " 2102]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_tokenizer.convert_tokens_to_ids(['hell', 'fuck', 'heck', 'devil', 'shit', 'doing', \n",
    "                                      'doin', 'wearing', 'making', 'thinking', 'all', 'tess', '##t'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['shi', '##ii', '##t', 'shit', 'crap', 'fl', '##lly', '##y', '##y']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_tokenizer.tokenize(\"shiiit shit crap flllyyy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentData(batch_sent_id=0, sent_pos_oov=3, bert_pos_oov=4, tok_ids=[101, 2017, 2024, 2074, 103, 2437, 2039, 11895, 6137, 2102, 2006, 1996, 13109, 9215, 2100, 2100, 1012, 102], oov_token='flailing')\n",
      "SentData(batch_sent_id=0, sent_pos_oov=6, bert_pos_oov=8, tok_ids=[101, 2017, 2024, 2074, 13109, 29544, 2437, 2039, 103, 2006, 1996, 13109, 9215, 2100, 2100, 1012, 102], oov_token='shiiit')\n",
      "SentData(batch_sent_id=0, sent_pos_oov=9, bert_pos_oov=13, tok_ids=[101, 2017, 2024, 2074, 13109, 29544, 2437, 2039, 11895, 6137, 2102, 2006, 1996, 103, 1012, 102], oov_token='flllyyy')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[BertPred(batch_sent_id=0, sent_pos_oov=3, bert_pos_oov=4, logits=array([-2.2550676 , -6.9771633 , -2.9452157 , -6.796351  , -5.1999493 ,\n",
       "        -0.16598895, -1.003738  , -3.052899  , -2.2550676 , -1.2968991 ,\n",
       "        -0.26938516, -0.26938516, -2.8959308 ], dtype=float32)),\n",
       " BertPred(batch_sent_id=0, sent_pos_oov=6, bert_pos_oov=8, logits=array([-0.14233172, -5.738283  , -2.9242346 , -5.991565  , -7.521471  ,\n",
       "        -1.7374325 ,  4.7143626 ,  2.8124328 , -0.14233172, -3.4299695 ,\n",
       "        -0.6083918 , -0.6083918 , -0.99990255], dtype=float32)),\n",
       " BertPred(batch_sent_id=0, sent_pos_oov=9, bert_pos_oov=13, logits=array([-2.0155    , -2.015716  , -1.2033051 , -1.7736413 , -2.8350475 ,\n",
       "        -1.4686524 ,  2.0290556 ,  0.31409857, -2.0155    , -1.7462821 ,\n",
       "        -1.1763945 , -1.1763945 ,  4.2806478 ], dtype=float32))]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_list = ['you are just flailing making up shiiit on the flllyyy .'.split()]\n",
    "\n",
    "batch_data_raw, tok_ids_batch, tok_mask_batch = get_batch_data(torch_device, \n",
    "                                                bert_tokenizer, \n",
    "                                                sent_list,\n",
    "                                                MAX_BERT_LEN)\n",
    "\n",
    "for s in batch_data_raw:\n",
    "    print(s)\n",
    "\n",
    "\n",
    "bert_ids_to_make_pred = bert_tokenizer.convert_tokens_to_ids(\n",
    "    ['fl', '##ailing', 'flaming', 'shi', '##ii', '##t', 'shit', 'crap', 'fl', '##lly', '##y', '##y', 'fly'])    \n",
    "    \n",
    "get_bert_preds_for_words_batch(torch_device,\n",
    "                               bert_model_mlm, \n",
    "                               batch_data_raw, tok_ids_batch, tok_mask_batch,\n",
    "                               bert_ids_to_make_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.mean([-1.6624606, -11.621088 ,  -3.8026168])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentData(batch_sent_id=0, sent_pos_oov=2, bert_pos_oov=3, tok_ids=[101, 2054, 1996, 103, 2024, 2017, 2725, 2290, 2182, 1029, 102], oov_token='fcuk')\n",
      "SentData(batch_sent_id=0, sent_pos_oov=5, bert_pos_oov=7, tok_ids=[101, 2054, 1996, 4429, 6968, 2024, 2017, 103, 2182, 1029, 102], oov_token='doingg')\n",
      "SentData(batch_sent_id=1, sent_pos_oov=3, bert_pos_oov=4, tok_ids=[101, 2023, 2003, 1037, 103, 6251, 102], oov_token='*strangge*')\n",
      "SentData(batch_sent_id=2, sent_pos_oov=3, bert_pos_oov=4, tok_ids=[101, 2023, 2003, 1037, 103, 102], oov_token='bbbbbbest')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[BertPred(batch_sent_id=0, sent_pos_oov=2, bert_pos_oov=3, logits=array([ 1.5889928e+01,  1.3050140e+01,  1.1907359e+01,  1.0735191e+01,\n",
       "         7.7608805e+00,  1.1303816e+00, -3.6249363e-01, -1.2756933e+00,\n",
       "         1.4766751e-02, -2.3537908e-01, -2.4631200e+00,  4.4421670e-01,\n",
       "        -9.1704965e-01], dtype=float32)),\n",
       " BertPred(batch_sent_id=0, sent_pos_oov=5, bert_pos_oov=7, logits=array([-6.5866718e+00, -3.7194700e+00, -4.6205916e+00, -7.7321978e+00,\n",
       "        -1.2913905e-03,  2.0477596e+01,  1.1890603e+01,  9.2444258e+00,\n",
       "        -9.0952718e-01, -5.5600286e+00, -4.1842346e+00, -7.5178404e+00,\n",
       "        -1.5601634e+00], dtype=float32)),\n",
       " BertPred(batch_sent_id=1, sent_pos_oov=3, bert_pos_oov=4, logits=array([-2.5699182 , -2.195084  , -1.8034788 , -4.066658  , -0.28844202,\n",
       "        -0.7897885 , -0.93698156, -0.15000674,  1.9507252 ,  0.99364173,\n",
       "         3.9929423 , -1.4381317 , -1.2424338 ], dtype=float32)),\n",
       " BertPred(batch_sent_id=2, sent_pos_oov=3, bert_pos_oov=4, logits=array([-3.1645226 , -2.6828816 , -4.205613  , -5.619283  , -0.73749506,\n",
       "        -0.76936173, -1.1861775 , -1.6763517 , -1.0727652 , -2.7078848 ,\n",
       "         1.7696402 , -3.3062441 , -2.5123472 ], dtype=float32))]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_list = ['What the fcuk are you doingg here ?'.lower().split(),\n",
    "             'This is a *strangge* sentence'.lower().split(),\n",
    "             'This is a bbbbbbest'.lower().split()]\n",
    "\n",
    "batch_data_raw, tok_ids_batch, tok_mask_batch = get_batch_data(torch_device, \n",
    "                                                bert_tokenizer, \n",
    "                                                sent_list,\n",
    "                                                MAX_BERT_LEN)\n",
    "\n",
    "for s in batch_data_raw:\n",
    "    print(s)\n",
    "\n",
    "\n",
    "bert_ids_to_make_pred = bert_tokenizer.convert_tokens_to_ids(\n",
    "    ['hell', 'fuck', 'heck', 'devil', 'shit', 'doing', 'doin', \n",
    "     'thinking', 'strange', 'rest', 'test', 'tess', '##t'])    \n",
    "    \n",
    "get_bert_preds_for_words_batch(torch_device,\n",
    "                               bert_model_mlm, \n",
    "                               batch_data_raw, tok_ids_batch, tok_mask_batch,\n",
    "                               bert_ids_to_make_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bert_model_mlm.to('cpu')\n",
    "#torch.zeros(3,device=torch.device(\"cuda\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['б']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_tokenizer.tokenize('б')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import *\n",
    "from overrides import overrides\n",
    "import allennlp\n",
    "from allennlp.data.vocabulary import Vocabulary\n",
    "from allennlp.data.dataset_readers import DatasetReader\n",
    "\n",
    "from allennlp.data import Instance\n",
    "from allennlp.data.token_indexers import TokenIndexer, SingleIdTokenIndexer\n",
    "from allennlp.data.tokenizers import Token\n",
    "\n",
    "from allennlp.data.tokenizers.word_splitter import SpacyWordSplitter\n",
    "from allennlp.data.token_indexers import WordpieceIndexer, SingleIdTokenIndexer\n",
    "from allennlp.data.fields import TextField, SequenceLabelField, LabelField, MetadataField, ArrayField\n",
    "class MemoryOptimizedTextField(TextField):\n",
    "    @overrides\n",
    "    def __init__(self, tokens: List[str], token_indexers: Dict[str, TokenIndexer]) -> None:\n",
    "        self.tokens = tokens\n",
    "        self._token_indexers = token_indexers\n",
    "        self._indexed_tokens: Optional[Dict[str, TokenList]] = None\n",
    "        self._indexer_name_to_indexed_token: Optional[Dict[str, List[str]]] = None\n",
    "        # skip checks for tokens\n",
    "    @overrides\n",
    "    def index(self, vocab):\n",
    "        super().index(vocab)\n",
    "        self.tokens = None # empty tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_lowercase(s):\n",
    "  return sum([int(c == c.lower()) for c in s])\n",
    "\n",
    "\n",
    "def get_canon_case_map(nlp):\n",
    "  dict_tmp = dict()\n",
    "\n",
    "  for t in nlp.vocab:\n",
    "    dict_tmp[t.text.lower()] = set()\n",
    "\n",
    "  for t in nlp.vocab:\n",
    "    dict_tmp[t.text.lower()].add(t.text)\n",
    "\n",
    "  dict_map = dict()\n",
    "\n",
    "  for key, tset in dict_tmp.items():\n",
    "    lst = [(count_lowercase(s), s) for s in tset]\n",
    "    lst.sort(reverse=True)\n",
    "    choice_str = lst[0][1]\n",
    "    dict_map[key] = choice_str\n",
    "\n",
    "  return dict_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# returned word list can contain upper-case letters\n",
    "def read_embeds_and_words_subset(file_name, word_map):\n",
    "  word_list, embed_list = [], []\n",
    "\n",
    "  with open(file_name, encoding=\"utf8\") as f:\n",
    "    for line in f:\n",
    "      line = line.strip()\n",
    "      if not line:\n",
    "        continue\n",
    "      fld = line.split()\n",
    "      w = fld[0]\n",
    "      if w in word_map:\n",
    "        word_list.append(word_map[w])\n",
    "        embed_list.append(np.array([float(s) for s in fld[1:]]))\n",
    "\n",
    "  return word_list, np.vstack(embed_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_nlp = spacy.load(\"en_core_web_sm\",\n",
    "                     disable=['parser', 'ner', 'pos'])\n",
    "\n",
    "spacy_word_map = get_canon_case_map(spacy_nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[This, have, n't, true, .]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(spacy_nlp(\"This haven't true.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " \"shiit\" in spacy_nlp.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 49601 spacy words from the fasttext-dictionary file\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "THIS FUNCTION IS WRONG, IT NEEDS TO CREATE GLOBAL WORD->EMBED MAPS, but later we need to index\n",
    "only SPACY embeds.\n",
    "\n",
    "word_arr, embed_arr = read_embeds_and_words_subset(os.path.join('../data/jigsaw/', 'ft_basic_toks.txt'), \n",
    "                                                   spacy_word_map)\n",
    "print('Read %d spacy words from the fasttext-dictionary file' % len(word_arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_embed_map = { word_arr[i].lower() : embed_arr[i] for i in range(len(word_arr)) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'shiit'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-71-013c0a258279>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m'gillette'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mspacy_word_map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mword_to_embed_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'shiit'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: 'shiit'"
     ]
    }
   ],
   "source": [
    "'gillette' in spacy_word_map\n",
    "word_to_embed_map['shiit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nmslib, time\n",
    "\n",
    "def create_embed_index(embeds, M = 30, efC = 200, efS = 200):\n",
    "  num_threads = 0\n",
    "  index_time_params = {'M': M, 'indexThreadQty': num_threads, 'efConstruction': efC, 'post': 0}\n",
    "  print('Index-time parameters', index_time_params)\n",
    "\n",
    "  # Space name should correspond to the space name\n",
    "  # used for brute-force search\n",
    "  space_name = 'cosinesimil'\n",
    "\n",
    "  # Intitialize the library, specify the space, the type of the vector and add data points\n",
    "  index = nmslib.init(method='hnsw', space=space_name, data_type=nmslib.DataType.DENSE_VECTOR)\n",
    "  index.addDataPointBatch(embeds)\n",
    "\n",
    "  # Create an index\n",
    "  start = time.time()\n",
    "  index_time_params = {'M': M, 'indexThreadQty': num_threads, 'efConstruction': efC}\n",
    "  index.createIndex(index_time_params)\n",
    "  end = time.time()\n",
    "  print('Index-time parameters', index_time_params)\n",
    "  print('Indexing time = %f' % (end - start))\n",
    "\n",
    "  # Setting query-time parameters\n",
    "  query_time_params = {'efSearch': efS}\n",
    "  print('Setting query-time parameters', query_time_params)\n",
    "  index.setQueryTimeParams(query_time_params)\n",
    "\n",
    "  return index\n",
    "\n",
    "\n",
    "def create_word_index(words, M = 30, efC = 200, efS = 200):\n",
    "  num_threads = 0\n",
    "  index_time_params = {'M': M, 'indexThreadQty': num_threads, 'efConstruction': efC, 'post': 0}\n",
    "  print('Index-time parameters', index_time_params)\n",
    "\n",
    "  # Space name should correspond to the space name\n",
    "  # used for brute-force search\n",
    "  space_name = 'leven'\n",
    "\n",
    "  # Intitialize the library, specify the space, the type of the vector and add data points\n",
    "  index = nmslib.init(method='hnsw', space=space_name, dtype=nmslib.DistType.INT, data_type=nmslib.DataType.OBJECT_AS_STRING)\n",
    "  index.addDataPointBatch(list(words))\n",
    "\n",
    "  # Create an index\n",
    "  start = time.time()\n",
    "  index_time_params = {'M': M, 'indexThreadQty': num_threads, 'efConstruction': efC}\n",
    "  index.createIndex(index_time_params)\n",
    "  end = time.time()\n",
    "  print('Index-time parameters', index_time_params)\n",
    "  print('Indexing time = %f' % (end - start))\n",
    "\n",
    "  # Setting query-time parameters\n",
    "  query_time_params = {'efSearch': efS}\n",
    "  print('Setting query-time parameters', query_time_params)\n",
    "  index.setQueryTimeParams(query_time_params)\n",
    "\n",
    "  return index\n",
    "\n",
    "def query_index(index, K, query_arr, num_threads=0, efS=200):\n",
    "  # Querying\n",
    "  query_time_params = {'efSearch': efS}\n",
    "  print('Setting query-time parameters', query_time_params)\n",
    "  index.setQueryTimeParams(query_time_params)\n",
    "\n",
    "  query_qty = len(query_arr)\n",
    "  start = time.time()\n",
    "  res = index.knnQueryBatch(query_arr, k=K, num_threads=num_threads)\n",
    "  end = time.time()\n",
    "  print('kNN time total=%f (sec), per query=%f (sec), per query adjusted for thread number=%f (sec)' %\n",
    "        (end - start, float(end - start) / query_qty, num_threads * float(end - start) / query_qty))\n",
    "\n",
    "  return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "SURFACE_SIMIL = 'surface'\n",
    "GLOBAL_SIMIL = 'global'\n",
    "CONTEXT_SIMIL = 'context'\n",
    "\n",
    "def add_pooled_res(res, word, simil, res_key):\n",
    "    if not word in res:\n",
    "        res[word] = dict()\n",
    "    res[word][res_key] = simil\n",
    "\n",
    "\n",
    "def get_pooled_neighbors(embed_index, word_index, \n",
    "                     word_arr, word_to_embed_map,\n",
    "                     K, query_word, num_threads=0, efS=200):\n",
    "    tmp_res = dict()\n",
    "\n",
    "    ids_word, dists = query_index(word_index, 2*K, [query_word], num_threads=num_threads, efS=efS)[0]\n",
    "    qty_word = len(ids)\n",
    "    for i in range(qty_word):\n",
    "        w_res = word_arr[ids_word[i]]\n",
    "        max_len = float(max(len(w_res), len(query_word)))\n",
    "        simil = 1 - float(dists[i])/max_len\n",
    "        add_pooled_res(tmp_res, w_res, simil, SURFACE_SIMIL)\n",
    "        \n",
    "    ids_embed, dists = query_index(embed_index, 2*K, np.array([word_to_embed_map[query_word]]), \n",
    "                                  num_threads=num_threads, efS=efS)[0]\n",
    "    qty_embed = len(ids)\n",
    "    for i in range(qty_embed):\n",
    "        w_res = word_arr[ids_embed[i]]\n",
    "        simil = 1 - 0.5 * dists[i]\n",
    "        add_pooled_res(tmp_res, w_res, simil, GLOBAL_SIMIL)\n",
    "    \n",
    "    final_res = dict()\n",
    "    \n",
    "    i_word = 0\n",
    "    i_embed = 0\n",
    "    while (i_word < qty_word or i_embed < qty_embed) and len(final_res) < K:\n",
    "        if i_word < qty_word:\n",
    "            w_res = word_arr[ids_word[i_word]]\n",
    "            if not w_res in seen:\n",
    "                final_res[w_res] = tmp_res[w_res]                \n",
    "            i_word += 1\n",
    "        if i_embed < qty_embed:\n",
    "            w_res = word_arr[ids_embed[i_embed]]\n",
    "            if not w_res in seen:\n",
    "                final_res[w_res] = tmp_res[w_res] \n",
    "            i_embed += 1\n",
    "    \n",
    "    return final_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.data.fields import  ArrayField, MetadataField, TextField\n",
    "from allennlp.data.token_indexers import TokenIndexer\n",
    "from overrides import overrides\n",
    "\n",
    "class MemoryOptimizedTextField(TextField):\n",
    "\n",
    "  @overrides\n",
    "  def __init__(self, tokens: List[str], token_indexers: Dict[str, TokenIndexer]) -> None:\n",
    "    self.tokens = tokens\n",
    "    self._token_indexers = token_indexers\n",
    "    self._indexed_tokens: Optional[Dict[str, TokenList]] = None\n",
    "    self._indexer_name_to_indexed_token: Optional[Dict[str, List[str]]] = None\n",
    "    # skip checks for tokens\n",
    "\n",
    "  @overrides\n",
    "  def index(self, vocab):\n",
    "    super().index(vocab)\n",
    "    self.tokens = None  # empty tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import *\n",
    "from allennlp.data import Instance\n",
    "from allennlp.data.fields import  ArrayField, MetadataField, TextField\n",
    "\n",
    "#data_vocab_path = \"../data/jigsaw/data_vocab.bin\"\n",
    "#vocab=pickle.load(open(data_vocab_path,'rb'))\n",
    "\n",
    "def get_spacy_vocab_instances(nlp) -> Iterator[Instance]:\n",
    "  words = set([t.text.lower().strip() for t in nlp.vocab])\n",
    "\n",
    "  fields = {}\n",
    "\n",
    "  for w in words:\n",
    "    w = w.strip()\n",
    "    if w and w.find('\\n') < 0:\n",
    "      fields[\"tokens\"] = MemoryOptimizedTextField([w], {\"tokens\": SingleIdTokenIndexer()})\n",
    "      yield Instance(fields)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "49584it [00:00, 68003.04it/s]\n"
     ]
    }
   ],
   "source": [
    "spacy_word_arr = []\n",
    "spacy_embed_arr = []\n",
    "\n",
    "from allennlp.data.vocabulary import Vocabulary\n",
    "\n",
    "vocab = Vocabulary.from_instances(get_spacy_vocab_instances(spacy_nlp))\n",
    "\n",
    "for idx, token in vocab.get_index_to_token_vocabulary().items():\n",
    "    if idx > 1:\n",
    "        spacy_word_arr.append(token)\n",
    "        spacy_embed_arr.append(word_to_embed_map[token])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@@PADDING@@\n",
      "@@UNKNOWN@@\n",
      "arbitration\n",
      "actuary\n",
      "recalculated\n",
      "59\n"
     ]
    }
   ],
   "source": [
    "for idx, token in vocab.get_index_to_token_vocabulary().items():\n",
    "    if idx > 5: break\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index-time parameters {'M': 30, 'indexThreadQty': 0, 'efConstruction': 200, 'post': 0}\n",
      "Index-time parameters {'M': 30, 'indexThreadQty': 0, 'efConstruction': 200}\n",
      "Indexing time = 15.615927\n",
      "Setting query-time parameters {'efSearch': 200}\n"
     ]
    }
   ],
   "source": [
    "embed_index = create_embed_index(embed_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index-time parameters {'M': 30, 'indexThreadQty': 0, 'efConstruction': 200, 'post': 0}\n",
      "Index-time parameters {'M': 30, 'indexThreadQty': 0, 'efConstruction': 200}\n",
      "Indexing time = 11.862540\n",
      "Setting query-time parameters {'efSearch': 200}\n"
     ]
    }
   ],
   "source": [
    "word_index = create_word_index(word_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting query-time parameters {'efSearch': 200}\n",
      "kNN time total=0.003677 (sec), per query=0.003677 (sec), per query adjusted for thread number=0.000000 (sec)\n"
     ]
    }
   ],
   "source": [
    "res = query_index(word_index, 10, ['shiiit'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(array([ 2304,  5072,  6422, 23011,   320, 32017, 36871, 40168, 40247,\n",
       "         40869], dtype=int32),\n",
       "  array([2, 2, 2, 2, 2, 3, 3, 3, 3, 3], dtype=int32))]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spirit 2\n",
      "shift 2\n",
      "shirt 2\n",
      "shrift 2\n",
      "shit 2\n",
      "chitin 3\n",
      "Zhiyi 3\n",
      "Zhimin 3\n",
      "Bailit 3\n",
      "Zhilin 3\n"
     ]
    }
   ],
   "source": [
    "for ids, dists in res:\n",
    "    qty = len(ids)\n",
    "    for i in range(qty):\n",
    "        print(word_arr[ids[i]], dists[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'shiiit'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-eb7620e15495>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membed_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword_to_embed_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'shiiit'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: 'shiiit'"
     ]
    }
   ],
   "source": [
    "res = query_index(embed_index, 10, np.array([word_to_embed_map['shiit']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ids, dists in res:\n",
    "    qty = len(ids)\n",
    "    for i in range(qty):\n",
    "        print(word_arr[ids[i]], dists[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file = \"../data/jigsaw/test_basic.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, gc\n",
    "import copy\n",
    "from scipy.special import softmax\n",
    "\n",
    "\n",
    "DEBUG_PRINT=False\n",
    "\n",
    "\n",
    "all_src_sents = []\n",
    "\n",
    "with open(test_file) as f\n",
    "  for line in f:\n",
    "    obj = json.loads(line)\n",
    "    all_src_sents.append(obj[\"tokens\"])\n",
    "    \n",
    "t0 = time.time()\n",
    "preds = []\n",
    "\n",
    "all_dst_sents = []\n",
    "\n",
    "batch_qty_step = 20\n",
    "\n",
    "for batch_start_sent_id in range(0, len(all_src_sents), batch_qty_step):\n",
    "    print('Batch start', batch_start_sent_id)\n",
    "\n",
    "    batch_qty = min(batch_qty_step, len(all_src_sents) - batch_start_sent_id)\n",
    "\n",
    "    batch_sents = [all_src_sents[k] for k in range(batch_start_sent_id,\n",
    "                                                   batch_start_sent_id + batch_qty)]\n",
    "\n",
    "    # batch_data raw contains elements\n",
    "    # UtterData = namedtuple('SentData', ['batch_sent_id', 'sent_pos_oov', 'bert_pos_oov', tok_ids', 'oov_token')\n",
    "    #\n",
    "    # tok_ids_batch is a Tensor with padded Bert-specific token IDs ready\n",
    "    # to be fed into a BERT model\n",
    "    batch_data_raw, tok_ids_batch = get_batch_data(torch_device,\n",
    "                                                 bert_tokenizer,\n",
    "                                                 batch_sents, \n",
    "                                                 MAX_BERT_LEN)\n",
    "\n",
    "    query_arr = [e.oov_token for e in batch_data_raw]\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    neighb_tok_ids = []\n",
    "    \n",
    "    preds = get_bert_preds_for_words_batch(torch_device,\n",
    "                                           bert_model_mlm,\n",
    "                                           batch_data_raw, tok_ids_batch,\n",
    "                                           neighb_tok_ids)\n",
    "\n",
    "    assert(len(preds) == query_qty)\n",
    "    for qid in range(query_qty):\n",
    "        e = batch_data_raw[qid]\n",
    "        glob_sent_id = batch_start_sent_id + e.batch_sent_id\n",
    "        assert(batch_sents[e.batch_sent_id] == all_src_sents[glob_sent_id])\n",
    "        if is_apost_token(e.oov_token) or e.oov_token == \"n't\":\n",
    "            # Thing's like \"I don't\" or \"You're\" are tokenized as do \"I do n't\" or \"You 're'\"\n",
    "            pass # TODO fix this\n",
    "        elif query_tok_oov_id[qid] > 1:  \n",
    "            # Let's map neighbor IDs from each queries to respective \n",
    "            # logits from the prediction set\n",
    "            logit_map = dict() # from Bert-specific token IDs to predicted logits\n",
    "            assert(len(preds[qid].logits) == len(neighb_tok_ids))\n",
    "            for i in range(len(neighb_tok_ids)):\n",
    "                logit_map[neighb_tok_ids[i]] = preds[qid].logits[i]\n",
    "\n",
    "            e = batch_data_raw[qid]\n",
    "            if DEBUG_PRINT: \n",
    "                print(all_src_sents[glob_sent_id])\n",
    "                print(\"### OOV ###\", e.oov_token)\n",
    "                print([bert_id2tok[bert_tok_id] for bert_tok_id in nbrs[qid][0]])\n",
    "\n",
    "            nbrs_sel_logits = []\n",
    "            nbrs_sel_toks = []\n",
    "            nbrs_sel_dists = []\n",
    "\n",
    "            nbrs_ids = nbrs[qid][0]\n",
    "            nbrs_dist = nbrs[qid][1]\n",
    "\n",
    "            #print('Logit map:', logit_map)\n",
    "            #print('neighb_tok_ids', neighb_tok_ids)\n",
    "\n",
    "            nqty = len(nbrs_ids)\n",
    "            for t in range(nqty):\n",
    "                bert_tok_id = nbrs_ids[t]\n",
    "                # nid is Bert-speicifc token ID\n",
    "                if not bert_tok_id in neighb_tok_ids:\n",
    "                    if DEBUG_PRINT: \n",
    "                        print('Missing %s distance %g ' \n",
    "                              % (bert_id2tok[bert_tok_id],\n",
    "                                 nbrs_dist[t]))\n",
    "                else:\n",
    "                    if nbrs_dist[t] < MAX_COSINE_DIST:\n",
    "                        nbrs_sel_logits.append(logit_map[bert_tok_id])\n",
    "                        nbrs_sel_toks.append(bert_id2tok[bert_tok_id]) \n",
    "                        nbrs_sel_dists.append(nbrs_dist[t])\n",
    "\n",
    "            if nbrs_sel_logits:\n",
    "                nbrs_softmax = softmax(np.array(nbrs_sel_logits))\n",
    "                nbrs_simil = 1 - np.array(nbrs_sel_dists)\n",
    "                nbrs_simil_adj = nbrs_softmax * nbrs_simil \n",
    "\n",
    "                best_tok_id = np.argmax(nbrs_simil_adj)\n",
    "\n",
    "                #print(\"batch sent id:\",e.batch_sent_id, e.pos_oov, best_tok_id)\n",
    "                #print(replace_dict[e.batch_sent_id])\n",
    "                assert(not e.pos_oov in replace_dict[e.batch_sent_id])\n",
    "                replace_dict[e.batch_sent_id][e.pos_oov] = nbrs_sel_toks[best_tok_id]\n",
    "\n",
    "                if DEBUG_PRINT: \n",
    "                    print('Selected info, best_tok:', nbrs_sel_toks[best_tok_id])\n",
    "                    for k in range(len(nbrs_sel_logits)):\n",
    "                        print(nbrs_sel_toks[k], nbrs_softmax[k], \n",
    "                              nbrs_sel_dists[k], nbrs_simil_adj[k])\n",
    "            else:\n",
    "                if DEBUG_PRINT: print('Nothing found!')\n",
    "\n",
    "            #if DEBUG_PRINT: print(preds[qid])\n",
    "            if DEBUG_PRINT: \n",
    "                print(\"====================================================================\")\n",
    "\n",
    "\n",
    "\n",
    "    #gc.collect()\n",
    "    #torch.cuda.empty_cache()\n",
    "    for k in range(0, batch_qty):\n",
    "        src_sent = batch_sents[k]\n",
    "        rd = replace_dict[k]\n",
    "        #print('Replacement dict:', rd)\n",
    "        dst_sent = replace_by_patterns(tokenizer, src_sent, rd)\n",
    "        all_dst_sents.append(dst_sent)\n",
    "        if DEBUG_PRINT:\n",
    "            print(\"====================================================================\")\n",
    "            print('Replacement dict:', rd)\n",
    "            print(src_sent)\n",
    "            print('------------')\n",
    "            print(dst_sent)\n",
    "            print(\"====================================================================\")\n",
    "\n",
    "    #break\n",
    "\n",
    "t1 = time.time()\n",
    "print('# of src sentences:', len(all_src_sents), \n",
    "      \"# of dst sentences:\", len(all_dst_sents),\n",
    "      ' time elapsed:', t1 - t0)\n",
    "src_data['comment_text'] = all_dst_sents\n",
    "src_data.to_csv(dst_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#src_data[src_data[\"toxic\"]==1].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fl = pd.read_csv(src_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fl[fl[\"toxic\"]==1].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'flaming' in nlp.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
