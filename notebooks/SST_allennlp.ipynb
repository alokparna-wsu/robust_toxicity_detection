{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import *\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from functools import partial\n",
    "from overrides import overrides\n",
    "\n",
    "from allennlp.data import Instance\n",
    "from allennlp.data.fields import TextField, SequenceLabelField, LabelField\n",
    "from allennlp.data.token_indexers import TokenIndexer, SingleIdTokenIndexer\n",
    "from allennlp.data.tokenizers import Token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODOs:\n",
    "\n",
    "- Make compatible with GPU\n",
    "- Try replicating SST results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self, **kwargs):\n",
    "        for k, v in kwargs.items():\n",
    "            setattr(self, k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config(\n",
    "    testing=True,\n",
    "    seed=1,\n",
    "    batch_size=64, # This is probably too large: need to handle effective v.s. machine batch size\n",
    "    embed_dim=256,\n",
    "    hidden_sz=768,\n",
    "    dataset=\"sst\",\n",
    "    n_classes=2,\n",
    "    max_seq_len=128, # necessary to limit memory usage\n",
    "#     bert_model=None,\n",
    "    bert_model=\"bert-base-cased\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = Path(\"../data\") / config.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set random seed manually to replicate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x11a3b1e50>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(config.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.data.vocabulary import Vocabulary\n",
    "from allennlp.data.dataset_readers import DatasetReader, StanfordSentimentTreeBankDatasetReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader_registry = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def register(name: str):\n",
    "    def dec(x: Callable):\n",
    "        reader_registry[name] = x\n",
    "        return x\n",
    "    return dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "@register(\"jigsaw\")\n",
    "class JigsawDatasetReader(DatasetReader):\n",
    "    def __init__(self, tokenizer: Callable[[str], List[str]]=lambda x: x.split(),\n",
    "                 token_indexers: Dict[str, TokenIndexer] = None, # TODO: Handle mapping from BERT\n",
    "                 max_seq_len: Optional[int]=config.max_seq_len) -> None:\n",
    "        super().__init__(lazy=False)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.token_indexers = token_indexers or {\"tokens\": SingleIdTokenIndexer()}\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "    @overrides\n",
    "    def text_to_instance(self, tokens: List[Token], label: str = None) -> Instance:\n",
    "        # TODO: Reimplement\n",
    "        sentence_field = TextField(tokens, self.token_indexers)\n",
    "        fields = {\"tokens\": sentence_field}\n",
    "\n",
    "        label_field = LabelField(label=label, skip_indexing=True)\n",
    "        fields[\"label\"] = label_field\n",
    "\n",
    "        return Instance(fields)\n",
    "    \n",
    "    @overrides\n",
    "    def _read(self, file_path: str) -> Iterator[Instance]:\n",
    "        df = pd.read_csv(file_path)\n",
    "        if config.testing: df = df.head(10000)\n",
    "        for i, row in df.iterrows():\n",
    "            yield self.text_to_instance(\n",
    "                [Token(x) for x in self.tokenizer(row[\"comment_text\"])],\n",
    "                row[\"toxic\"]\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "@register(\"imdb\")\n",
    "class IMDBDatasetReader(DatasetReader):\n",
    "    def __init__(self, tokenizer=None, \n",
    "                 token_indexers: Dict[str, TokenIndexer] = None,\n",
    "                 max_seq_len=None) -> None:\n",
    "        super().__init__(lazy=False)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.token_indexers = token_indexers or {\"tokens\": SingleIdTokenIndexer()}\n",
    "        self.max_seq_len = max_seq_len\n",
    "    \n",
    "    @overrides\n",
    "    def text_to_instance(self, tokens: List[Token], label: str = None) -> Instance:\n",
    "        sentence_field = TextField(tokens, self.token_indexers)\n",
    "        fields = {\"tokens\": sentence_field}\n",
    "        \n",
    "        # TODO: Add statistical features?\n",
    "\n",
    "        label_field = LabelField(label=label)\n",
    "        fields[\"label\"] = label_field\n",
    "\n",
    "        return Instance(fields)\n",
    "\n",
    "    @overrides\n",
    "    def _read(self, file_path: str) -> Iterator[Instance]:\n",
    "        # TODO: Implement\n",
    "        for label in [\"pos\", \"neg\"]:\n",
    "            for file in (Path(file_path) / label).glob(\"*.txt\"):\n",
    "                text = file.open(\"rt\", encoding=\"utf-8\").read()\n",
    "                yield self.text_to_instance([Token(word) for word in self.tokenizer(text)], \n",
    "                                            label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "@register(\"sst\")\n",
    "class SSTDatasetReader(StanfordSentimentTreeBankDatasetReader):\n",
    "    def __init__(self, *args, tokenizer=None, **kwargs):\n",
    "        super().__init__(*args, granularity=f\"{config.n_classes}-class\", **kwargs)\n",
    "        self._tokenizer = tokenizer\n",
    "        \n",
    "    @overrides\n",
    "    def text_to_instance(self, tokens: List[str], sentiment: str=None) -> Instance:\n",
    "        \"\"\"\n",
    "        Forcibly re-tokenize the input to be wordpiece tokenized\n",
    "        \"\"\"\n",
    "        tokens = self._tokenizer(\" \".join(tokens))\n",
    "        return super().text_to_instance(tokens, sentiment=sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare token handlers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/18/2019 10:10:59 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt from cache at /Users/keitakurita/.pytorch_pretrained_bert/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1\n"
     ]
    }
   ],
   "source": [
    "from allennlp.data.token_indexers import PretrainedBertIndexer, SingleIdTokenIndexer\n",
    "if config.bert_model is not None:\n",
    "    token_indexer = PretrainedBertIndexer(\n",
    "        pretrained_model=config.bert_model,\n",
    "        max_pieces=config.max_seq_len,\n",
    "        do_lowercase=\"uncased\" in config.bert_model,\n",
    "     )\n",
    "    # apparently we need to truncate the sequence here, which is a stupid design decision\n",
    "    def tokenizer(s: str):\n",
    "        return token_indexer.wordpiece_tokenizer(s)[:config.max_seq_len - 2]\n",
    "else:\n",
    "    token_indexer = SingleIdTokenIndexer(\n",
    "        lowercase_tokens=False,  # don't lowercase by default\n",
    "    )\n",
    "    tokenizer = lambda x: x.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader_cls = reader_registry[config.dataset]\n",
    "reader = reader_cls(tokenizer=tokenizer,\n",
    "                    token_indexers={\"tokens\": token_indexer})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]01/18/2019 10:29:05 - INFO - allennlp.data.dataset_readers.stanford_sentiment_tree_bank -   Reading instances from lines in file at: ../data/sst/trees/train.txt\n",
      "6920it [00:02, 2549.30it/s]\n",
      "0it [00:00, ?it/s]01/18/2019 10:29:07 - INFO - allennlp.data.dataset_readers.stanford_sentiment_tree_bank -   Reading instances from lines in file at: ../data/sst/trees/dev.txt\n",
      "872it [00:00, 2716.32it/s]\n",
      "0it [00:00, ?it/s]01/18/2019 10:29:08 - INFO - allennlp.data.dataset_readers.stanford_sentiment_tree_bank -   Reading instances from lines in file at: ../data/sst/trees/test.txt\n",
      "1821it [00:00, 2073.79it/s]\n"
     ]
    }
   ],
   "source": [
    "if config.dataset == \"imdb\":\n",
    "    data_dir = DATA_ROOT / \"imdb\" / \"aclImdb\"\n",
    "    train_ds, test_ds = (reader.read(data_dir / fname) for fname in [\"train\", \"test\"])\n",
    "    val_ds = None\n",
    "elif config.dataset == \"sst\":\n",
    "    data_dir = DATA_ROOT / \"trees\"\n",
    "    train_ds, val_ds, test_ds = (reader.read(data_dir / fname) for fname in [\"train.txt\", \"dev.txt\", \"test.txt\"])\n",
    "else:\n",
    "    train_ds, test_ds = (reader.read(DATA_ROOT / fname) for fname in [\"train.csv\", \"test_proced.csv\"])\n",
    "    val_ds = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6920, 1821)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_ds), len(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/18/2019 10:29:10 - INFO - allennlp.data.vocabulary -   Fitting token dictionary from dataset.\n",
      "100%|██████████| 6920/6920 [00:00<00:00, 144745.86it/s]\n"
     ]
    }
   ],
   "source": [
    "vocab = Vocabulary.from_instances(train_ds)\n",
    "if config.bert_model is not None: \n",
    "    token_indexer._add_encoding_to_vocabulary(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.training.metrics import CategoricalAccuracy\n",
    "from allennlp.data.iterators import BucketIterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Allow for customization\n",
    "iterator = BucketIterator(batch_size=config.batch_size, \n",
    "                          biggest_batch_first=True,\n",
    "                          sorting_keys=[(\"tokens\", \"num_tokens\")],)\n",
    "iterator.index_with(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    batch = next(iter(iterator(train_ds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': {'tokens': tensor([[  101,  1249,  1342,  1193,  1112,  1103,  2523,  4642,  1106,  1294,\n",
       "            2305,  1104,  1157,  1641,  1959,   117,  1175,  2606,   170,  3321,\n",
       "            7275,  1206,  1103,  1273,   112,  1116, 19857,   117,  4044, 28137,\n",
       "           12734, 10136, 18208,  1200,   118, 26161,  2064, 28137,  6983, 16513,\n",
       "            2511,   118,  2069, 22672, 28137,  1105, 20497,  6696,  2944,  4096,\n",
       "            1115,  1185,  2971,  1104, 21304, 18977, 15604, 21155, 12805,  5389,\n",
       "            6185,  1169,  2738,   119,   102,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0],\n",
       "          [  101,  1332,  1122,   112,  1116,  1136,  2095, 13621,  1107, 16358,\n",
       "           26654,  1348,  1143,  2858,  7412,  1918,   117,   169, 28152,  5230,\n",
       "            2453,  4373,   140,  2149,  5710,   112, 28131,  1110,   170,  4105,\n",
       "             117,  7345,   117,  1105, 24815,  3789, 28137,  7412,  1918,  1164,\n",
       "             170,  1685,  1590,  1150,  3349,  1242,  1614,  1107,  1297,   117,\n",
       "            1133, 10434,  1131,   112,  2339,  1561,  1123,  1534,  1196,  1131,\n",
       "            3370,  1106, 14414,  1123,  6149,   119,   102,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0],\n",
       "          [  101,  2770,   140, 15432,  1144,  1155,  1103, 26452,  1116,  1104,\n",
       "            1126, 20069,   117,  6122, 28137, 21209,  1116,  7644,   117,  1133,\n",
       "            3769,  1146,  1167,  1104,   170,   172, 22362,  1183,   169, 28152,\n",
       "           12004,  5651,   112, 28131,  1231,  7877,  3556,   117,  1114,  1103,\n",
       "            7569,  1113,  2191, 28137,  5521, 26703,   188,  1732,  7435,  5745,\n",
       "            1105,  1992, 28137, 17159, 25502,  1115,  3114,   185,  1596,  1157,\n",
       "            1641,  1126,  1170,  1582, 26960,   119,   102,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0],\n",
       "          [  101,  1875,  2881,   112,  1116,   184,  2007,  1106, 27629, 27102,\n",
       "            1297,   112,  1116,  4608,  1880,  1110,   170, 26084,  6647,  1105,\n",
       "            1107,  2528, 12807,  2227, 23487,  1186,  1164,  1103, 14673,  1757,\n",
       "            1104,  8366,  1348, 10116,  1232,  1130,   153, 14089,  2217,  1104,\n",
       "            2185,  2606,   170, 11925, 10771,  1361,  1105,  3073,  5208,  3121,\n",
       "            2285,  1322, 26687,  1115,   112,  1116,  8362, 14467,  6697,  1174,\n",
       "            1105, 21359, 24348,  1193,  4252,  2225,  3365,  3798,   119,   102,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0],\n",
       "          [  101,   118, 26161,  2064, 28137,   140,  2858, 18066,   112,  1116,\n",
       "             118,  2069, 22672, 28137,  1963,  1169,  1129,  4806,  1104,  1217,\n",
       "             170,  2113,  5576,  1548,  6617,  1643, 14523,   117,  1133,  1122,\n",
       "            1144,   170, 15194,   117,  1228, 14262,  2305,  1104,  1947,  1105,\n",
       "            8594,  1115,  5401,  1119,  1108,  4401,  1118,  1199,  1104,  1103,\n",
       "           18992,  1150,  1138,  2002,  1140,   117,  2108,  1103,  3291,  1424,\n",
       "            3330,  1105,  6536,  1573,  2692,  2953,  1324,   119,   102,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0],\n",
       "          [  101,  1247,  1132,   183, 28131,  1204,  1315,  1242,  2441,  1115,\n",
       "            1169,  1129,  1112,  7344,  6276,   117,  1228, 14262,  1105,  1762,\n",
       "           18900,  1158,   118, 26161,  2064, 28137,  1443,   170,  3528,   188,\n",
       "           18208, 19386,  1104,  1103,  1301,  1186,   117,  1120,  1655,   118,\n",
       "            2069, 22672, 28137,   117,  1133,   169, 28152,  2896,  1979,   112,\n",
       "           28131,  8701,  1106,  1202,  1155,  1210,  2385,  1218,   117,  1543,\n",
       "            1122,  1141,  1104,  1103,  1214,   112,  1116,  1211, 24815,  6596,\n",
       "             119,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0],\n",
       "          [  101,   118, 26161,  2064, 28137,  1392,   118,  2069, 22672, 28137,\n",
       "           16816,  1366,  1293, 13142,  2716,   183,  8734,  9650,   170,  1823,\n",
       "            3177, 27453,  2180,  2099,  1169,  1129,  1165,  1119,  1110,  1136,\n",
       "            1167, 23284,  1193,  4349,  1107,  1103, 10499,  2008,  2191, 28137,\n",
       "            8766,  4578,  5332,  1104,   169,  9954,  1193,  3171,  1188,   112,\n",
       "             118, 26161,  2064, 28137,  1729,   118,  2069, 22672, 28137,  1105,\n",
       "             169,  9954,  1193,  3171,  1337,   117,   112,  5163,   118, 26161,\n",
       "            2064, 28137,  1137,  4963,   118,  2069, 22672, 28137,  1111,  1224,\n",
       "            1142,  1214,   119,   102],\n",
       "          [  101,  1232,   188, 15633,  1181,  1114,  8594,   118, 26161,  2064,\n",
       "           28137,   112,   146,  2936, 21785,  3596,  5886,   117,   112, 19961,\n",
       "            1116, 14760, 13200,  1643,  1170,   170,  1897,   117, 14044,   117,\n",
       "             171, 10354,  4999,  3670,  1114,  1126,  8143,  5579,  9332,   118,\n",
       "            2069, 22672, 28137,  1105, 20787,  2340,  1146, 23562,  1116,   118,\n",
       "           26161,  2064, 28137,  4826,   112,  1116, 14247, 10595,  1144,  1151,\n",
       "            2125,  1114, 12556, 15615,  1324,   117,   170, 10509,  8143,  6093,\n",
       "            1150, 27180,  1116,  2490,  1105,  1917,  1213,   118,  2069, 22672,\n",
       "           28137,   102,     0,     0]]),\n",
       "  'tokens-offsets': tensor([[ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18,\n",
       "           19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36,\n",
       "           37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54,\n",
       "           55, 56, 57, 58, 59, 60, 61, 62, 63,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "            0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "          [ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18,\n",
       "           19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36,\n",
       "           37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54,\n",
       "           55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65,  0,  0,  0,  0,  0,  0,  0,\n",
       "            0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "          [ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18,\n",
       "           19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36,\n",
       "           37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54,\n",
       "           55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65,  0,  0,  0,  0,  0,  0,  0,\n",
       "            0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "          [ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18,\n",
       "           19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36,\n",
       "           37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54,\n",
       "           55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68,  0,  0,  0,  0,\n",
       "            0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "          [ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18,\n",
       "           19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36,\n",
       "           37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54,\n",
       "           55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,  0,  0,  0,  0,  0,\n",
       "            0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "          [ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18,\n",
       "           19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36,\n",
       "           37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54,\n",
       "           55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70,  0,  0,\n",
       "            0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "          [ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18,\n",
       "           19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36,\n",
       "           37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54,\n",
       "           55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72,\n",
       "           73, 74, 75, 76, 77, 78, 79, 80, 81, 82],\n",
       "          [ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18,\n",
       "           19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36,\n",
       "           37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54,\n",
       "           55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72,\n",
       "           73, 74, 75, 76, 77, 78, 79, 80,  0,  0]]),\n",
       "  'mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 0, 0]])},\n",
       " 'label': tensor([1, 0, 1, 1, 0, 0, 0, 0])}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  101,  1249,  1342,  1193,  1112,  1103,  2523,  4642,  1106,  1294,\n",
       "          2305,  1104,  1157,  1641,  1959,   117,  1175,  2606,   170,  3321,\n",
       "          7275,  1206,  1103,  1273,   112,  1116, 19857,   117,  4044, 28137,\n",
       "         12734, 10136, 18208,  1200,   118, 26161,  2064, 28137,  6983, 16513,\n",
       "          2511,   118,  2069, 22672, 28137,  1105, 20497,  6696,  2944,  4096,\n",
       "          1115,  1185,  2971,  1104, 21304, 18977, 15604, 21155, 12805,  5389,\n",
       "          6185,  1169,  2738,   119,   102,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0],\n",
       "        [  101,  1332,  1122,   112,  1116,  1136,  2095, 13621,  1107, 16358,\n",
       "         26654,  1348,  1143,  2858,  7412,  1918,   117,   169, 28152,  5230,\n",
       "          2453,  4373,   140,  2149,  5710,   112, 28131,  1110,   170,  4105,\n",
       "           117,  7345,   117,  1105, 24815,  3789, 28137,  7412,  1918,  1164,\n",
       "           170,  1685,  1590,  1150,  3349,  1242,  1614,  1107,  1297,   117,\n",
       "          1133, 10434,  1131,   112,  2339,  1561,  1123,  1534,  1196,  1131,\n",
       "          3370,  1106, 14414,  1123,  6149,   119,   102,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0],\n",
       "        [  101,  2770,   140, 15432,  1144,  1155,  1103, 26452,  1116,  1104,\n",
       "          1126, 20069,   117,  6122, 28137, 21209,  1116,  7644,   117,  1133,\n",
       "          3769,  1146,  1167,  1104,   170,   172, 22362,  1183,   169, 28152,\n",
       "         12004,  5651,   112, 28131,  1231,  7877,  3556,   117,  1114,  1103,\n",
       "          7569,  1113,  2191, 28137,  5521, 26703,   188,  1732,  7435,  5745,\n",
       "          1105,  1992, 28137, 17159, 25502,  1115,  3114,   185,  1596,  1157,\n",
       "          1641,  1126,  1170,  1582, 26960,   119,   102,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0],\n",
       "        [  101,  1875,  2881,   112,  1116,   184,  2007,  1106, 27629, 27102,\n",
       "          1297,   112,  1116,  4608,  1880,  1110,   170, 26084,  6647,  1105,\n",
       "          1107,  2528, 12807,  2227, 23487,  1186,  1164,  1103, 14673,  1757,\n",
       "          1104,  8366,  1348, 10116,  1232,  1130,   153, 14089,  2217,  1104,\n",
       "          2185,  2606,   170, 11925, 10771,  1361,  1105,  3073,  5208,  3121,\n",
       "          2285,  1322, 26687,  1115,   112,  1116,  8362, 14467,  6697,  1174,\n",
       "          1105, 21359, 24348,  1193,  4252,  2225,  3365,  3798,   119,   102,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0],\n",
       "        [  101,   118, 26161,  2064, 28137,   140,  2858, 18066,   112,  1116,\n",
       "           118,  2069, 22672, 28137,  1963,  1169,  1129,  4806,  1104,  1217,\n",
       "           170,  2113,  5576,  1548,  6617,  1643, 14523,   117,  1133,  1122,\n",
       "          1144,   170, 15194,   117,  1228, 14262,  2305,  1104,  1947,  1105,\n",
       "          8594,  1115,  5401,  1119,  1108,  4401,  1118,  1199,  1104,  1103,\n",
       "         18992,  1150,  1138,  2002,  1140,   117,  2108,  1103,  3291,  1424,\n",
       "          3330,  1105,  6536,  1573,  2692,  2953,  1324,   119,   102,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0],\n",
       "        [  101,  1247,  1132,   183, 28131,  1204,  1315,  1242,  2441,  1115,\n",
       "          1169,  1129,  1112,  7344,  6276,   117,  1228, 14262,  1105,  1762,\n",
       "         18900,  1158,   118, 26161,  2064, 28137,  1443,   170,  3528,   188,\n",
       "         18208, 19386,  1104,  1103,  1301,  1186,   117,  1120,  1655,   118,\n",
       "          2069, 22672, 28137,   117,  1133,   169, 28152,  2896,  1979,   112,\n",
       "         28131,  8701,  1106,  1202,  1155,  1210,  2385,  1218,   117,  1543,\n",
       "          1122,  1141,  1104,  1103,  1214,   112,  1116,  1211, 24815,  6596,\n",
       "           119,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0],\n",
       "        [  101,   118, 26161,  2064, 28137,  1392,   118,  2069, 22672, 28137,\n",
       "         16816,  1366,  1293, 13142,  2716,   183,  8734,  9650,   170,  1823,\n",
       "          3177, 27453,  2180,  2099,  1169,  1129,  1165,  1119,  1110,  1136,\n",
       "          1167, 23284,  1193,  4349,  1107,  1103, 10499,  2008,  2191, 28137,\n",
       "          8766,  4578,  5332,  1104,   169,  9954,  1193,  3171,  1188,   112,\n",
       "           118, 26161,  2064, 28137,  1729,   118,  2069, 22672, 28137,  1105,\n",
       "           169,  9954,  1193,  3171,  1337,   117,   112,  5163,   118, 26161,\n",
       "          2064, 28137,  1137,  4963,   118,  2069, 22672, 28137,  1111,  1224,\n",
       "          1142,  1214,   119,   102],\n",
       "        [  101,  1232,   188, 15633,  1181,  1114,  8594,   118, 26161,  2064,\n",
       "         28137,   112,   146,  2936, 21785,  3596,  5886,   117,   112, 19961,\n",
       "          1116, 14760, 13200,  1643,  1170,   170,  1897,   117, 14044,   117,\n",
       "           171, 10354,  4999,  3670,  1114,  1126,  8143,  5579,  9332,   118,\n",
       "          2069, 22672, 28137,  1105, 20787,  2340,  1146, 23562,  1116,   118,\n",
       "         26161,  2064, 28137,  4826,   112,  1116, 14247, 10595,  1144,  1151,\n",
       "          2125,  1114, 12556, 15615,  1324,   117,   170, 10509,  8143,  6093,\n",
       "          1150, 27180,  1116,  2490,  1105,  1917,  1213,   118,  2069, 22672,\n",
       "         28137,   102,     0,     0]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"tokens\"][\"tokens\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 84])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"tokens\"][\"tokens\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.models import Model\n",
    "from allennlp.modules.text_field_embedders import TextFieldEmbedder, BasicTextFieldEmbedder\n",
    "from allennlp.modules.token_embedders import Embedding\n",
    "from allennlp.modules.token_embedders.bert_token_embedder import BertEmbedder, PretrainedBertEmbedder\n",
    "from allennlp.modules.seq2vec_encoders import Seq2VecEncoder, PytorchSeq2VecWrapper\n",
    "from allennlp.modules.stacked_bidirectional_lstm import StackedBidirectionalLstm\n",
    "from allennlp.nn.util import get_text_field_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LstmEncoder(nn.Module):\n",
    "    def __init__(self, lstm):\n",
    "        super().__init__()\n",
    "        self.lstm = lstm\n",
    "        \n",
    "    def forward(self, x, mask): # TODO: replace with allennlp built in modules\n",
    "        _, (state, _) = self.lstm(x)\n",
    "        state = torch.cat([state[0, :, :], state[1, :, :]], dim=1)\n",
    "        return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertPooler(nn.Module):\n",
    "    \"\"\"Source code copied\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(768, 768)\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "    def forward(self, hidden_states, mask):\n",
    "        # We \"pool\" the model by simply taking the hidden state corresponding\n",
    "        # to the first token.\n",
    "        first_token_tensor = hidden_states[:, 0]\n",
    "        pooled_output = self.dense(first_token_tensor)\n",
    "        pooled_output = self.activation(pooled_output)\n",
    "        return pooled_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentAnalysisModel(Model):\n",
    "    def __init__(self, word_embeddings: TextFieldEmbedder,\n",
    "                 encoder: StackedBidirectionalLstm,\n",
    "                 out_sz=config.n_classes):\n",
    "        super().__init__(vocab)\n",
    "        self.word_embeddings = word_embeddings\n",
    "        self.encoder = encoder\n",
    "#         self.projection = nn.Linear(encoder.get_output_dim(), out_sz)\n",
    "        self.projection = nn.Linear(config.hidden_sz, out_sz)\n",
    "        self.accuracy = CategoricalAccuracy()\n",
    "        \n",
    "    def forward(self,\n",
    "                tokens: Dict[str, torch.Tensor],\n",
    "                label: torch.Tensor = None) -> torch.Tensor:\n",
    "        mask = get_text_field_mask(tokens)\n",
    "        embeddings = self.word_embeddings(tokens[\"tokens\"])\n",
    "        state = self.encoder(embeddings, mask)\n",
    "\n",
    "        class_logits = self.projection(state)\n",
    "        \n",
    "        output = {\"class_logits\": class_logits}\n",
    "        if label is not None:\n",
    "            output[\"accuracy\"] = self.accuracy(class_logits, label, None)\n",
    "            output[\"loss\"] = nn.CrossEntropyLoss()(class_logits, label)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def get_metrics(self, reset: bool = False) -> Dict[str, float]:\n",
    "        return {\"accuracy\": self.accuracy.get_metric(reset)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/18/2019 10:26:02 - INFO - pytorch_pretrained_bert.modeling -   loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased.tar.gz from cache at /Users/keitakurita/.pytorch_pretrained_bert/a803ce83ca27fecf74c355673c434e51c265fb8a3e0e57ac62a80e38ba98d384.681017f415dfb33ec8d0e04fe51a619f3f01532ecea04edbfd48c5d160550d9c\n",
      "01/18/2019 10:26:02 - INFO - pytorch_pretrained_bert.modeling -   extracting archive file /Users/keitakurita/.pytorch_pretrained_bert/a803ce83ca27fecf74c355673c434e51c265fb8a3e0e57ac62a80e38ba98d384.681017f415dfb33ec8d0e04fe51a619f3f01532ecea04edbfd48c5d160550d9c to temp dir /var/folders/hy/1czs1y5j2d58zgkqx6w_wnpw0000gn/T/tmp_0utw264\n",
      "01/18/2019 10:26:05 - INFO - pytorch_pretrained_bert.modeling -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if config.bert_model is None:\n",
    "    token_embedding = Embedding(num_embeddings=vocab.get_vocab_size('tokens'),\n",
    "                                embedding_dim=config.embed_dim)\n",
    "    word_embeddings = BasicTextFieldEmbedder({\"tokens\": token_embedding})\n",
    "\n",
    "    # encoder = PytorchSeq2VecWrapper(nn.LSTM(config.embed_dim, config.hidden_sz, batch_first=True,\n",
    "    #                                         bidirectional=True))\n",
    "    encoder = LstmEncoder(\n",
    "        nn.LSTM(config.embed_dim, config.hidden_sz, batch_first=True, bidirectional=True)\n",
    "    )\n",
    "\n",
    "else:\n",
    "    word_embeddings = PretrainedBertEmbedder(\n",
    "        pretrained_model=config.bert_model,\n",
    "        top_layer_only=True, # conserve memory\n",
    "    )\n",
    "    encoder = BertPooler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentimentAnalysisModel(\n",
    "    word_embeddings, \n",
    "    encoder, \n",
    "    out_sz=config.n_classes,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentimentAnalysisModel(\n",
       "  (word_embeddings): PretrainedBertEmbedder(\n",
       "    (bert_model): BertModel(\n",
       "      (embeddings): BertEmbeddings(\n",
       "        (word_embeddings): Embedding(28996, 768)\n",
       "        (position_embeddings): Embedding(512, 768)\n",
       "        (token_type_embeddings): Embedding(2, 768)\n",
       "        (LayerNorm): BertLayerNorm()\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (encoder): BertEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): BertLayerNorm()\n",
       "                (dropout): Dropout(p=0.1)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (1): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): BertLayerNorm()\n",
       "                (dropout): Dropout(p=0.1)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (2): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): BertLayerNorm()\n",
       "                (dropout): Dropout(p=0.1)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (3): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): BertLayerNorm()\n",
       "                (dropout): Dropout(p=0.1)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (4): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): BertLayerNorm()\n",
       "                (dropout): Dropout(p=0.1)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (5): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): BertLayerNorm()\n",
       "                (dropout): Dropout(p=0.1)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (6): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): BertLayerNorm()\n",
       "                (dropout): Dropout(p=0.1)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (7): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): BertLayerNorm()\n",
       "                (dropout): Dropout(p=0.1)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (8): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): BertLayerNorm()\n",
       "                (dropout): Dropout(p=0.1)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (9): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): BertLayerNorm()\n",
       "                (dropout): Dropout(p=0.1)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (10): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): BertLayerNorm()\n",
       "                (dropout): Dropout(p=0.1)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (11): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): BertLayerNorm()\n",
       "                (dropout): Dropout(p=0.1)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (pooler): BertPooler(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (activation): Tanh()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (encoder): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       "  (projection): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic sanity checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.isnan(list(model.word_embeddings.parameters())[0].detach().numpy()).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[False, False]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[np.isnan(x.detach().numpy()).any() for x in list(model.encoder.parameters())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[False, False]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[np.isinf(x.detach().numpy()).any() for x in list(model.encoder.parameters())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0811,  0.2274, -0.0964,  ..., -0.6108, -0.4058, -0.0385],\n",
       "        [-0.2271,  0.3030, -0.2617,  ..., -0.6076, -0.2934,  0.0424],\n",
       "        [-0.1593,  0.2987, -0.2722,  ..., -0.6099, -0.3207, -0.0030],\n",
       "        ...,\n",
       "        [-0.1332,  0.4748, -0.2816,  ..., -0.7125, -0.2877, -0.0654],\n",
       "        [-0.1009,  0.2775, -0.1097,  ..., -0.5298, -0.2518,  0.0039],\n",
       "        [-0.0431,  0.5821, -0.4042,  ..., -0.4234, -0.2138,  0.1691]],\n",
       "       grad_fn=<TanhBackward>)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = batch[\"tokens\"]\n",
    "encoder(model.word_embeddings(tokens[\"tokens\"]), get_text_field_mask(tokens))\n",
    "# encoder(model.word_embeddings(tokens[\"tokens\"]))[1][0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = model(**batch)[\"loss\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6900, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.CrossEntropyLoss()(model(**batch)[\"class_logits\"][:10, :], batch[\"label\"][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6905, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[-2.8748e-04,  1.4027e-04,  2.7458e-04,  ...,  3.5053e-04,\n",
       "           7.0984e-08, -1.5276e-04],\n",
       "         [ 2.1549e-03, -1.2364e-03, -2.3016e-03,  ..., -2.7667e-03,\n",
       "           9.1618e-05,  1.2072e-03],\n",
       "         [-2.7512e-03,  1.2500e-03,  2.6125e-03,  ...,  3.2471e-03,\n",
       "          -3.3257e-05, -1.3607e-03],\n",
       "         ...,\n",
       "         [ 8.0093e-04, -2.2354e-04, -5.5953e-04,  ..., -6.8971e-04,\n",
       "           1.0722e-04,  2.7345e-04],\n",
       "         [ 1.2561e-03, -6.4963e-04, -1.2403e-03,  ..., -1.5432e-03,\n",
       "           2.7694e-05,  6.6736e-04],\n",
       "         [ 2.1756e-03, -1.0390e-03, -2.0930e-03,  ..., -2.5987e-03,\n",
       "           6.2243e-05,  1.1039e-03]]),\n",
       " tensor([-8.7704e-04,  6.5429e-03, -8.0143e-03, -2.4327e-03, -1.6460e-03,\n",
       "          9.6156e-04,  8.4481e-03, -2.2472e-03, -1.1686e-03, -4.3503e-03,\n",
       "         -2.8182e-03, -4.1889e-03, -2.7003e-03,  3.6320e-03,  1.2600e-03,\n",
       "          5.2557e-03, -9.4235e-04, -6.7970e-05,  2.4847e-03,  2.6885e-03,\n",
       "         -1.8087e-03, -6.4176e-04,  5.3326e-04,  1.7899e-04,  3.9646e-04,\n",
       "          2.6846e-03,  2.9211e-04, -6.7244e-03,  1.1936e-03, -2.7174e-04,\n",
       "          5.5480e-04,  1.1939e-03,  3.1206e-03,  4.2798e-03, -6.2434e-03,\n",
       "          2.0115e-03, -3.7010e-03, -4.8293e-03,  1.3602e-03, -2.9013e-03,\n",
       "         -2.2868e-03,  3.1443e-03,  1.3997e-03,  3.2569e-03,  3.0877e-03,\n",
       "          1.6190e-03, -3.0040e-03, -1.8088e-03,  1.1663e-03,  5.0758e-03,\n",
       "         -4.8889e-03, -1.3964e-03, -3.1802e-03,  3.0033e-03, -8.0231e-03,\n",
       "          4.5617e-03,  4.3965e-03, -4.8646e-03, -1.5205e-03, -5.6717e-04,\n",
       "         -2.4544e-03,  3.2511e-03,  1.6415e-03, -3.5023e-03, -5.5523e-03,\n",
       "         -4.7814e-03, -6.3537e-03,  7.9003e-04,  2.4521e-03,  2.0804e-03,\n",
       "          2.0317e-03, -3.0157e-04,  5.7328e-03,  4.3350e-03, -1.7988e-03,\n",
       "          3.3037e-03, -2.1039e-04,  1.5782e-03, -5.6436e-03, -8.4827e-04,\n",
       "         -3.0787e-03, -4.1413e-03,  3.8011e-03,  7.4663e-03,  5.2858e-03,\n",
       "          4.2159e-03, -3.6225e-03, -2.0396e-03,  2.1787e-03, -2.6578e-03,\n",
       "          9.2102e-04, -7.5095e-03,  8.3280e-04,  1.4418e-03, -4.8450e-03,\n",
       "         -5.9165e-04, -3.0412e-03,  1.4986e-03, -1.1411e-03, -2.8686e-03,\n",
       "         -5.0935e-03,  4.7867e-03,  5.8810e-04,  1.7447e-03, -2.8158e-03,\n",
       "         -3.5826e-03, -1.7026e-03, -8.3942e-04,  1.2500e-03,  4.3178e-03,\n",
       "          4.1025e-03, -2.8226e-03,  5.1888e-03,  6.4923e-03,  1.8822e-03,\n",
       "         -4.6026e-03, -6.9787e-03,  1.7203e-03, -3.8698e-03, -1.3963e-03,\n",
       "         -1.1645e-03,  2.8306e-04,  1.8432e-03, -3.9960e-03,  2.7594e-04,\n",
       "          6.5880e-03,  6.4117e-04,  5.8653e-03, -4.6712e-04, -3.7052e-03,\n",
       "          1.5836e-03, -5.0821e-04, -2.1208e-03,  1.0387e-03, -1.2348e-03,\n",
       "          7.1910e-03,  5.0871e-04,  1.1311e-03,  3.2626e-03, -4.7824e-03,\n",
       "          5.0846e-03,  4.1993e-03,  6.8383e-03,  2.4374e-03,  2.7208e-03,\n",
       "          3.0384e-03,  2.7355e-04, -8.9880e-04, -4.3217e-03, -2.5396e-03,\n",
       "          3.5227e-03, -1.3979e-04,  9.4074e-04, -5.4262e-03, -5.5265e-03,\n",
       "         -6.4299e-03, -2.6265e-04, -2.5164e-03, -3.5277e-03,  1.5900e-03,\n",
       "         -2.2699e-03,  1.0844e-03,  1.9285e-03, -1.2797e-03,  6.3486e-03,\n",
       "          1.2910e-03,  4.9249e-03, -2.7404e-03,  1.1680e-03, -2.7347e-03,\n",
       "         -6.5730e-04,  4.9729e-03, -2.8064e-03, -3.4354e-03, -1.8408e-03,\n",
       "          2.8963e-03,  2.3768e-03,  1.2360e-03,  4.2340e-03,  5.3467e-03,\n",
       "         -3.0669e-03, -9.0047e-03,  2.0235e-03, -5.4226e-03, -3.3642e-03,\n",
       "          8.2574e-03, -5.8527e-03,  2.1092e-03, -5.8538e-04, -5.4469e-03,\n",
       "          2.2527e-03,  2.8855e-03, -1.8109e-03, -4.4253e-03, -9.0084e-04,\n",
       "         -2.7060e-04,  7.9165e-03,  5.8615e-03,  2.9155e-04, -4.7160e-03,\n",
       "          5.1480e-03,  1.2195e-03,  4.1188e-03,  1.3715e-04, -1.3421e-03,\n",
       "          1.3019e-03, -3.4356e-03,  2.8063e-03, -3.0688e-03, -1.9526e-03,\n",
       "         -4.0388e-05, -2.5226e-03, -7.7036e-03, -1.3299e-03, -5.3174e-03,\n",
       "         -2.0308e-03, -2.0781e-03, -2.0446e-03, -1.0382e-03, -1.8462e-03,\n",
       "         -3.3478e-03,  2.2229e-03, -6.9154e-03,  4.1568e-03, -4.9913e-03,\n",
       "          3.8726e-04,  4.5066e-03,  1.0780e-03,  1.1085e-03, -2.8012e-04,\n",
       "         -1.9324e-03, -3.5648e-03, -1.5744e-03, -4.6016e-03, -2.0707e-03,\n",
       "          3.2894e-04, -2.0966e-03,  1.0444e-03, -7.5061e-03, -6.5000e-04,\n",
       "          2.3515e-04,  1.9326e-03, -8.1775e-04,  4.6490e-03,  6.2933e-04,\n",
       "         -3.1413e-04, -5.8944e-03,  6.0861e-03, -6.1046e-03, -7.2283e-03,\n",
       "          1.3994e-03, -7.1710e-04, -7.6011e-05,  2.2463e-03, -5.7571e-03,\n",
       "          4.0350e-03,  1.8500e-03, -1.6584e-03, -5.8466e-03, -3.6307e-03,\n",
       "          4.7064e-03, -3.5249e-03,  7.5555e-03,  2.7375e-03, -3.4009e-03,\n",
       "          6.2802e-04, -6.2238e-03, -1.2980e-03, -2.2339e-03, -1.1041e-03,\n",
       "          9.2625e-04,  4.8468e-05, -4.0788e-04,  8.1524e-03,  3.4762e-03,\n",
       "         -2.1175e-03,  1.9211e-03,  5.0663e-04, -7.9757e-03,  1.8903e-03,\n",
       "         -2.5728e-03,  5.6553e-04, -5.5245e-04,  6.1650e-03, -2.6100e-03,\n",
       "         -5.9713e-03, -1.8454e-03, -6.3873e-03,  1.9854e-03,  5.7651e-03,\n",
       "         -4.6105e-04, -3.7822e-03,  6.7532e-03,  4.0913e-03,  1.0577e-03,\n",
       "          7.7856e-03,  6.6150e-03,  1.0056e-03,  2.7955e-04,  2.4587e-03,\n",
       "          4.6902e-03,  5.6155e-03, -4.0147e-04,  3.2059e-04,  4.2893e-03,\n",
       "         -4.7196e-03,  1.5242e-03,  4.5480e-03, -3.7916e-03,  1.9894e-03,\n",
       "         -2.0950e-03,  4.7939e-03, -2.7458e-03,  4.3363e-03,  1.7971e-03,\n",
       "          4.2489e-03, -1.1402e-03, -2.7088e-03,  3.2982e-04,  1.0287e-03,\n",
       "         -1.8833e-03,  3.1670e-03, -5.0185e-03, -4.6378e-03, -1.1981e-03,\n",
       "          1.9244e-03, -6.9522e-05, -2.4149e-03, -4.1607e-03,  6.6399e-04,\n",
       "          6.9232e-04,  5.5660e-03,  5.4549e-03, -5.4816e-03, -6.6451e-03,\n",
       "         -7.7512e-04, -6.3673e-03, -1.3091e-03,  6.0144e-03, -1.1638e-03,\n",
       "         -3.4473e-03,  1.1549e-03, -1.3382e-03, -1.4788e-03,  2.2383e-03,\n",
       "          1.3678e-03, -2.8450e-03, -9.1323e-04, -1.4250e-03,  8.4333e-04,\n",
       "         -3.0157e-03,  4.2068e-03, -6.7470e-03,  2.8835e-03, -3.1681e-03,\n",
       "         -4.9136e-03,  2.2098e-03, -3.1934e-03, -2.7974e-04, -3.2375e-03,\n",
       "          3.7715e-03, -2.5373e-03, -1.7205e-03, -2.6461e-03,  2.9150e-03,\n",
       "         -6.1643e-04,  7.9635e-05, -1.8422e-03,  2.8778e-03, -6.6381e-04,\n",
       "          1.6705e-04, -3.6125e-03, -7.0818e-03, -5.6397e-03,  3.2287e-03,\n",
       "         -1.0305e-03, -2.0532e-03,  4.7964e-03,  5.3396e-03, -7.9892e-03,\n",
       "          1.0890e-03, -3.6260e-03, -6.2251e-03,  3.7529e-03,  1.7986e-03,\n",
       "         -4.9009e-03, -1.3862e-03,  1.1354e-04,  1.3559e-03,  1.0092e-04,\n",
       "         -2.0632e-03,  2.2041e-03,  1.4826e-03, -5.8857e-05, -9.0187e-04,\n",
       "          6.3453e-04,  9.4100e-04, -1.9026e-03,  2.0731e-03,  2.6525e-03,\n",
       "         -2.8726e-03,  1.0281e-03, -3.9544e-03, -5.3252e-03, -6.1763e-04,\n",
       "         -3.0988e-04, -2.1178e-03,  3.6577e-03, -4.1511e-03, -4.0038e-03,\n",
       "          8.4395e-04,  3.2735e-03, -4.4052e-04, -5.5948e-04, -2.1874e-03,\n",
       "         -3.7388e-03,  6.1722e-03,  3.4306e-03,  5.1412e-03, -5.3096e-04,\n",
       "          7.4015e-03,  7.3704e-03, -3.1787e-04,  3.2466e-03, -3.7139e-03,\n",
       "         -4.5258e-03,  2.1270e-03, -4.0087e-03, -5.5192e-03, -4.7348e-04,\n",
       "          1.9357e-03, -1.1739e-03, -9.0563e-04,  1.6731e-03, -2.8971e-03,\n",
       "          2.4360e-03, -9.4533e-05,  3.2359e-03, -5.0952e-03,  7.4874e-03,\n",
       "          3.9091e-03, -8.3310e-04,  1.4500e-03,  1.2680e-03,  3.8034e-03,\n",
       "          4.7826e-03,  3.0217e-03,  8.0031e-04, -2.9523e-04, -8.7597e-04,\n",
       "          2.1758e-03, -1.1200e-03,  6.4803e-03,  3.8136e-03, -2.7662e-03,\n",
       "          1.8657e-03, -1.7276e-03,  1.5332e-03,  2.6947e-03,  1.3436e-04,\n",
       "         -1.9642e-03, -6.3071e-04, -3.5473e-03,  5.5529e-03, -1.5128e-03,\n",
       "          1.6296e-03, -5.7090e-03, -3.4284e-04,  3.5160e-03, -2.1043e-03,\n",
       "          2.5456e-03, -1.0199e-04, -5.5690e-03,  1.2146e-03, -1.6449e-03,\n",
       "          4.8286e-04,  2.1975e-03, -3.7678e-03, -3.8429e-03, -2.6606e-03,\n",
       "          7.6481e-03,  4.9227e-03,  2.6233e-03, -3.5159e-03,  3.3951e-03,\n",
       "          2.0506e-03, -2.0074e-03,  2.6005e-03, -2.8540e-03,  5.0689e-03,\n",
       "         -2.2681e-04, -8.6215e-04, -4.4149e-04, -1.2571e-04,  4.7494e-03,\n",
       "          1.0128e-03,  2.3742e-03, -5.5225e-03, -3.9522e-03,  1.6920e-03,\n",
       "         -1.5584e-03,  2.3032e-03, -3.1173e-03,  2.1034e-03, -6.2090e-04,\n",
       "          3.4320e-03, -1.3307e-03, -9.1702e-05,  9.9214e-04, -1.8889e-03,\n",
       "         -1.9585e-03,  2.6186e-03, -1.6967e-03, -1.0533e-03, -4.7924e-03,\n",
       "         -6.0117e-03,  1.9239e-04,  6.4193e-03, -7.1332e-03,  6.3221e-03,\n",
       "         -1.4573e-03, -7.8350e-04, -8.2982e-03,  1.0955e-03, -5.2525e-03,\n",
       "          6.9149e-04, -2.6449e-04, -5.6892e-03, -3.6280e-04,  4.0832e-03,\n",
       "         -3.8940e-03,  1.2554e-03,  3.6123e-03,  3.9260e-03, -3.6098e-06,\n",
       "         -9.4930e-04, -6.5298e-03,  9.6567e-04, -2.4537e-03, -5.4624e-03,\n",
       "          1.2146e-03, -5.2013e-03,  3.2338e-03, -4.0543e-03, -1.8766e-03,\n",
       "         -4.0135e-03,  7.7456e-03,  3.4868e-04,  2.2042e-03, -6.8108e-03,\n",
       "         -3.5403e-03, -3.1416e-04,  3.7565e-03, -3.3154e-03,  1.5295e-04,\n",
       "         -2.9998e-03,  3.4385e-03, -5.2373e-03, -7.2490e-03,  4.1313e-03,\n",
       "         -2.4453e-04, -4.2566e-03, -2.3764e-04, -3.7968e-03, -3.9699e-03,\n",
       "          2.7575e-03, -2.1206e-03,  2.2922e-03, -3.9501e-03,  4.5593e-03,\n",
       "         -2.2379e-03, -2.8867e-03, -4.9048e-03, -2.1105e-04,  1.7750e-03,\n",
       "          3.4390e-03,  1.0600e-03, -6.2290e-04,  4.4700e-04, -7.1593e-04,\n",
       "         -1.7903e-03, -7.6030e-04, -4.5978e-03,  3.6805e-03, -4.3998e-04,\n",
       "         -5.1996e-03,  1.3742e-03, -1.9901e-03,  2.1315e-03,  1.1551e-03,\n",
       "          4.9248e-03, -1.3338e-03,  3.3786e-03,  2.0450e-03,  9.3883e-04,\n",
       "          2.8230e-04, -2.3335e-03,  3.0210e-03, -3.4489e-03, -4.4532e-03,\n",
       "         -2.8851e-03,  8.4738e-04, -2.0094e-03,  8.5514e-04, -2.9340e-03,\n",
       "          4.8537e-03,  1.5945e-03,  2.8465e-03, -3.1676e-03,  2.5991e-04,\n",
       "         -3.8240e-03, -7.6904e-04,  1.7446e-03,  5.8505e-03,  1.7920e-03,\n",
       "         -3.5166e-04,  2.4621e-03,  2.9776e-03,  9.5379e-04, -9.8259e-05,\n",
       "          4.8596e-03,  5.5630e-03,  5.7233e-03,  1.5060e-03,  5.4012e-03,\n",
       "         -6.0550e-03,  5.0320e-03, -1.2052e-03, -1.7945e-03,  1.0061e-03,\n",
       "         -3.3745e-03,  1.8923e-03, -4.8665e-03, -1.1402e-03, -6.8535e-04,\n",
       "         -1.1727e-03, -7.5948e-03, -7.6251e-03,  8.6082e-04, -4.6195e-03,\n",
       "          6.8948e-04, -3.6911e-03, -1.6678e-03,  7.0128e-04,  1.6133e-04,\n",
       "         -5.3511e-03,  2.9236e-04,  1.1332e-03,  3.1568e-03,  3.9403e-03,\n",
       "         -9.9181e-04, -1.5161e-03,  3.2434e-03, -1.7259e-03, -3.1371e-03,\n",
       "         -5.6555e-03, -7.0580e-04,  4.9600e-03, -6.1876e-04, -2.3836e-03,\n",
       "          3.8235e-04, -2.2082e-03,  1.7191e-03,  3.8632e-03, -1.3177e-03,\n",
       "          2.8293e-03,  1.8383e-03,  3.0369e-03,  3.8432e-03,  2.0557e-03,\n",
       "          2.4048e-03,  2.9858e-03, -4.7052e-04, -1.6441e-03,  5.0663e-04,\n",
       "         -2.7943e-03,  4.6830e-03, -7.8149e-04,  4.9714e-03, -1.1554e-03,\n",
       "          5.4793e-03,  3.8830e-04, -2.1952e-03, -1.9106e-03,  4.2166e-03,\n",
       "         -7.9211e-03,  2.1903e-03, -2.3212e-03, -2.1264e-06,  3.5347e-03,\n",
       "          3.8029e-03,  4.3493e-03, -1.2079e-03, -4.0800e-03, -6.1631e-04,\n",
       "         -3.0119e-03,  7.2832e-04, -4.9819e-03,  4.3098e-03, -1.0447e-03,\n",
       "         -1.5190e-03, -1.1614e-04, -2.5851e-03, -5.6223e-04, -4.9063e-04,\n",
       "          4.5372e-03,  4.2080e-03, -6.6868e-05, -3.0418e-03,  5.1526e-04,\n",
       "          2.0349e-04, -4.3987e-03,  2.2547e-03, -1.1916e-03,  3.2855e-03,\n",
       "         -1.8304e-03,  1.2044e-03,  2.4214e-03, -3.5604e-03, -5.6617e-03,\n",
       "         -1.6140e-03, -3.7594e-03, -5.2860e-03,  5.0630e-03, -4.3036e-03,\n",
       "          4.2658e-03,  3.4950e-05,  5.9622e-03,  1.7839e-03, -8.5128e-03,\n",
       "         -1.3461e-04,  1.0286e-03, -1.1385e-04, -5.4970e-03, -2.5639e-03,\n",
       "         -6.9422e-03,  4.7890e-03, -4.3443e-03,  1.5148e-03,  1.7384e-04,\n",
       "         -2.1194e-03, -4.5452e-03, -2.3099e-03, -1.5242e-04,  4.0474e-03,\n",
       "          7.7698e-04,  2.3903e-03, -2.7853e-03, -1.3553e-04,  1.7910e-04,\n",
       "         -3.6298e-03, -1.7069e-03,  2.4251e-03,  4.1669e-04, -1.0525e-03,\n",
       "         -7.8970e-04, -7.6336e-03, -2.3954e-03, -1.1302e-03,  3.7129e-03,\n",
       "         -7.4790e-03,  4.1602e-03,  6.0730e-03,  1.4657e-04, -7.7727e-04,\n",
       "          2.0437e-03,  3.8930e-03,  6.6083e-03])]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x.grad for x in list(model.encoder.parameters())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.training.trainer import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_options = {\n",
    "    # TODO: Add appropriate learning rate scheduler\n",
    "    \"should_log_parameter_statistics\": True,\n",
    "    \"should_log_learning_rate\": True,\n",
    "    \"patience\": 3,\n",
    "    \"num_epochs\": 10,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    iterator=iterator,\n",
    "    train_dataset=train_ds,\n",
    "    validation_dataset=val_ds,\n",
    "    serialization_dir=DATA_ROOT / \"ckpts\",\n",
    "    **training_options,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/18/2019 10:31:08 - INFO - allennlp.training.trainer -   Beginning training.\n",
      "01/18/2019 10:31:08 - INFO - allennlp.training.trainer -   Epoch 0/9\n",
      "01/18/2019 10:31:08 - INFO - allennlp.training.trainer -   Peak CPU memory usage MB: 1521.72544\n",
      "01/18/2019 10:31:08 - INFO - allennlp.training.trainer -   Training\n",
      "accuracy: 0.7148, loss: 0.5558 ||: 100%|██████████| 109/109 [08:55<00:00,  4.50s/it]\n",
      "01/18/2019 10:40:04 - INFO - allennlp.training.trainer -   Validating\n",
      "accuracy: 0.7890, loss: 0.4587 ||: 100%|██████████| 14/14 [00:56<00:00,  5.05s/it]\n",
      "01/18/2019 10:41:00 - INFO - allennlp.training.trainer -                     Training |  Validation\n",
      "01/18/2019 10:41:00 - INFO - allennlp.training.trainer -   loss          |     0.556  |     0.459\n",
      "01/18/2019 10:41:00 - INFO - allennlp.training.trainer -   accuracy      |     0.715  |     0.789\n",
      "01/18/2019 10:41:00 - INFO - allennlp.training.trainer -   cpu_memory_MB |  1521.725  |       N/A\n",
      "01/18/2019 10:41:00 - INFO - allennlp.training.trainer -   Epoch duration: 00:09:52\n",
      "01/18/2019 10:41:00 - INFO - allennlp.training.trainer -   Estimated training time remaining: 1:28:54\n",
      "01/18/2019 10:41:00 - INFO - allennlp.training.trainer -   Epoch 1/9\n",
      "01/18/2019 10:41:00 - INFO - allennlp.training.trainer -   Peak CPU memory usage MB: 1521.72544\n",
      "01/18/2019 10:41:01 - INFO - allennlp.training.trainer -   Training\n",
      "accuracy: 0.8104, loss: 0.4140 ||: 100%|██████████| 109/109 [08:44<00:00,  5.14s/it]\n",
      "01/18/2019 10:49:45 - INFO - allennlp.training.trainer -   Validating\n",
      "accuracy: 0.8417, loss: 0.3710 ||: 100%|██████████| 14/14 [00:58<00:00,  5.24s/it]\n",
      "01/18/2019 10:50:44 - INFO - allennlp.training.trainer -                     Training |  Validation\n",
      "01/18/2019 10:50:44 - INFO - allennlp.training.trainer -   loss          |     0.414  |     0.371\n",
      "01/18/2019 10:50:44 - INFO - allennlp.training.trainer -   accuracy      |     0.810  |     0.842\n",
      "01/18/2019 10:50:44 - INFO - allennlp.training.trainer -   cpu_memory_MB |  1521.725  |       N/A\n",
      "01/18/2019 10:50:44 - INFO - allennlp.training.trainer -   Epoch duration: 00:09:43\n",
      "01/18/2019 10:50:44 - INFO - allennlp.training.trainer -   Estimated training time remaining: 1:18:26\n",
      "01/18/2019 10:50:44 - INFO - allennlp.training.trainer -   Epoch 2/9\n",
      "01/18/2019 10:50:44 - INFO - allennlp.training.trainer -   Peak CPU memory usage MB: 1521.72544\n",
      "01/18/2019 10:50:44 - INFO - allennlp.training.trainer -   Training\n",
      "accuracy: 0.6250, loss: 0.7782 ||:   1%|          | 1/109 [00:02<03:58,  2.21s/it]"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
