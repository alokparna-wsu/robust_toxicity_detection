{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import *\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from functools import partial\n",
    "from overrides import overrides\n",
    "\n",
    "from allennlp.data import Instance\n",
    "from allennlp.data.fields import TextField, SequenceLabelField, LabelField\n",
    "from allennlp.data.token_indexers import TokenIndexer, SingleIdTokenIndexer\n",
    "from allennlp.data.tokenizers import Token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODOs:\n",
    "\n",
    "- Make compatible with GPU\n",
    "- Try replicating SST results\n",
    "- Write results to MongoDB Atlas\n",
    "- Store weights in s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config(dict):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        for k, v in kwargs.items():\n",
    "            setattr(self, k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# for papermill\n",
    "testing = True\n",
    "seed = 1\n",
    "batch_size = 64\n",
    "embed_dim = 256\n",
    "hidden_sz = 768\n",
    "dataset = \"sst\"\n",
    "n_classes = 2\n",
    "max_seq_len = 128\n",
    "bert_model = \"bert-base-cased\"\n",
    "run_id = \"replicate_0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Can we make this play better with papermill?\n",
    "config = Config(\n",
    "    testing=testing,\n",
    "    seed=seed,\n",
    "    batch_size=batch_size, # This is probably too large: need to handle effective v.s. machine batch size\n",
    "    embed_dim=embed_dim,\n",
    "    hidden_sz=hidden_sz,\n",
    "    dataset=dataset,\n",
    "    n_classes=n_classes,\n",
    "    max_seq_len=max_seq_len, # necessary to limit memory usage\n",
    "#     bert_model=None,\n",
    "    bert_model=bert_model,\n",
    "    run_id=run_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "now = datetime.datetime.now()\n",
    "RUN_ID = config.run_id if config.run_id is not None else now.strftime(\"%m_%d_%H:%M:%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = Path(\"../data\") / config.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set random seed manually to replicate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x119502e50>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(config.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.data.vocabulary import Vocabulary\n",
    "from allennlp.data.dataset_readers import DatasetReader, StanfordSentimentTreeBankDatasetReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader_registry = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def register(name: str):\n",
    "    def dec(x: Callable):\n",
    "        reader_registry[name] = x\n",
    "        return x\n",
    "    return dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@register(\"jigsaw\")\n",
    "class JigsawDatasetReader(DatasetReader):\n",
    "    def __init__(self, tokenizer: Callable[[str], List[str]]=lambda x: x.split(),\n",
    "                 token_indexers: Dict[str, TokenIndexer] = None, # TODO: Handle mapping from BERT\n",
    "                 max_seq_len: Optional[int]=config.max_seq_len) -> None:\n",
    "        super().__init__(lazy=False)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.token_indexers = token_indexers or {\"tokens\": SingleIdTokenIndexer()}\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "    @overrides\n",
    "    def text_to_instance(self, tokens: List[Token], label: str = None) -> Instance:\n",
    "        # TODO: Reimplement\n",
    "        sentence_field = TextField(tokens, self.token_indexers)\n",
    "        fields = {\"tokens\": sentence_field}\n",
    "\n",
    "        label_field = LabelField(label=label, skip_indexing=True)\n",
    "        fields[\"label\"] = label_field\n",
    "\n",
    "        return Instance(fields)\n",
    "    \n",
    "    @overrides\n",
    "    def _read(self, file_path: str) -> Iterator[Instance]:\n",
    "        df = pd.read_csv(file_path)\n",
    "        if config.testing: df = df.head(10000)\n",
    "        for i, row in df.iterrows():\n",
    "            yield self.text_to_instance(\n",
    "                [Token(x) for x in self.tokenizer(row[\"comment_text\"])],\n",
    "                row[\"toxic\"]\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "@register(\"imdb\")\n",
    "class IMDBDatasetReader(DatasetReader):\n",
    "    def __init__(self, tokenizer=None, \n",
    "                 token_indexers: Dict[str, TokenIndexer] = None,\n",
    "                 max_seq_len=None) -> None:\n",
    "        super().__init__(lazy=False)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.token_indexers = token_indexers or {\"tokens\": SingleIdTokenIndexer()}\n",
    "        self.max_seq_len = max_seq_len\n",
    "    \n",
    "    @overrides\n",
    "    def text_to_instance(self, tokens: List[Token], label: str = None) -> Instance:\n",
    "        sentence_field = TextField(tokens, self.token_indexers)\n",
    "        fields = {\"tokens\": sentence_field}\n",
    "        \n",
    "        # TODO: Add statistical features?\n",
    "\n",
    "        label_field = LabelField(label=label)\n",
    "        fields[\"label\"] = label_field\n",
    "\n",
    "        return Instance(fields)\n",
    "\n",
    "    @overrides\n",
    "    def _read(self, file_path: str) -> Iterator[Instance]:\n",
    "        # TODO: Implement\n",
    "        for label in [\"pos\", \"neg\"]:\n",
    "            for file in (Path(file_path) / label).glob(\"*.txt\"):\n",
    "                text = file.open(\"rt\", encoding=\"utf-8\").read()\n",
    "                yield self.text_to_instance([Token(word) for word in self.tokenizer(text)], \n",
    "                                            label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "@register(\"sst\")\n",
    "class SSTDatasetReader(StanfordSentimentTreeBankDatasetReader):\n",
    "    def __init__(self, *args, tokenizer=None, **kwargs):\n",
    "        super().__init__(*args, granularity=f\"{config.n_classes}-class\", **kwargs)\n",
    "        self._tokenizer = tokenizer\n",
    "        \n",
    "    @overrides\n",
    "    def text_to_instance(self, tokens: List[str], sentiment: str=None) -> Instance:\n",
    "        \"\"\"\n",
    "        Forcibly re-tokenize the input to be wordpiece tokenized\n",
    "        \"\"\"\n",
    "        tokens = self._tokenizer(\" \".join(tokens))\n",
    "        return super().text_to_instance(tokens, sentiment=sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare token handlers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.data.token_indexers import WordpieceIndexer, SingleIdTokenIndexer\n",
    "from pytorch_pretrained_bert.tokenization import BertTokenizer\n",
    "\n",
    "class BertIndexerCustom(WordpieceIndexer):\n",
    "    \"\"\"\n",
    "    Virtually the same as PretrainedWordIndexer, except exposes more options.\n",
    "    \"\"\"\n",
    "    def __init__(self, pretrained_model: str,\n",
    "                 use_starting_offsets: bool = False,\n",
    "                 do_lowercase: bool = True,\n",
    "                 never_lowercase: List[str] = None,\n",
    "                 max_pieces: int = 512,\n",
    "                 start_tokens=[\"[CLS]\"],\n",
    "                 end_tokens=[\"[SEP]\"]) -> None:\n",
    "        assert not (pretrained_model.endswith(\"-cased\") and do_lowercase)\n",
    "        assert not (pretrained_model.endswith(\"-uncased\") and not do_lowercase)\n",
    "        bert_tokenizer = BertTokenizer.from_pretrained(pretrained_model,\n",
    "                                                       do_lower_case=do_lowercase)\n",
    "        super().__init__(vocab=bert_tokenizer.vocab,\n",
    "                         wordpiece_tokenizer=bert_tokenizer.wordpiece_tokenizer.tokenize,\n",
    "                         namespace=\"bert\",\n",
    "                         use_starting_offsets=use_starting_offsets,\n",
    "                         max_pieces=max_pieces,\n",
    "                         do_lowercase=do_lowercase,\n",
    "                         never_lowercase=never_lowercase,\n",
    "                         start_tokens=start_tokens,\n",
    "                         end_tokens=end_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/18/2019 15:19:20 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt from cache at /Users/keitakurita/.pytorch_pretrained_bert/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1\n"
     ]
    }
   ],
   "source": [
    "if config.bert_model is not None:\n",
    "    token_indexer = BertIndexerCustom(\n",
    "        pretrained_model=config.bert_model,\n",
    "        max_pieces=config.max_seq_len,\n",
    "        do_lowercase=\"uncased\" in config.bert_model,\n",
    "     )\n",
    "    # apparently we need to truncate the sequence here, which is a stupid design decision\n",
    "    def tokenizer(s: str):\n",
    "        return token_indexer.wordpiece_tokenizer(s)[:config.max_seq_len - 2]\n",
    "else:\n",
    "    token_indexer = SingleIdTokenIndexer(\n",
    "        lowercase_tokens=False,  # don't lowercase by default\n",
    "    )\n",
    "    tokenizer = lambda x: x.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader_cls = reader_registry[config.dataset]\n",
    "reader = reader_cls(tokenizer=tokenizer,\n",
    "                    token_indexers={\"tokens\": token_indexer})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]01/18/2019 15:19:21 - INFO - allennlp.data.dataset_readers.stanford_sentiment_tree_bank -   Reading instances from lines in file at: ../data/sst/trees/train.txt\n",
      "6920it [00:02, 2378.88it/s]\n",
      "0it [00:00, ?it/s]01/18/2019 15:19:23 - INFO - allennlp.data.dataset_readers.stanford_sentiment_tree_bank -   Reading instances from lines in file at: ../data/sst/trees/dev.txt\n",
      "872it [00:00, 2349.07it/s]\n",
      "0it [00:00, ?it/s]01/18/2019 15:19:24 - INFO - allennlp.data.dataset_readers.stanford_sentiment_tree_bank -   Reading instances from lines in file at: ../data/sst/trees/test.txt\n",
      "1821it [00:00, 2850.56it/s]\n"
     ]
    }
   ],
   "source": [
    "if config.dataset == \"imdb\":\n",
    "    data_dir = DATA_ROOT / \"imdb\" / \"aclImdb\"\n",
    "    train_ds, test_ds = (reader.read(data_dir / fname) for fname in [\"train\", \"test\"])\n",
    "    val_ds = None\n",
    "elif config.dataset == \"sst\":\n",
    "    data_dir = DATA_ROOT / \"trees\"\n",
    "    train_ds, val_ds, test_ds = (reader.read(data_dir / fname) for fname in [\"train.txt\", \"dev.txt\", \"test.txt\"])\n",
    "else:\n",
    "    train_ds, test_ds = (reader.read(DATA_ROOT / fname) for fname in [\"train.csv\", \"test_proced.csv\"])\n",
    "    val_ds = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6920, 1821)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_ds), len(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/18/2019 15:19:24 - INFO - allennlp.data.vocabulary -   Fitting token dictionary from dataset.\n",
      "100%|██████████| 6920/6920 [00:00<00:00, 120423.46it/s]\n"
     ]
    }
   ],
   "source": [
    "vocab = Vocabulary.from_instances(train_ds)\n",
    "if config.bert_model is not None: \n",
    "    token_indexer._add_encoding_to_vocabulary(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.training.metrics import CategoricalAccuracy\n",
    "from allennlp.data.iterators import BucketIterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Allow for customization\n",
    "iterator = BucketIterator(batch_size=config.batch_size, \n",
    "                          biggest_batch_first=True,\n",
    "                          sorting_keys=[(\"tokens\", \"num_tokens\")],)\n",
    "iterator.index_with(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    batch = next(iter(iterator(train_ds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': {'tokens': tensor([[  101,   169,  2131, 18757,  1513,   112,  1116,  8447,   118, 26161,\n",
       "            2064, 28137,  1110,   118,  2069, 22672, 28137,   170,  5439, 14439,\n",
       "             176, 20901,  1162, 28137,  8508,  5498,  1114,   170,  1716,  2572,\n",
       "            1176,   144, 16724,  9654,  1107,   170,  3223, 28137, 19972,  8967,\n",
       "            1105,   170,  2566,  1112,  2430,  6775,  1158,  1884,  1197, 28137,\n",
       "            1830, 24891,  2254, 28137,  7535,  1964, 28137,  1161, 28137,  7641,\n",
       "            2158, 11012,  4695,  9603,   119,   112,   102,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0],\n",
       "          [  101,   142, 28138,  1942, 28138,  1759,  1272,  1157, 22593,  6639,\n",
       "            2953, 12788,  1158,  3981,  1116,   117,  1489, 28137,  4980,  1813,\n",
       "           28137, 11015,  1823,  6603,  2249, 23825,  1633,   117,   127, 28137,\n",
       "            4980,  1813, 28137, 11015,  8633,  5631,  4982,  1105,  1275, 28137,\n",
       "            4980,  1813, 28137, 11015,  1985,  1819,   117,  7627,  1366,  1104,\n",
       "            1103,  3796,  1104,  1103, 10228,   117,   192,  3708,  3540, 11972,\n",
       "            1121,   170,  1677,  7138,  5015,   119,   102,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0],\n",
       "          [  101,  1332,  1122,   112,  1116,  1136,  2095, 13621,  1107, 16358,\n",
       "           26654,  1348,  1143,  2858,  7412,  1918,   117,   169, 28152,  5230,\n",
       "            2453,  4373,   140,  2149,  5710,   112, 28131,  1110,   170,  4105,\n",
       "             117,  7345,   117,  1105, 24815,  3789, 28137,  7412,  1918,  1164,\n",
       "             170,  1685,  1590,  1150,  3349,  1242,  1614,  1107,  1297,   117,\n",
       "            1133, 10434,  1131,   112,  2339,  1561,  1123,  1534,  1196,  1131,\n",
       "            3370,  1106, 14414,  1123,  6149,   119,   102,     0,     0,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0],\n",
       "          [  101,   118, 26161,  2064, 28137,   140,  2858, 18066,   112,  1116,\n",
       "             118,  2069, 22672, 28137,  1963,  1169,  1129,  4806,  1104,  1217,\n",
       "             170,  2113,  5576,  1548,  6617,  1643, 14523,   117,  1133,  1122,\n",
       "            1144,   170, 15194,   117,  1228, 14262,  2305,  1104,  1947,  1105,\n",
       "            8594,  1115,  5401,  1119,  1108,  4401,  1118,  1199,  1104,  1103,\n",
       "           18992,  1150,  1138,  2002,  1140,   117,  2108,  1103,  3291,  1424,\n",
       "            3330,  1105,  6536,  1573,  2692,  2953,  1324,   119,   102,     0,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0],\n",
       "          [  101,  1875,  2881,   112,  1116,   184,  2007,  1106, 27629, 27102,\n",
       "            1297,   112,  1116,  4608,  1880,  1110,   170, 26084,  6647,  1105,\n",
       "            1107,  2528, 12807,  2227, 23487,  1186,  1164,  1103, 14673,  1757,\n",
       "            1104,  8366,  1348, 10116,  1232,  1130,   153, 14089,  2217,  1104,\n",
       "            2185,  2606,   170, 11925, 10771,  1361,  1105,  3073,  5208,  3121,\n",
       "            2285,  1322, 26687,  1115,   112,  1116,  8362, 14467,  6697,  1174,\n",
       "            1105, 21359, 24348,  1193,  4252,  2225,  3365,  3798,   119,   102,\n",
       "               0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0],\n",
       "          [  101,  1247,  1132,   183, 28131,  1204,  1315,  1242,  2441,  1115,\n",
       "            1169,  1129,  1112,  7344,  6276,   117,  1228, 14262,  1105,  1762,\n",
       "           18900,  1158,   118, 26161,  2064, 28137,  1443,   170,  3528,   188,\n",
       "           18208, 19386,  1104,  1103,  1301,  1186,   117,  1120,  1655,   118,\n",
       "            2069, 22672, 28137,   117,  1133,   169, 28152,  2896,  1979,   112,\n",
       "           28131,  8701,  1106,  1202,  1155,  1210,  2385,  1218,   117,  1543,\n",
       "            1122,  1141,  1104,  1103,  1214,   112,  1116,  1211, 24815,  6596,\n",
       "             119,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "               0,     0,     0,     0],\n",
       "          [  101,  1232,   188, 15633,  1181,  1114,  8594,   118, 26161,  2064,\n",
       "           28137,   112,   146,  2936, 21785,  3596,  5886,   117,   112, 19961,\n",
       "            1116, 14760, 13200,  1643,  1170,   170,  1897,   117, 14044,   117,\n",
       "             171, 10354,  4999,  3670,  1114,  1126,  8143,  5579,  9332,   118,\n",
       "            2069, 22672, 28137,  1105, 20787,  2340,  1146, 23562,  1116,   118,\n",
       "           26161,  2064, 28137,  4826,   112,  1116, 14247, 10595,  1144,  1151,\n",
       "            2125,  1114, 12556, 15615,  1324,   117,   170, 10509,  8143,  6093,\n",
       "            1150, 27180,  1116,  2490,  1105,  1917,  1213,   118,  2069, 22672,\n",
       "           28137,   102,     0,     0],\n",
       "          [  101,   118, 26161,  2064, 28137,  1392,   118,  2069, 22672, 28137,\n",
       "           16816,  1366,  1293, 13142,  2716,   183,  8734,  9650,   170,  1823,\n",
       "            3177, 27453,  2180,  2099,  1169,  1129,  1165,  1119,  1110,  1136,\n",
       "            1167, 23284,  1193,  4349,  1107,  1103, 10499,  2008,  2191, 28137,\n",
       "            8766,  4578,  5332,  1104,   169,  9954,  1193,  3171,  1188,   112,\n",
       "             118, 26161,  2064, 28137,  1729,   118,  2069, 22672, 28137,  1105,\n",
       "             169,  9954,  1193,  3171,  1337,   117,   112,  5163,   118, 26161,\n",
       "            2064, 28137,  1137,  4963,   118,  2069, 22672, 28137,  1111,  1224,\n",
       "            1142,  1214,   119,   102]]),\n",
       "  'tokens-offsets': tensor([[ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18,\n",
       "           19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36,\n",
       "           37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54,\n",
       "           55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65,  0,  0,  0,  0,  0,  0,  0,\n",
       "            0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "          [ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18,\n",
       "           19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36,\n",
       "           37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54,\n",
       "           55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65,  0,  0,  0,  0,  0,  0,  0,\n",
       "            0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "          [ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18,\n",
       "           19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36,\n",
       "           37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54,\n",
       "           55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65,  0,  0,  0,  0,  0,  0,  0,\n",
       "            0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "          [ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18,\n",
       "           19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36,\n",
       "           37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54,\n",
       "           55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,  0,  0,  0,  0,  0,\n",
       "            0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "          [ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18,\n",
       "           19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36,\n",
       "           37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54,\n",
       "           55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68,  0,  0,  0,  0,\n",
       "            0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "          [ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18,\n",
       "           19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36,\n",
       "           37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54,\n",
       "           55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70,  0,  0,\n",
       "            0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "          [ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18,\n",
       "           19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36,\n",
       "           37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54,\n",
       "           55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72,\n",
       "           73, 74, 75, 76, 77, 78, 79, 80,  0,  0],\n",
       "          [ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18,\n",
       "           19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36,\n",
       "           37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54,\n",
       "           55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72,\n",
       "           73, 74, 75, 76, 77, 78, 79, 80, 81, 82]]),\n",
       "  'mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "           0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
       "          [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "           1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])},\n",
       " 'label': tensor([1, 0, 0, 0, 1, 0, 0, 0])}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  101,   169,  2131, 18757,  1513,   112,  1116,  8447,   118, 26161,\n",
       "          2064, 28137,  1110,   118,  2069, 22672, 28137,   170,  5439, 14439,\n",
       "           176, 20901,  1162, 28137,  8508,  5498,  1114,   170,  1716,  2572,\n",
       "          1176,   144, 16724,  9654,  1107,   170,  3223, 28137, 19972,  8967,\n",
       "          1105,   170,  2566,  1112,  2430,  6775,  1158,  1884,  1197, 28137,\n",
       "          1830, 24891,  2254, 28137,  7535,  1964, 28137,  1161, 28137,  7641,\n",
       "          2158, 11012,  4695,  9603,   119,   112,   102,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0],\n",
       "        [  101,   142, 28138,  1942, 28138,  1759,  1272,  1157, 22593,  6639,\n",
       "          2953, 12788,  1158,  3981,  1116,   117,  1489, 28137,  4980,  1813,\n",
       "         28137, 11015,  1823,  6603,  2249, 23825,  1633,   117,   127, 28137,\n",
       "          4980,  1813, 28137, 11015,  8633,  5631,  4982,  1105,  1275, 28137,\n",
       "          4980,  1813, 28137, 11015,  1985,  1819,   117,  7627,  1366,  1104,\n",
       "          1103,  3796,  1104,  1103, 10228,   117,   192,  3708,  3540, 11972,\n",
       "          1121,   170,  1677,  7138,  5015,   119,   102,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0],\n",
       "        [  101,  1332,  1122,   112,  1116,  1136,  2095, 13621,  1107, 16358,\n",
       "         26654,  1348,  1143,  2858,  7412,  1918,   117,   169, 28152,  5230,\n",
       "          2453,  4373,   140,  2149,  5710,   112, 28131,  1110,   170,  4105,\n",
       "           117,  7345,   117,  1105, 24815,  3789, 28137,  7412,  1918,  1164,\n",
       "           170,  1685,  1590,  1150,  3349,  1242,  1614,  1107,  1297,   117,\n",
       "          1133, 10434,  1131,   112,  2339,  1561,  1123,  1534,  1196,  1131,\n",
       "          3370,  1106, 14414,  1123,  6149,   119,   102,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0],\n",
       "        [  101,   118, 26161,  2064, 28137,   140,  2858, 18066,   112,  1116,\n",
       "           118,  2069, 22672, 28137,  1963,  1169,  1129,  4806,  1104,  1217,\n",
       "           170,  2113,  5576,  1548,  6617,  1643, 14523,   117,  1133,  1122,\n",
       "          1144,   170, 15194,   117,  1228, 14262,  2305,  1104,  1947,  1105,\n",
       "          8594,  1115,  5401,  1119,  1108,  4401,  1118,  1199,  1104,  1103,\n",
       "         18992,  1150,  1138,  2002,  1140,   117,  2108,  1103,  3291,  1424,\n",
       "          3330,  1105,  6536,  1573,  2692,  2953,  1324,   119,   102,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0],\n",
       "        [  101,  1875,  2881,   112,  1116,   184,  2007,  1106, 27629, 27102,\n",
       "          1297,   112,  1116,  4608,  1880,  1110,   170, 26084,  6647,  1105,\n",
       "          1107,  2528, 12807,  2227, 23487,  1186,  1164,  1103, 14673,  1757,\n",
       "          1104,  8366,  1348, 10116,  1232,  1130,   153, 14089,  2217,  1104,\n",
       "          2185,  2606,   170, 11925, 10771,  1361,  1105,  3073,  5208,  3121,\n",
       "          2285,  1322, 26687,  1115,   112,  1116,  8362, 14467,  6697,  1174,\n",
       "          1105, 21359, 24348,  1193,  4252,  2225,  3365,  3798,   119,   102,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0],\n",
       "        [  101,  1247,  1132,   183, 28131,  1204,  1315,  1242,  2441,  1115,\n",
       "          1169,  1129,  1112,  7344,  6276,   117,  1228, 14262,  1105,  1762,\n",
       "         18900,  1158,   118, 26161,  2064, 28137,  1443,   170,  3528,   188,\n",
       "         18208, 19386,  1104,  1103,  1301,  1186,   117,  1120,  1655,   118,\n",
       "          2069, 22672, 28137,   117,  1133,   169, 28152,  2896,  1979,   112,\n",
       "         28131,  8701,  1106,  1202,  1155,  1210,  2385,  1218,   117,  1543,\n",
       "          1122,  1141,  1104,  1103,  1214,   112,  1116,  1211, 24815,  6596,\n",
       "           119,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0],\n",
       "        [  101,  1232,   188, 15633,  1181,  1114,  8594,   118, 26161,  2064,\n",
       "         28137,   112,   146,  2936, 21785,  3596,  5886,   117,   112, 19961,\n",
       "          1116, 14760, 13200,  1643,  1170,   170,  1897,   117, 14044,   117,\n",
       "           171, 10354,  4999,  3670,  1114,  1126,  8143,  5579,  9332,   118,\n",
       "          2069, 22672, 28137,  1105, 20787,  2340,  1146, 23562,  1116,   118,\n",
       "         26161,  2064, 28137,  4826,   112,  1116, 14247, 10595,  1144,  1151,\n",
       "          2125,  1114, 12556, 15615,  1324,   117,   170, 10509,  8143,  6093,\n",
       "          1150, 27180,  1116,  2490,  1105,  1917,  1213,   118,  2069, 22672,\n",
       "         28137,   102,     0,     0],\n",
       "        [  101,   118, 26161,  2064, 28137,  1392,   118,  2069, 22672, 28137,\n",
       "         16816,  1366,  1293, 13142,  2716,   183,  8734,  9650,   170,  1823,\n",
       "          3177, 27453,  2180,  2099,  1169,  1129,  1165,  1119,  1110,  1136,\n",
       "          1167, 23284,  1193,  4349,  1107,  1103, 10499,  2008,  2191, 28137,\n",
       "          8766,  4578,  5332,  1104,   169,  9954,  1193,  3171,  1188,   112,\n",
       "           118, 26161,  2064, 28137,  1729,   118,  2069, 22672, 28137,  1105,\n",
       "           169,  9954,  1193,  3171,  1337,   117,   112,  5163,   118, 26161,\n",
       "          2064, 28137,  1137,  4963,   118,  2069, 22672, 28137,  1111,  1224,\n",
       "          1142,  1214,   119,   102]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"tokens\"][\"tokens\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 84])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"tokens\"][\"tokens\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.models import Model\n",
    "from allennlp.modules.text_field_embedders import TextFieldEmbedder, BasicTextFieldEmbedder\n",
    "from allennlp.modules.token_embedders import Embedding\n",
    "from allennlp.modules.token_embedders.bert_token_embedder import BertEmbedder, PretrainedBertEmbedder\n",
    "from allennlp.modules.seq2vec_encoders import Seq2VecEncoder, PytorchSeq2VecWrapper\n",
    "from allennlp.modules.stacked_bidirectional_lstm import StackedBidirectionalLstm\n",
    "from allennlp.nn.util import get_text_field_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LstmEncoder(nn.Module):\n",
    "    def __init__(self, lstm):\n",
    "        super().__init__()\n",
    "        self.lstm = lstm\n",
    "        \n",
    "    def forward(self, x, mask): # TODO: replace with allennlp built in modules\n",
    "        _, (state, _) = self.lstm(x)\n",
    "        state = torch.cat([state[0, :, :], state[1, :, :]], dim=1)\n",
    "        return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertPooler(nn.Module):\n",
    "    \"\"\"Source code copied\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(768, 768)\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "    def forward(self, hidden_states, mask):\n",
    "        # We \"pool\" the model by simply taking the hidden state corresponding\n",
    "        # to the first token.\n",
    "        first_token_tensor = hidden_states[:, 0]\n",
    "        pooled_output = self.dense(first_token_tensor)\n",
    "        pooled_output = self.activation(pooled_output)\n",
    "        return pooled_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentAnalysisModel(Model):\n",
    "    def __init__(self, word_embeddings: TextFieldEmbedder,\n",
    "                 encoder: StackedBidirectionalLstm,\n",
    "                 out_sz=config.n_classes):\n",
    "        super().__init__(vocab)\n",
    "        self.word_embeddings = word_embeddings\n",
    "        self.encoder = encoder\n",
    "#         self.projection = nn.Linear(encoder.get_output_dim(), out_sz)\n",
    "        self.projection = nn.Linear(config.hidden_sz, out_sz)\n",
    "        self.accuracy = CategoricalAccuracy()\n",
    "        \n",
    "    def forward(self,\n",
    "                tokens: Dict[str, torch.Tensor],\n",
    "                label: torch.Tensor = None) -> torch.Tensor:\n",
    "        mask = get_text_field_mask(tokens)\n",
    "        embeddings = self.word_embeddings(tokens[\"tokens\"])\n",
    "        state = self.encoder(embeddings, mask)\n",
    "\n",
    "        class_logits = self.projection(state)\n",
    "        \n",
    "        output = {\"class_logits\": class_logits}\n",
    "        if label is not None:\n",
    "            output[\"accuracy\"] = self.accuracy(class_logits, label, None)\n",
    "            output[\"loss\"] = nn.CrossEntropyLoss()(class_logits, label)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def get_metrics(self, reset: bool = False) -> Dict[str, float]:\n",
    "        return {\"accuracy\": self.accuracy.get_metric(reset)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/18/2019 15:19:26 - INFO - pytorch_pretrained_bert.modeling -   loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased.tar.gz from cache at /Users/keitakurita/.pytorch_pretrained_bert/a803ce83ca27fecf74c355673c434e51c265fb8a3e0e57ac62a80e38ba98d384.681017f415dfb33ec8d0e04fe51a619f3f01532ecea04edbfd48c5d160550d9c\n",
      "01/18/2019 15:19:26 - INFO - pytorch_pretrained_bert.modeling -   extracting archive file /Users/keitakurita/.pytorch_pretrained_bert/a803ce83ca27fecf74c355673c434e51c265fb8a3e0e57ac62a80e38ba98d384.681017f415dfb33ec8d0e04fe51a619f3f01532ecea04edbfd48c5d160550d9c to temp dir /var/folders/hy/1czs1y5j2d58zgkqx6w_wnpw0000gn/T/tmp1vlo11x0\n",
      "01/18/2019 15:19:30 - INFO - pytorch_pretrained_bert.modeling -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if config.bert_model is None:\n",
    "    token_embedding = Embedding(num_embeddings=vocab.get_vocab_size('tokens'),\n",
    "                                embedding_dim=config.embed_dim)\n",
    "    word_embeddings = BasicTextFieldEmbedder({\"tokens\": token_embedding})\n",
    "\n",
    "    # encoder = PytorchSeq2VecWrapper(nn.LSTM(config.embed_dim, config.hidden_sz, batch_first=True,\n",
    "    #                                         bidirectional=True))\n",
    "    encoder = LstmEncoder(\n",
    "        nn.LSTM(config.embed_dim, config.hidden_sz, batch_first=True, bidirectional=True)\n",
    "    )\n",
    "\n",
    "else:\n",
    "    word_embeddings = PretrainedBertEmbedder(\n",
    "        pretrained_model=config.bert_model,\n",
    "        top_layer_only=True, # conserve memory\n",
    "    )\n",
    "    encoder = BertPooler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentimentAnalysisModel(\n",
    "    word_embeddings, \n",
    "    encoder, \n",
    "    out_sz=config.n_classes,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentimentAnalysisModel(\n",
       "  (word_embeddings): PretrainedBertEmbedder(\n",
       "    (bert_model): BertModel(\n",
       "      (embeddings): BertEmbeddings(\n",
       "        (word_embeddings): Embedding(28996, 768)\n",
       "        (position_embeddings): Embedding(512, 768)\n",
       "        (token_type_embeddings): Embedding(2, 768)\n",
       "        (LayerNorm): BertLayerNorm()\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "      (encoder): BertEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): BertLayerNorm()\n",
       "                (dropout): Dropout(p=0.1)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (1): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): BertLayerNorm()\n",
       "                (dropout): Dropout(p=0.1)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (2): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): BertLayerNorm()\n",
       "                (dropout): Dropout(p=0.1)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (3): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): BertLayerNorm()\n",
       "                (dropout): Dropout(p=0.1)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (4): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): BertLayerNorm()\n",
       "                (dropout): Dropout(p=0.1)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (5): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): BertLayerNorm()\n",
       "                (dropout): Dropout(p=0.1)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (6): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): BertLayerNorm()\n",
       "                (dropout): Dropout(p=0.1)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (7): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): BertLayerNorm()\n",
       "                (dropout): Dropout(p=0.1)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (8): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): BertLayerNorm()\n",
       "                (dropout): Dropout(p=0.1)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (9): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): BertLayerNorm()\n",
       "                (dropout): Dropout(p=0.1)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (10): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): BertLayerNorm()\n",
       "                (dropout): Dropout(p=0.1)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (11): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): BertLayerNorm()\n",
       "                (dropout): Dropout(p=0.1)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (pooler): BertPooler(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (activation): Tanh()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (encoder): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       "  (projection): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic sanity checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.isnan(list(model.word_embeddings.parameters())[0].detach().numpy()).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[False, False]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[np.isnan(x.detach().numpy()).any() for x in list(model.encoder.parameters())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[False, False]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[np.isinf(x.detach().numpy()).any() for x in list(model.encoder.parameters())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2845,  0.5205, -0.2903,  ..., -0.6036, -0.2421, -0.0638],\n",
       "        [-0.2780,  0.4673, -0.3143,  ..., -0.6079, -0.1663, -0.1194],\n",
       "        [-0.2507,  0.3541, -0.3031,  ..., -0.6354, -0.2382, -0.0679],\n",
       "        ...,\n",
       "        [-0.1060,  0.4919, -0.3137,  ..., -0.7491, -0.2599,  0.0784],\n",
       "        [ 0.2190,  0.4185, -0.3076,  ..., -0.4171, -0.1773,  0.1692],\n",
       "        [-0.1925,  0.2830, -0.1771,  ..., -0.5368, -0.2289, -0.0171]],\n",
       "       grad_fn=<TanhBackward>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = batch[\"tokens\"]\n",
    "encoder(model.word_embeddings(tokens[\"tokens\"]), get_text_field_mask(tokens))\n",
    "# encoder(model.word_embeddings(tokens[\"tokens\"]))[1][0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = model(**batch)[\"loss\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7603, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.CrossEntropyLoss()(model(**batch)[\"class_logits\"][:10, :], batch[\"label\"][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7166, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[ 0.0026, -0.0017, -0.0017,  ..., -0.0019,  0.0011,  0.0007],\n",
       "         [ 0.0004, -0.0003, -0.0003,  ..., -0.0003,  0.0002,  0.0001],\n",
       "         [-0.0049,  0.0032,  0.0032,  ...,  0.0035, -0.0020, -0.0014],\n",
       "         ...,\n",
       "         [-0.0017,  0.0011,  0.0011,  ...,  0.0013, -0.0007, -0.0005],\n",
       "         [-0.0005,  0.0003,  0.0003,  ...,  0.0004, -0.0002, -0.0001],\n",
       "         [-0.0043,  0.0028,  0.0028,  ...,  0.0032, -0.0019, -0.0011]]),\n",
       " tensor([ 8.0603e-03,  1.3489e-03, -1.5124e-02,  2.5406e-03, -1.3403e-02,\n",
       "         -5.6139e-03, -1.1616e-02,  7.7399e-03,  1.2475e-02,  7.3855e-03,\n",
       "         -2.7034e-03,  1.4998e-03,  6.3677e-03,  7.0324e-03, -1.0463e-02,\n",
       "          6.5658e-03,  9.1537e-04,  5.8868e-03, -4.9838e-04,  1.1895e-03,\n",
       "         -1.6767e-02,  8.7264e-04,  1.0641e-02,  9.3373e-04, -1.0958e-04,\n",
       "         -2.4146e-03,  1.1429e-02, -2.5398e-03,  6.6740e-03,  8.7339e-03,\n",
       "          1.6180e-03, -3.7155e-04,  6.2519e-03,  6.1777e-03,  1.1338e-03,\n",
       "         -9.2085e-04, -6.2459e-03, -4.9747e-03, -6.5644e-03,  2.9722e-04,\n",
       "         -3.8853e-03,  1.8841e-03,  2.3512e-03,  2.0116e-04,  3.3079e-03,\n",
       "         -8.1463e-04,  1.0614e-02, -2.6385e-03, -1.0039e-03, -3.1404e-03,\n",
       "          7.0449e-03,  9.2928e-04,  1.6352e-02,  7.3425e-04, -5.7184e-03,\n",
       "         -6.5051e-03,  4.7528e-03,  1.3540e-03, -1.2059e-02, -2.7989e-03,\n",
       "         -5.9371e-03, -1.4319e-03, -3.3172e-04, -1.2266e-02, -3.4258e-03,\n",
       "         -4.7362e-03, -2.1041e-03,  7.4947e-04, -1.0856e-03,  3.2286e-03,\n",
       "          7.9034e-03, -1.0112e-02,  1.1612e-03,  7.7176e-03,  1.1874e-02,\n",
       "          3.6389e-03, -7.0960e-03,  8.5924e-03, -5.6152e-03,  2.1010e-03,\n",
       "         -1.7638e-02,  6.5959e-03,  1.3071e-02, -1.3276e-02,  1.1198e-02,\n",
       "         -1.3589e-02,  9.0282e-04, -5.9505e-03, -8.6912e-03, -8.1555e-03,\n",
       "         -1.2485e-03,  1.3318e-03, -2.2829e-03,  9.3107e-03,  1.4707e-03,\n",
       "          2.9642e-03,  3.8206e-03,  3.8117e-04,  3.6958e-03,  4.9353e-03,\n",
       "          8.6335e-03,  1.3265e-03,  7.9656e-03,  1.7577e-03,  1.9445e-03,\n",
       "          8.7220e-04, -8.5016e-04, -1.1452e-03, -5.1568e-03,  5.8089e-03,\n",
       "          7.4641e-03, -2.0203e-03,  5.9792e-03, -1.7105e-02, -2.5993e-03,\n",
       "         -2.8778e-03, -1.7790e-03, -2.3415e-03, -3.3845e-05, -5.6873e-03,\n",
       "          1.4683e-03,  9.2869e-04, -8.1436e-03, -2.6717e-03, -2.0908e-04,\n",
       "         -1.4274e-02,  3.7527e-04, -1.1910e-02,  1.3683e-02,  1.1646e-03,\n",
       "          4.6252e-03,  4.9008e-03, -3.5106e-03,  8.2758e-03,  8.9694e-03,\n",
       "         -6.4882e-04,  2.3321e-03, -7.9440e-03,  9.7297e-04,  1.4497e-02,\n",
       "         -1.6294e-03, -4.3424e-03,  1.3550e-03, -6.1401e-03, -1.2759e-02,\n",
       "         -9.4678e-04, -4.8334e-03, -1.1945e-03, -8.6664e-03, -6.8633e-03,\n",
       "         -1.6738e-02, -9.6453e-03, -2.8908e-03, -1.5561e-03,  1.1221e-02,\n",
       "         -4.9049e-03, -3.8282e-03,  3.3967e-03,  3.1550e-04, -3.2699e-03,\n",
       "          6.3693e-03,  8.7010e-03, -8.2540e-03,  4.3220e-03,  3.2955e-03,\n",
       "          3.9235e-03, -6.5352e-03,  3.5233e-03,  3.2104e-03, -1.5607e-03,\n",
       "         -1.0819e-02,  9.2815e-03, -1.1225e-03,  7.3802e-03,  2.0596e-03,\n",
       "          1.4604e-02,  1.1755e-03,  1.1723e-02,  9.7227e-03, -6.6957e-03,\n",
       "          3.9581e-03,  3.5786e-03,  4.7141e-03,  1.2526e-02,  4.7679e-03,\n",
       "         -1.2307e-04,  5.2926e-03,  1.2690e-02, -1.4087e-02, -3.1455e-03,\n",
       "         -2.4204e-03, -6.3843e-03, -3.4735e-03,  1.2200e-02, -6.3505e-03,\n",
       "         -2.6164e-03, -2.4686e-03, -5.2197e-03, -3.8681e-03, -5.9663e-03,\n",
       "          9.4582e-03, -1.0106e-02,  1.8917e-03,  1.2157e-02,  7.1970e-03,\n",
       "         -1.1798e-02,  7.4103e-04, -1.1410e-02, -8.2246e-03,  4.4772e-03,\n",
       "         -9.3190e-03,  1.6435e-02, -5.1339e-04,  2.8971e-03,  7.1759e-03,\n",
       "         -1.1437e-02,  7.2219e-03,  4.4745e-03, -5.0828e-03, -1.1396e-02,\n",
       "         -6.1655e-03,  1.1310e-03, -1.0384e-03, -9.8004e-03,  6.2744e-03,\n",
       "          1.2386e-02,  2.0376e-03, -8.6435e-03,  1.2745e-02,  7.1476e-03,\n",
       "         -8.0686e-03,  1.2705e-02,  1.1598e-03, -1.3133e-02, -6.1703e-04,\n",
       "          1.2578e-02, -3.8788e-03, -8.5895e-03, -5.1012e-04,  6.8864e-03,\n",
       "          6.2207e-04,  1.1446e-03, -1.1198e-04, -1.2669e-02,  7.1173e-03,\n",
       "         -6.0065e-03,  1.8070e-02, -2.2413e-03,  6.6265e-03, -4.7880e-04,\n",
       "         -1.4571e-02,  9.6385e-03,  1.3287e-02, -1.1798e-02,  1.5624e-04,\n",
       "          4.7657e-03,  6.5172e-03,  4.9963e-03,  1.8086e-03,  1.0134e-02,\n",
       "          9.4911e-03,  5.5449e-03,  3.6084e-03, -1.5820e-03, -8.3179e-03,\n",
       "         -3.7556e-03, -6.2108e-03, -7.8328e-03, -3.1682e-03,  1.2530e-02,\n",
       "          2.4375e-03,  5.3311e-03,  8.2729e-03,  5.7637e-03,  9.5958e-03,\n",
       "         -1.3654e-03, -3.3205e-03, -6.9394e-03,  2.1791e-03,  7.1608e-03,\n",
       "          1.5312e-03,  1.6595e-03,  4.0592e-03,  8.9033e-03, -3.0511e-03,\n",
       "         -7.6513e-03,  9.5745e-03, -9.4751e-03,  5.0401e-03, -4.3063e-03,\n",
       "          5.1539e-04, -2.1540e-03, -3.8607e-03, -5.0798e-03, -4.2793e-03,\n",
       "         -3.3975e-04, -1.0857e-03,  1.1006e-02,  5.2085e-03, -1.0224e-03,\n",
       "         -2.4085e-03,  2.6069e-03,  8.7909e-03,  1.8302e-02,  1.3707e-02,\n",
       "         -6.7424e-03, -4.3459e-03,  1.0454e-02,  1.0012e-02, -1.6993e-02,\n",
       "          1.4266e-04,  5.9894e-04, -1.0171e-02, -1.1562e-02, -6.9141e-03,\n",
       "         -9.7604e-03,  5.8287e-03, -3.4079e-03,  1.0519e-02,  1.2322e-02,\n",
       "          5.3677e-03,  1.6817e-03, -8.0404e-03,  7.3336e-03, -3.3141e-03,\n",
       "         -1.0879e-02,  3.2028e-03, -8.9453e-03, -6.5082e-03, -6.2576e-03,\n",
       "          2.2220e-03,  1.3142e-02,  2.5664e-03, -1.1081e-02, -1.3632e-02,\n",
       "          1.3128e-02,  1.0376e-02, -3.1492e-03,  1.0252e-02, -5.5898e-03,\n",
       "          1.4823e-03, -2.0405e-03,  1.2417e-03, -6.8039e-03, -5.4176e-03,\n",
       "         -1.2096e-02, -2.4515e-03, -1.9136e-02, -1.4755e-03, -1.2720e-02,\n",
       "          2.7994e-03, -1.5910e-02,  1.3002e-03, -1.4413e-03, -8.2949e-03,\n",
       "          1.3813e-02,  9.9814e-03,  3.8032e-03, -3.7984e-03, -9.3492e-03,\n",
       "         -7.9957e-03,  1.1052e-03, -9.3715e-03, -9.9331e-03, -9.0845e-03,\n",
       "         -6.4555e-03, -3.4402e-04, -2.0214e-03, -1.1424e-03, -5.0248e-03,\n",
       "          3.7685e-03,  2.6551e-04,  1.4494e-02, -5.6386e-03,  4.2532e-03,\n",
       "         -6.1565e-03, -1.1936e-02,  2.8461e-03,  6.3319e-03,  1.0553e-03,\n",
       "         -6.2767e-03,  6.9034e-03,  3.1804e-03, -1.5475e-02,  2.9724e-03,\n",
       "         -1.5433e-03,  1.2261e-02,  1.1472e-02, -9.4932e-03, -1.1915e-03,\n",
       "         -6.5154e-03, -4.3506e-03,  4.1503e-03,  1.2763e-02,  1.4084e-02,\n",
       "         -1.5098e-02, -1.0229e-02,  1.4903e-02,  1.3337e-03, -4.9737e-03,\n",
       "         -1.7174e-03, -2.5224e-03,  6.4124e-03,  3.2565e-03,  5.4869e-03,\n",
       "          1.1769e-02,  1.5828e-03, -9.0417e-03,  8.1261e-03,  5.7809e-03,\n",
       "          1.6897e-02, -6.9384e-03,  1.4475e-02, -5.9681e-03, -8.0809e-03,\n",
       "          1.8367e-02,  7.8518e-03, -5.7232e-03, -2.5245e-03, -1.1015e-02,\n",
       "          4.5638e-03, -5.4634e-03,  6.3002e-03,  8.9445e-03,  5.5600e-04,\n",
       "          6.3198e-03,  1.1058e-03,  9.4414e-03,  1.3966e-02,  5.8816e-03,\n",
       "          1.1455e-02,  9.0636e-03, -8.0288e-05, -4.7011e-03,  1.9994e-03,\n",
       "         -1.5658e-03,  6.6350e-03,  8.0762e-03,  4.2770e-04,  1.2181e-02,\n",
       "          2.0065e-03, -2.2867e-03,  4.9777e-03, -8.8398e-03,  1.3723e-02,\n",
       "          4.6619e-03,  3.0017e-03, -8.1464e-05,  8.6195e-03,  3.4713e-04,\n",
       "         -8.5878e-03,  1.3971e-03,  9.7279e-03,  3.6641e-03, -2.4790e-03,\n",
       "          2.3753e-03,  4.0712e-03, -1.0446e-02, -4.8660e-03, -1.4923e-03,\n",
       "         -3.6984e-03, -1.9463e-04,  1.2389e-02, -3.9219e-03, -2.1335e-03,\n",
       "          1.0484e-02, -1.7723e-03, -1.0744e-03,  1.4013e-03,  5.1022e-03,\n",
       "         -1.0246e-02,  8.6961e-03,  1.2648e-02,  4.8893e-03,  1.2791e-02,\n",
       "         -5.7538e-03,  1.1827e-02, -9.4110e-03, -8.9187e-03, -1.4707e-02,\n",
       "          8.1802e-03,  6.2025e-03,  1.1163e-02, -4.5329e-03, -1.4395e-02,\n",
       "         -3.0303e-03,  2.6463e-03, -4.0385e-03,  2.0763e-04, -6.5868e-04,\n",
       "          1.4371e-02,  8.4083e-03, -2.5374e-04, -2.7499e-04, -5.9265e-04,\n",
       "         -2.8626e-03, -3.9726e-03,  1.6139e-02,  7.3533e-03,  1.0341e-02,\n",
       "          5.3358e-03, -7.2533e-03,  4.7724e-03,  1.8777e-02, -9.5019e-03,\n",
       "         -2.2140e-03, -1.2666e-02, -5.1540e-03, -6.8279e-03, -1.5469e-02,\n",
       "         -1.3756e-03,  8.7167e-04, -7.3460e-03,  1.4596e-02, -1.7120e-02,\n",
       "          1.0370e-02, -8.6111e-03, -1.4047e-02, -2.4392e-03, -9.1546e-03,\n",
       "         -1.2165e-04, -6.5753e-03, -8.3423e-04, -4.8245e-03,  8.4074e-03,\n",
       "         -6.6488e-03,  7.4957e-03,  7.5542e-03, -7.7075e-03, -1.9935e-03,\n",
       "         -1.2669e-02,  7.3598e-03, -4.3975e-04,  4.5619e-03, -7.4205e-03,\n",
       "          7.6162e-04,  3.8129e-03, -1.9337e-03, -4.3022e-03, -2.0997e-03,\n",
       "         -6.2276e-04, -2.0538e-03, -1.4086e-02,  8.9169e-03,  9.1041e-04,\n",
       "         -2.8397e-03, -4.6573e-03,  2.9688e-03, -2.9060e-03, -5.1329e-03,\n",
       "          2.0611e-03, -2.5896e-03, -4.1816e-03,  7.0775e-03,  8.1085e-03,\n",
       "         -9.4277e-03,  9.5983e-03,  1.5544e-03, -6.8351e-03,  7.6594e-03,\n",
       "          2.5256e-03,  5.2801e-03, -1.0941e-03,  1.6180e-03, -4.4052e-03,\n",
       "          3.0864e-03, -2.7621e-03,  3.1775e-03, -1.5871e-02,  5.4464e-03,\n",
       "         -1.6604e-02, -7.1785e-03, -1.3044e-03,  9.1860e-04,  1.5471e-03,\n",
       "         -6.6137e-04,  5.0086e-03,  3.4128e-03,  4.3711e-03, -1.4240e-02,\n",
       "         -6.1051e-04, -1.4824e-02,  7.4983e-03, -7.1565e-04, -3.9734e-03,\n",
       "          4.3587e-03,  2.0597e-03, -1.1421e-02,  6.6868e-03,  5.7366e-03,\n",
       "         -7.9400e-03,  4.8835e-03, -1.3444e-03, -4.4534e-04, -8.7181e-03,\n",
       "         -1.0249e-02, -9.7601e-04,  5.7224e-03,  7.4204e-03,  6.8222e-04,\n",
       "         -4.6946e-03,  2.0782e-03,  9.7929e-04,  3.5574e-03, -7.4647e-03,\n",
       "          7.3375e-03, -3.8869e-03,  1.3648e-02,  1.0694e-03, -3.7883e-03,\n",
       "          8.7144e-03,  5.3186e-03, -8.1166e-03,  1.4456e-02,  1.1033e-02,\n",
       "         -9.4927e-03,  1.4418e-02, -1.5358e-03, -8.0710e-03,  6.3026e-04,\n",
       "         -9.9721e-03, -6.8245e-03,  2.9203e-03,  8.0843e-03, -5.4760e-03,\n",
       "          2.9891e-03,  1.7764e-02, -1.4272e-03, -3.3941e-03, -1.1339e-02,\n",
       "          2.6092e-03,  8.9046e-03,  2.6765e-03,  2.4539e-03, -1.0659e-02,\n",
       "         -7.3707e-03, -5.8804e-03, -2.6737e-03,  2.6303e-03, -4.6351e-03,\n",
       "          1.0470e-02,  2.3048e-03,  5.4758e-03,  2.8145e-03,  8.0056e-03,\n",
       "         -3.3917e-03,  9.7997e-04,  8.7241e-03,  4.8754e-03, -1.0847e-02,\n",
       "         -1.0633e-02,  2.4728e-03,  5.3679e-03,  6.9149e-03, -1.0145e-02,\n",
       "          7.7339e-04, -1.1483e-02, -9.2446e-04, -1.2902e-02, -1.8591e-03,\n",
       "         -1.5938e-03,  1.3739e-02,  3.1426e-03, -5.8271e-03, -1.1191e-04,\n",
       "         -6.5262e-03,  1.6329e-02, -6.6660e-03,  9.3215e-05,  7.6533e-03,\n",
       "          7.4767e-03, -6.6515e-03,  3.7874e-03,  4.0628e-03,  1.4293e-03,\n",
       "          2.6632e-03,  1.3887e-02,  1.1186e-04,  2.0191e-03,  3.5432e-03,\n",
       "         -6.1621e-03,  3.8732e-03, -2.0614e-03, -4.6382e-03, -4.6462e-03,\n",
       "          1.0500e-02, -3.3783e-03, -3.6075e-03, -6.5008e-03, -1.2802e-02,\n",
       "         -9.9688e-03,  7.8381e-03, -2.5894e-03, -1.0116e-03, -5.4437e-03,\n",
       "         -6.7322e-03, -4.8049e-04,  1.1211e-02,  1.3657e-02,  6.7601e-03,\n",
       "          8.9962e-03,  4.3533e-04,  2.1009e-03,  3.5461e-03, -2.0611e-03,\n",
       "          6.0966e-03, -8.5711e-04,  1.4445e-02,  8.7687e-03,  3.4435e-03,\n",
       "          4.0547e-03,  9.5866e-03,  9.4592e-04,  6.3066e-03,  5.9712e-03,\n",
       "         -3.0261e-03, -4.3743e-03,  6.8089e-03,  5.2541e-03,  1.1762e-02,\n",
       "         -1.0177e-03, -2.4455e-03, -1.0104e-02, -6.1951e-03, -3.1903e-03,\n",
       "         -1.4160e-02, -4.7013e-03,  1.1337e-02, -4.3111e-03, -9.1092e-03,\n",
       "          1.5376e-03, -7.8709e-03,  1.4935e-02,  1.6602e-02,  6.4227e-03,\n",
       "          1.0610e-02,  4.6943e-03,  4.5870e-04,  1.1215e-03, -1.1132e-03,\n",
       "          1.1251e-02, -1.5179e-02,  6.1755e-04,  1.0071e-04, -5.8752e-04,\n",
       "          3.8016e-03,  1.5262e-02, -4.5267e-03,  7.4518e-03,  1.3193e-02,\n",
       "          9.3880e-03,  6.3069e-03,  6.0684e-03,  5.8092e-03, -3.2816e-03,\n",
       "          9.5968e-03,  3.1728e-03, -1.9093e-03, -4.0010e-03,  1.9260e-03,\n",
       "          7.2664e-03,  1.2015e-02, -7.6073e-03, -1.5549e-03, -1.1238e-02,\n",
       "         -5.3546e-03, -1.6670e-03, -1.3631e-02])]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x.grad for x in list(model.encoder.parameters())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.training.trainer import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_options = {\n",
    "    # TODO: Add appropriate learning rate scheduler\n",
    "    \"should_log_parameter_statistics\": True,\n",
    "    \"should_log_learning_rate\": True,\n",
    "    \"patience\": 3,\n",
    "    \"num_epochs\": 10,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    iterator=iterator,\n",
    "    train_dataset=train_ds,\n",
    "    validation_dataset=val_ds,\n",
    "    serialization_dir=DATA_ROOT / \"ckpts\" / RUN_ID,\n",
    "    **training_options,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/18/2019 15:19:38 - INFO - allennlp.training.trainer -   Beginning training.\n",
      "01/18/2019 15:19:38 - INFO - allennlp.training.trainer -   Epoch 0/9\n",
      "01/18/2019 15:19:38 - INFO - allennlp.training.trainer -   Peak CPU memory usage MB: 1180.868608\n",
      "01/18/2019 15:19:38 - INFO - allennlp.training.trainer -   Training\n",
      "accuracy: 0.7677, loss: 0.4799 ||: 100%|██████████| 109/109 [09:06<00:00,  6.18s/it]\n",
      "01/18/2019 15:28:45 - INFO - allennlp.training.trainer -   Validating\n",
      "accuracy: 0.8360, loss: 0.3880 ||: 100%|██████████| 14/14 [00:57<00:00,  5.25s/it]\n",
      "01/18/2019 15:29:43 - INFO - allennlp.training.trainer -                     Training |  Validation\n",
      "01/18/2019 15:29:43 - INFO - allennlp.training.trainer -   loss          |     0.480  |     0.388\n",
      "01/18/2019 15:29:43 - INFO - allennlp.training.trainer -   accuracy      |     0.768  |     0.836\n",
      "01/18/2019 15:29:43 - INFO - allennlp.training.trainer -   cpu_memory_MB |  1180.869  |       N/A\n",
      "01/18/2019 15:29:44 - INFO - allennlp.training.trainer -   Best validation performance so far. Copying weights to '../data/sst/ckpts/replicate_0/best.th'.\n",
      "01/18/2019 15:29:44 - INFO - allennlp.training.trainer -   Epoch duration: 00:10:05\n",
      "01/18/2019 15:29:44 - INFO - allennlp.training.trainer -   Estimated training time remaining: 1:30:53\n",
      "01/18/2019 15:29:44 - INFO - allennlp.training.trainer -   Epoch 1/9\n",
      "01/18/2019 15:29:44 - INFO - allennlp.training.trainer -   Peak CPU memory usage MB: 1205.592064\n",
      "01/18/2019 15:29:44 - INFO - allennlp.training.trainer -   Training\n",
      "accuracy: 0.8023, loss: 0.4275 ||: 100%|██████████| 109/109 [10:03<00:00,  4.45s/it]\n",
      "01/18/2019 15:39:48 - INFO - allennlp.training.trainer -   Validating\n",
      "accuracy: 0.8360, loss: 0.3762 ||: 100%|██████████| 14/14 [01:02<00:00,  5.47s/it]\n",
      "01/18/2019 15:40:50 - INFO - allennlp.training.trainer -                     Training |  Validation\n",
      "01/18/2019 15:40:50 - INFO - allennlp.training.trainer -   loss          |     0.427  |     0.376\n",
      "01/18/2019 15:40:50 - INFO - allennlp.training.trainer -   accuracy      |     0.802  |     0.836\n",
      "01/18/2019 15:40:50 - INFO - allennlp.training.trainer -   cpu_memory_MB |  1205.592  |       N/A\n",
      "01/18/2019 15:40:50 - INFO - allennlp.training.trainer -   Best validation performance so far. Copying weights to '../data/sst/ckpts/replicate_0/best.th'.\n",
      "01/18/2019 15:40:51 - INFO - allennlp.training.trainer -   Epoch duration: 00:11:06\n",
      "01/18/2019 15:40:51 - INFO - allennlp.training.trainer -   Estimated training time remaining: 1:24:51\n",
      "01/18/2019 15:40:51 - INFO - allennlp.training.trainer -   Epoch 2/9\n",
      "01/18/2019 15:40:51 - INFO - allennlp.training.trainer -   Peak CPU memory usage MB: 1205.592064\n",
      "01/18/2019 15:40:51 - INFO - allennlp.training.trainer -   Training\n",
      "accuracy: 0.8186, loss: 0.3995 ||: 100%|██████████| 109/109 [09:24<00:00,  4.30s/it]\n",
      "01/18/2019 15:50:16 - INFO - allennlp.training.trainer -   Validating\n",
      "accuracy: 0.7993, loss: 0.4332 ||: 100%|██████████| 14/14 [00:55<00:00,  5.23s/it]\n",
      "01/18/2019 15:51:12 - INFO - allennlp.training.trainer -                     Training |  Validation\n",
      "01/18/2019 15:51:12 - INFO - allennlp.training.trainer -   loss          |     0.399  |     0.433\n",
      "01/18/2019 15:51:12 - INFO - allennlp.training.trainer -   accuracy      |     0.819  |     0.799\n",
      "01/18/2019 15:51:12 - INFO - allennlp.training.trainer -   cpu_memory_MB |  1205.592  |       N/A\n",
      "01/18/2019 15:51:13 - INFO - allennlp.training.trainer -   Epoch duration: 00:10:21\n",
      "01/18/2019 15:51:13 - INFO - allennlp.training.trainer -   Estimated training time remaining: 1:13:40\n",
      "01/18/2019 15:51:13 - INFO - allennlp.training.trainer -   Epoch 3/9\n",
      "01/18/2019 15:51:13 - INFO - allennlp.training.trainer -   Peak CPU memory usage MB: 1205.592064\n",
      "01/18/2019 15:51:13 - INFO - allennlp.training.trainer -   Training\n",
      "accuracy: 0.8202, loss: 0.4005 ||: 100%|██████████| 109/109 [08:46<00:00,  3.63s/it]\n",
      "01/18/2019 15:59:59 - INFO - allennlp.training.trainer -   Validating\n",
      "accuracy: 0.7729, loss: 0.4827 ||: 100%|██████████| 14/14 [00:52<00:00,  4.77s/it]\n",
      "01/18/2019 16:00:52 - INFO - allennlp.training.trainer -                     Training |  Validation\n",
      "01/18/2019 16:00:52 - INFO - allennlp.training.trainer -   loss          |     0.400  |     0.483\n",
      "01/18/2019 16:00:52 - INFO - allennlp.training.trainer -   accuracy      |     0.820  |     0.773\n",
      "01/18/2019 16:00:52 - INFO - allennlp.training.trainer -   cpu_memory_MB |  1205.592  |       N/A\n",
      "01/18/2019 16:00:53 - INFO - allennlp.training.trainer -   Epoch duration: 00:09:39\n",
      "01/18/2019 16:00:53 - INFO - allennlp.training.trainer -   Estimated training time remaining: 1:01:51\n",
      "01/18/2019 16:00:53 - INFO - allennlp.training.trainer -   Epoch 4/9\n",
      "01/18/2019 16:00:53 - INFO - allennlp.training.trainer -   Peak CPU memory usage MB: 1205.592064\n",
      "01/18/2019 16:00:53 - INFO - allennlp.training.trainer -   Training\n",
      "accuracy: 0.8228, loss: 0.3973 ||: 100%|██████████| 109/109 [08:19<00:00,  3.90s/it]\n",
      "01/18/2019 16:09:12 - INFO - allennlp.training.trainer -   Validating\n",
      "accuracy: 0.8360, loss: 0.3749 ||: 100%|██████████| 14/14 [00:51<00:00,  4.58s/it]\n",
      "01/18/2019 16:10:04 - INFO - allennlp.training.trainer -                     Training |  Validation\n",
      "01/18/2019 16:10:04 - INFO - allennlp.training.trainer -   loss          |     0.397  |     0.375\n",
      "01/18/2019 16:10:04 - INFO - allennlp.training.trainer -   accuracy      |     0.823  |     0.836\n",
      "01/18/2019 16:10:04 - INFO - allennlp.training.trainer -   cpu_memory_MB |  1205.592  |       N/A\n",
      "01/18/2019 16:10:05 - INFO - allennlp.training.trainer -   Best validation performance so far. Copying weights to '../data/sst/ckpts/replicate_0/best.th'.\n",
      "01/18/2019 16:10:07 - INFO - allennlp.training.trainer -   Epoch duration: 00:09:13\n",
      "01/18/2019 16:10:07 - INFO - allennlp.training.trainer -   Estimated training time remaining: 0:50:28\n",
      "01/18/2019 16:10:07 - INFO - allennlp.training.trainer -   Epoch 5/9\n",
      "01/18/2019 16:10:07 - INFO - allennlp.training.trainer -   Peak CPU memory usage MB: 1205.592064\n",
      "01/18/2019 16:10:07 - INFO - allennlp.training.trainer -   Training\n",
      "accuracy: 0.8212, loss: 0.3898 ||: 100%|██████████| 109/109 [08:10<00:00,  5.25s/it]\n",
      "01/18/2019 16:18:17 - INFO - allennlp.training.trainer -   Validating\n",
      "accuracy: 0.8429, loss: 0.3652 ||: 100%|██████████| 14/14 [00:52<00:00,  4.62s/it]\n",
      "01/18/2019 16:19:09 - INFO - allennlp.training.trainer -                     Training |  Validation\n",
      "01/18/2019 16:19:09 - INFO - allennlp.training.trainer -   loss          |     0.390  |     0.365\n",
      "01/18/2019 16:19:09 - INFO - allennlp.training.trainer -   accuracy      |     0.821  |     0.843\n",
      "01/18/2019 16:19:09 - INFO - allennlp.training.trainer -   cpu_memory_MB |  1205.592  |       N/A\n",
      "01/18/2019 16:19:10 - INFO - allennlp.training.trainer -   Best validation performance so far. Copying weights to '../data/sst/ckpts/replicate_0/best.th'.\n",
      "01/18/2019 16:19:11 - INFO - allennlp.training.trainer -   Epoch duration: 00:09:04\n",
      "01/18/2019 16:19:11 - INFO - allennlp.training.trainer -   Estimated training time remaining: 0:39:41\n",
      "01/18/2019 16:19:11 - INFO - allennlp.training.trainer -   Epoch 6/9\n",
      "01/18/2019 16:19:11 - INFO - allennlp.training.trainer -   Peak CPU memory usage MB: 1224.237056\n",
      "01/18/2019 16:19:11 - INFO - allennlp.training.trainer -   Training\n",
      "accuracy: 0.8223, loss: 0.3872 ||: 100%|██████████| 109/109 [08:22<00:00,  3.22s/it]\n",
      "01/18/2019 16:27:33 - INFO - allennlp.training.trainer -   Validating\n",
      "accuracy: 0.8463, loss: 0.3691 ||: 100%|██████████| 14/14 [00:51<00:00,  4.58s/it]\n",
      "01/18/2019 16:28:25 - INFO - allennlp.training.trainer -                     Training |  Validation\n",
      "01/18/2019 16:28:25 - INFO - allennlp.training.trainer -   loss          |     0.387  |     0.369\n",
      "01/18/2019 16:28:25 - INFO - allennlp.training.trainer -   accuracy      |     0.822  |     0.846\n",
      "01/18/2019 16:28:25 - INFO - allennlp.training.trainer -   cpu_memory_MB |  1224.237  |       N/A\n",
      "01/18/2019 16:28:26 - INFO - allennlp.training.trainer -   Epoch duration: 00:09:15\n",
      "01/18/2019 16:28:26 - INFO - allennlp.training.trainer -   Estimated training time remaining: 0:29:29\n",
      "01/18/2019 16:28:26 - INFO - allennlp.training.trainer -   Epoch 7/9\n",
      "01/18/2019 16:28:26 - INFO - allennlp.training.trainer -   Peak CPU memory usage MB: 1224.269824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/18/2019 16:28:26 - INFO - allennlp.training.trainer -   Training\n",
      "accuracy: 0.8273, loss: 0.3847 ||: 100%|██████████| 109/109 [08:01<00:00,  3.76s/it]\n",
      "01/18/2019 16:36:27 - INFO - allennlp.training.trainer -   Validating\n",
      "accuracy: 0.8222, loss: 0.4070 ||: 100%|██████████| 14/14 [00:55<00:00,  5.02s/it]\n",
      "01/18/2019 16:37:23 - INFO - allennlp.training.trainer -                     Training |  Validation\n",
      "01/18/2019 16:37:23 - INFO - allennlp.training.trainer -   loss          |     0.385  |     0.407\n",
      "01/18/2019 16:37:23 - INFO - allennlp.training.trainer -   accuracy      |     0.827  |     0.822\n",
      "01/18/2019 16:37:23 - INFO - allennlp.training.trainer -   cpu_memory_MB |  1224.270  |       N/A\n",
      "01/18/2019 16:37:24 - INFO - allennlp.training.trainer -   Epoch duration: 00:08:57\n",
      "01/18/2019 16:37:24 - INFO - allennlp.training.trainer -   Estimated training time remaining: 0:19:26\n",
      "01/18/2019 16:37:24 - INFO - allennlp.training.trainer -   Epoch 8/9\n",
      "01/18/2019 16:37:24 - INFO - allennlp.training.trainer -   Peak CPU memory usage MB: 1224.269824\n",
      "01/18/2019 16:37:24 - INFO - allennlp.training.trainer -   Training\n",
      "accuracy: 0.8240, loss: 0.3932 ||: 100%|██████████| 109/109 [08:10<00:00,  5.32s/it]\n",
      "01/18/2019 16:45:34 - INFO - allennlp.training.trainer -   Validating\n",
      "accuracy: 0.8475, loss: 0.3624 ||: 100%|██████████| 14/14 [00:54<00:00,  4.85s/it]\n",
      "01/18/2019 16:46:29 - INFO - allennlp.training.trainer -                     Training |  Validation\n",
      "01/18/2019 16:46:29 - INFO - allennlp.training.trainer -   loss          |     0.393  |     0.362\n",
      "01/18/2019 16:46:29 - INFO - allennlp.training.trainer -   accuracy      |     0.824  |     0.847\n",
      "01/18/2019 16:46:29 - INFO - allennlp.training.trainer -   cpu_memory_MB |  1224.270  |       N/A\n",
      "01/18/2019 16:46:30 - INFO - allennlp.training.trainer -   Best validation performance so far. Copying weights to '../data/sst/ckpts/replicate_0/best.th'.\n",
      "01/18/2019 16:46:30 - INFO - allennlp.training.trainer -   Epoch duration: 00:09:06\n",
      "01/18/2019 16:46:30 - INFO - allennlp.training.trainer -   Estimated training time remaining: 0:09:39\n",
      "01/18/2019 16:46:30 - INFO - allennlp.training.trainer -   Epoch 9/9\n",
      "01/18/2019 16:46:30 - INFO - allennlp.training.trainer -   Peak CPU memory usage MB: 1224.269824\n",
      "01/18/2019 16:46:30 - INFO - allennlp.training.trainer -   Training\n",
      "accuracy: 0.8237, loss: 0.3980 ||: 100%|██████████| 109/109 [08:20<00:00,  6.22s/it]\n",
      "01/18/2019 16:54:51 - INFO - allennlp.training.trainer -   Validating\n",
      "accuracy: 0.8326, loss: 0.3854 ||: 100%|██████████| 14/14 [00:51<00:00,  4.35s/it]\n",
      "01/18/2019 16:55:43 - INFO - allennlp.training.trainer -                     Training |  Validation\n",
      "01/18/2019 16:55:43 - INFO - allennlp.training.trainer -   loss          |     0.398  |     0.385\n",
      "01/18/2019 16:55:43 - INFO - allennlp.training.trainer -   accuracy      |     0.824  |     0.833\n",
      "01/18/2019 16:55:43 - INFO - allennlp.training.trainer -   cpu_memory_MB |  1224.270  |       N/A\n",
      "01/18/2019 16:55:45 - INFO - allennlp.training.trainer -   Epoch duration: 00:09:14\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'peak_cpu_memory_MB': 1224.269824,\n",
       " 'training_duration': '01:36:04',\n",
       " 'training_start_epoch': 0,\n",
       " 'training_epochs': 9,\n",
       " 'epoch': 9,\n",
       " 'training_accuracy': 0.8236994219653179,\n",
       " 'training_loss': 0.39802677273203474,\n",
       " 'training_cpu_memory_MB': 1224.269824,\n",
       " 'validation_accuracy': 0.8325688073394495,\n",
       " 'validation_loss': 0.3854148132460458,\n",
       " 'best_epoch': 8,\n",
       " 'best_validation_accuracy': 0.8474770642201835,\n",
       " 'best_validation_loss': 0.3624090020145689}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
