{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "dependencies"
    ]
   },
   "outputs": [],
   "source": [
    "depends_on = [\n",
    "    \"preproc_jigsaw\",\n",
    "    \"jigsaw_create_augmented_data\",\n",
    "    \"create_fasttext_matrix\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings for seamlessly running on colab\n",
    "import os\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive')\n",
    "    os.environ[\"IS_COLAB\"] = \"True\"\n",
    "except ImportError:\n",
    "    os.environ[\"IS_COLAB\"] = \"False\"    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"SLACK_TOKEN\" not in os.environ:\n",
    "    os.environ[\"SLACK_TOKEN\"] = \"\" # TODO: insert here for slack notifications\n",
    "if \"SLACK_ID\" not in os.environ:\n",
    "    os.environ[\"SLACK_ID\"] = \"\" # TODO: insert here for slack notifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "if [ \"$IS_COLAB\" = \"True\" ]; then\n",
    "    pip install git+https://github.com/facebookresearch/fastText.git\n",
    "    pip install torch\n",
    "    pip install torchvision\n",
    "    pip install --upgrade git+https://github.com/keitakurita/allennlp.git@develop\n",
    "    pip install dnspython\n",
    "    pip install jupyter_slack\n",
    "    pip install git+https://github.com/keitakurita/Better_LSTM_PyTorch.git\n",
    "    pip install nmslib\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from typing import *\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from functools import partial\n",
    "from overrides import overrides\n",
    "import warnings\n",
    "\n",
    "from allennlp.data import Instance\n",
    "from allennlp.data.token_indexers import TokenIndexer, SingleIdTokenIndexer\n",
    "from allennlp.data.tokenizers import Token\n",
    "from allennlp.nn import util as nn_util\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger(__name__)  # pylint: disable=invalid-name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from contextlib import contextmanager\n",
    "\n",
    "class Config(dict):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        for k, v in kwargs.items():\n",
    "            setattr(self, k, v)\n",
    "    \n",
    "    def set(self, key, val):\n",
    "        self[key] = val\n",
    "        setattr(self, key, val)\n",
    "\n",
    "@contextmanager\n",
    "def timer(name):\n",
    "    t0 = time.time()\n",
    "    yield\n",
    "    print(f'[{name}] done in {time.time() - t0:.0f} s')\n",
    "    \n",
    "import functools\n",
    "import traceback\n",
    "import sys\n",
    "\n",
    "def get_ref_free_exc_info():\n",
    "    \"Free traceback from references to locals/globals to avoid circular reference leading to gc.collect() unable to reclaim memory\"\n",
    "    type, val, tb = sys.exc_info()\n",
    "    traceback.clear_frames(tb)\n",
    "    return (type, val, tb)\n",
    "\n",
    "def gpu_mem_restore(func):\n",
    "    \"Reclaim GPU RAM if CUDA out of memory happened, or execution was interrupted\"\n",
    "    @functools.wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        try:\n",
    "            return func(*args, **kwargs)\n",
    "        except:\n",
    "            type, val, tb = get_ref_free_exc_info() # must!\n",
    "            raise type(val).with_traceback(tb) from None\n",
    "    return wrapper\n",
    "\n",
    "def ifnone(a: Any, alt: Any): return alt if a is None else a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = TypeVar(\"T\")\n",
    "TensorDict = Dict[str, Union[torch.Tensor, Dict[str, torch.Tensor]]]  # pylint: disable=invalid-name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# for papermill\n",
    "testing = False # set to False when running experiments\n",
    "debugging = False\n",
    "seed = 1\n",
    "use_bt = False\n",
    "computational_batch_size = 64\n",
    "batch_size = 128\n",
    "lr = 4e-3\n",
    "lr_schedule = \"slanted_triangular\"\n",
    "epochs = 4 if not testing else 1\n",
    "hidden_sz = 128\n",
    "dataset = \"jigsaw\"\n",
    "n_classes = 6\n",
    "max_seq_len = 512\n",
    "download_data = False\n",
    "ft_model_path = \"../data/jigsaw/ft_basic_toks.txt\"\n",
    "max_vocab_size = 400000\n",
    "dropouti = 0.2\n",
    "dropoutw = 0.0\n",
    "dropoute = 0.1\n",
    "dropoute_max = None\n",
    "dropoutr = 0.3 # TODO: Implement\n",
    "val_ratio = 0.0\n",
    "use_mbern_loss =False\n",
    "use_focal_loss = False\n",
    "label_smoothing_eps = 0.\n",
    "focal_loss_alpha = 1.\n",
    "focal_loss_gamma = 2.\n",
    "use_augmented = False\n",
    "freeze_embeddings = True\n",
    "mixup_ratio = 0.0\n",
    "discrete_mixup_ratio = 0.0\n",
    "attention_bias = True\n",
    "use_attention_aux = False\n",
    "weight_decay = 0.\n",
    "bias_init = True\n",
    "neg_splits = 1\n",
    "num_layers = 2\n",
    "rnn_type = \"lstm\"\n",
    "rnn_residual = False\n",
    "pooling_type = \"augmented_multipool\" # attention or multipool or augmented_multipool\n",
    "model_type = \"standard\"\n",
    "cache_elmo_embeddings = True\n",
    "use_word_level_features = False\n",
    "use_sentence_level_features = False\n",
    "bucket = True\n",
    "compute_thres_on_test = False\n",
    "find_lr = False\n",
    "permute_sentences = False\n",
    "run_id = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Can we make this play better with papermill?\n",
    "config = Config(\n",
    "    testing=testing,\n",
    "    debugging=debugging,\n",
    "    seed=seed,\n",
    "    use_bt=use_bt,\n",
    "    computational_batch_size=computational_batch_size,\n",
    "    batch_size=batch_size,\n",
    "    lr=lr,\n",
    "    lr_schedule=lr_schedule,\n",
    "    epochs=epochs,\n",
    "    hidden_sz=hidden_sz,\n",
    "    dataset=dataset,\n",
    "    n_classes=n_classes,\n",
    "    max_seq_len=max_seq_len, # necessary to limit memory usage\n",
    "    ft_model_path=ft_model_path,\n",
    "    max_vocab_size=max_vocab_size,\n",
    "    dropouti=dropouti,\n",
    "    dropoutw=dropoutw,\n",
    "    dropoute=dropoute,\n",
    "    dropoute_max=dropoute_max,\n",
    "    dropoutr=dropoutr,\n",
    "    val_ratio=val_ratio,\n",
    "    use_mbern_loss=use_mbern_loss,\n",
    "    use_focal_loss=use_focal_loss,\n",
    "    label_smoothing_eps=label_smoothing_eps,\n",
    "    focal_loss_alpha=focal_loss_alpha,\n",
    "    focal_loss_gamma=focal_loss_gamma,\n",
    "    use_augmented=use_augmented,\n",
    "    freeze_embeddings=freeze_embeddings,\n",
    "    attention_bias=attention_bias,\n",
    "    use_attention_aux=use_attention_aux,\n",
    "    weight_decay=weight_decay,\n",
    "    bias_init=bias_init,\n",
    "    neg_splits=neg_splits,\n",
    "    num_layers=num_layers,\n",
    "    rnn_type=rnn_type,\n",
    "    rnn_residual=rnn_residual,\n",
    "    pooling_type=pooling_type,\n",
    "    model_type=model_type,\n",
    "    cache_elmo_embeddings=cache_elmo_embeddings,\n",
    "    use_word_level_features=use_word_level_features,\n",
    "    use_sentence_level_features=use_sentence_level_features,\n",
    "    mixup_ratio=mixup_ratio,\n",
    "    discrete_mixup_ratio=discrete_mixup_ratio,\n",
    "    bucket=bucket,\n",
    "    compute_thres_on_test=compute_thres_on_test,\n",
    "    permute_sentences=permute_sentences,\n",
    "    find_lr=find_lr,\n",
    "    run_id=run_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.common.checks import ConfigurationError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.model_type != \"standard\" and \"bert\" not in config.model_type and \"elmo\" not in config.model_type:\n",
    "    raise ConfigurationError(f\"Invalid model type {config.model_type} given\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.mixup_ratio > 0. and config.bucket:\n",
    "    raise ConfigurationError(f\"Mixup should be combined with complete random shuffling of the input\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"bert\" in config.model_type and config.computational_batch_size > 16:\n",
    "    raise ConfigurationError(\"Batch size too large for BERT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.permute_sentences and (config.use_word_level_features or config.use_sentence_level_features):\n",
    "    raise ConfigurationError(\"Token shuffling does not yet transfer to wlf and slf.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "now = datetime.datetime.now()\n",
    "RUN_ID = config.run_id if config.run_id is not None else now.strftime(\"%m_%d_%H:%M:%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_GPU = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.environ[\"IS_COLAB\"] != \"True\":\n",
    "    DATA_ROOT = Path(\"../data\") / config.dataset\n",
    "else:\n",
    "    DATA_ROOT = Path(\"./gdrive/My Drive/Colab_Workspace/Colab Notebooks/data\") / config.dataset\n",
    "    config.ft_model_path = str(DATA_ROOT / \"ft_basic_toks.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p {DATA_ROOT}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "if download_data:\n",
    "    \n",
    "    if config.val_ratio > 0.0:\n",
    "        #fnames = [\"train_wo_val.csv\", \"test_proced.csv\", \"val.csv\", \"ft_model.txt\"]\n",
    "        raise ConfigurationError(f\"Validation dataset not processed.\")\n",
    "    \n",
    "    else:\n",
    "        fnames = [\"train_basic.jsonl\", \"test_basic_aug.jsonl\", \"ft_basic_toks.txt\"]\n",
    "    \n",
    "    if config.use_augmented or config.discrete_mixup_ratio > 0.0: \n",
    "        #fnames.append(\"train_extra.csv\")\n",
    "        raise ConfigurationError(f\"Augmented datasets not processed.\")\n",
    "    \n",
    "    for fname in fnames:\n",
    "        if not (DATA_ROOT / fname).exists():\n",
    "            print(subprocess.Popen([f\"aws s3 cp s3://nnfornlp/raw_data/jigsaw/{fname} {str(DATA_ROOT)}\"],\n",
    "                                   shell=True, stdout=subprocess.PIPE).stdout.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert_oov_map.bin\r\n",
      "ckpts\r\n",
      "data_ft_vocab\r\n",
      "data_vocab.bin\r\n",
      "distractor_targets.txt\r\n",
      "example.csv\r\n",
      "ft_asymmetric_noised_toks.txt\r\n",
      "ft_basic_toks.txt\r\n",
      "ft_model_bert_basic_tok.txt\r\n",
      "ft_model_no_bt.txt\r\n",
      "ft_model.txt\r\n",
      "ft_noised_distract_large_toks.txt\r\n",
      "ft_noised_large_toks.txt\r\n",
      "ft_noised_toks.txt\r\n",
      "ft_symmetric_hard_noised_toks.txt\r\n",
      "ft_symmetric_natural_noised_toks.txt\r\n",
      "sample_pred_bert_oov.csv\r\n",
      "sample_pred_bert_oov_fasttext_knn.csv\r\n",
      "sample_pred.csv\r\n",
      "sample_submission.csv\r\n",
      "small_test_proced.csv\r\n",
      "small_train.csv\r\n",
      "test_asymmetric_noised.csv\r\n",
      "test_asymmetric_noised.jsonl\r\n",
      "test_augmented.jsonl\r\n",
      "test_basic_aug.jsonl\r\n",
      "test_basic.csv\r\n",
      "test_basic.jsonl\r\n",
      "test_basic_preds_5cbdf665145ce50cdcb3443d.npy\r\n",
      "test_basic_preds_5cbdffb3145ce5102902573c.npy\r\n",
      "test_basic_preds_5cbe0b0c145ce513a4e33ed4.npy\r\n",
      "test_basic_preds_5cbe1ef3145ce5191af228f6.npy\r\n",
      "test_basic_preds_5cbe29f7145ce51b590c4682.npy\r\n",
      "test_basic_preds_5cbe3713145ce51e29b0eb03.npy\r\n",
      "test_basic_preds_5cbe547f145ce524b08751f3.npy\r\n",
      "test_basic_preds_5cbe5ef2145ce5293a0453b3.npy\r\n",
      "test_basic_preds_5cbe6846145ce52c349ca45b.npy\r\n",
      "test_basic_preds_5cbe709b145ce52d8f1e42a0.npy\r\n",
      "test_basic_preds_5cbe7a49145ce52eeb24976d.npy\r\n",
      "test_basic_preds_5cc0e185145ce51ee4c7f7c4.npy\r\n",
      "test_basic_preds_5cc0ea07145ce523ea526941.npy\r\n",
      "test_basic_preds_5cc0f3ff145ce5260b296308.npy\r\n",
      "test_basic_preds.npy\r\n",
      "test.csv\r\n",
      "test_labels.csv\r\n",
      "test_noised.csv\r\n",
      "test_noised_distract.csv\r\n",
      "test_noised_distract.jsonl\r\n",
      "test_noised_distract_large.csv\r\n",
      "test_noised_distract_large.jsonl\r\n",
      "test_noised_distract_large_preds_5cc27afc145ce55793da2f3c.npy\r\n",
      "test_noised_distract_preds_5cc0106e145ce52cb24744ce.npy\r\n",
      "test_noised_distract_preds_5cc0abfd145ce50c21fec6d2.npy\r\n",
      "test_noised.jsonl\r\n",
      "test_noised_large.csv\r\n",
      "test_noised_large.jsonl\r\n",
      "test_noised_large_preds_5cc12ec2145ce5473d2f338d.npy\r\n",
      "test_noised_preds_5cbfe8cc145ce51f5fa359ee.npy\r\n",
      "test_noised_preds_5cc00522145ce5319773fe35.npy\r\n",
      "test_noised_preds_5cc01d7d145ce565516a3460.npy\r\n",
      "test_noised_preds_5cc02ce5145ce55f5139be7a.npy\r\n",
      "test_noised_preds_5cc0a148145ce508f3ac2ea3.npy\r\n",
      "test_proced.csv\r\n",
      "test_proced_no_oov1.csv\r\n",
      "test_symmetric_hard_noised.csv\r\n",
      "test_symmetric_hard_noised.jsonl\r\n",
      "test_symmetric_natural_noised.csv\r\n",
      "test_symmetric_natural_noised.jsonl\r\n",
      "thres_basic_preds_5cbdf665145ce50cdcb3443d.npy\r\n",
      "thres_basic_preds_5cbdffb3145ce5102902573c.npy\r\n",
      "thres_basic_preds_5cbe0b0c145ce513a4e33ed4.npy\r\n",
      "thres_basic_preds_5cbe1ef3145ce5191af228f6.npy\r\n",
      "thres_basic_preds_5cbe29f7145ce51b590c4682.npy\r\n",
      "thres_basic_preds_5cbe3713145ce51e29b0eb03.npy\r\n",
      "thres_basic_preds_5cbe547f145ce524b08751f3.npy\r\n",
      "thres_basic_preds_5cbe5ef2145ce5293a0453b3.npy\r\n",
      "thres_basic_preds_5cbe6846145ce52c349ca45b.npy\r\n",
      "thres_basic_preds_5cbe709b145ce52d8f1e42a0.npy\r\n",
      "thres_basic_preds_5cbe7a49145ce52eeb24976d.npy\r\n",
      "thres_basic_preds_5cc0e185145ce51ee4c7f7c4.npy\r\n",
      "thres_basic_preds_5cc0ea07145ce523ea526941.npy\r\n",
      "thres_basic_preds_5cc0f3ff145ce5260b296308.npy\r\n",
      "thres_basic_preds.npy\r\n",
      "thres_noised_distract_large_preds_5cc27afc145ce55793da2f3c.npy\r\n",
      "thres_noised_distract_preds_5cc0106e145ce52cb24744ce.npy\r\n",
      "thres_noised_distract_preds_5cc0abfd145ce50c21fec6d2.npy\r\n",
      "thres_noised_large_preds_5cc12ec2145ce5473d2f338d.npy\r\n",
      "thres_noised_preds_5cbfe8cc145ce51f5fa359ee.npy\r\n",
      "thres_noised_preds_5cc00522145ce5319773fe35.npy\r\n",
      "thres_noised_preds_5cc01d7d145ce565516a3460.npy\r\n",
      "thres_noised_preds_5cc02ce5145ce55f5139be7a.npy\r\n",
      "thres_noised_preds_5cc0a148145ce508f3ac2ea3.npy\r\n",
      "tmp_model.pth\r\n",
      "toxic_targets.txt\r\n",
      "train_asymmetric_noised.csv\r\n",
      "train_asymmetric_noised.jsonl\r\n",
      "train_augmented.jsonl\r\n",
      "train_basic_aug.jsonl\r\n",
      "train_basic.csv\r\n",
      "train_basic.jsonl\r\n",
      "train.csv\r\n",
      "train_ds.bin\r\n",
      "train_extra.csv\r\n",
      "train_extra_interpolated.csv\r\n",
      "train_new_jigsaw.csv\r\n",
      "train_new_jigsaw_raw.csv\r\n",
      "train_noised.csv\r\n",
      "train_noised_distract_large.csv\r\n",
      "train_noised_distract_large.jsonl\r\n",
      "train_noised.jsonl\r\n",
      "train_noised_large.csv\r\n",
      "train_noised_large.jsonl\r\n",
      "train_symmetric_hard_noised.csv\r\n",
      "train_symmetric_hard_noised.jsonl\r\n",
      "train_symmetric_natural_noised.csv\r\n",
      "train_symmetric_natural_noised.jsonl\r\n",
      "train_with_bt.csv\r\n",
      "train_wo_val.csv\r\n",
      "val.csv\r\n",
      "voc_asymmetric_noised_toks\r\n",
      "voc_basic_toks\r\n",
      "voc_noised_distract_large_toks\r\n",
      "voc_noised_large_toks\r\n",
      "voc_noised_toks\r\n",
      "voc_symmetric_hard_noised_toks\r\n",
      "voc_symmetric_natural_noised_toks\r\n",
      "wiki.en.bin\r\n"
     ]
    }
   ],
   "source": [
    "!ls {DATA_ROOT}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set random seed manually to replicate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f7269849bf0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(config.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.data.dataset_readers import DatasetReader, StanfordSentimentTreeBankDatasetReader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.environ[\"IS_COLAB\"] != \"True\":\n",
    "    import sys\n",
    "    sys.path.append(\"../preprocessing\")\n",
    "    from data_loader import *\n",
    "    from basic_processing import word_level_features, sentence_level_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_cols = JIGSAW_LABEL_NAMES\n",
    "\n",
    "from enum import IntEnum\n",
    "ColIdx = IntEnum('ColIdx', [(x.upper(), i) for i, x in enumerate(label_cols)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare token handlers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from functools import wraps\n",
    "\n",
    "def maybeshuffle(_tokenize):\n",
    "    def func(*args, **kwargs):\n",
    "        arr = _tokenize(*args, **kwargs)\n",
    "        if config.permute_sentences:\n",
    "            random.shuffle(arr)\n",
    "        return arr\n",
    "    return func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.data.tokenizers.word_splitter import SpacyWordSplitter\n",
    "from allennlp.data.token_indexers import WordpieceIndexer, SingleIdTokenIndexer\n",
    "\n",
    "\n",
    "if config.model_type == \"standard\" or (\"elmo\" in config.model_type and config.cache_elmo_embeddings):\n",
    "    \n",
    "    from allennlp.data.token_indexers import SingleIdTokenIndexer\n",
    "    token_indexer = SingleIdTokenIndexer(\n",
    "        lowercase_tokens=True,\n",
    "    )\n",
    "    \n",
    "    @maybeshuffle\n",
    "    def token_extender(x: List[str]):\n",
    "        return x\n",
    "    \n",
    "elif \"elmo\" in config.model_type:\n",
    "    \n",
    "    from allennlp.data.token_indexers.elmo_indexer import ELMoTokenCharactersIndexer\n",
    "    token_indexer = ELMoTokenCharactersIndexer()\n",
    "    \n",
    "    @maybeshuffle\n",
    "    def token_extender(x: List[str]):\n",
    "        # add start and end of sentence tokens\n",
    "        return [\"<S>\"] + x + [\"</S>\"]\n",
    "    \n",
    "    if config.use_word_level_features or config.use_sentence_level_features:\n",
    "        raise ConfigurationError(\"Elmo tokenization does not yet transfer to wlf and slf.\")\n",
    "    \n",
    "elif \"bert\" in config.model_type:\n",
    "    \n",
    "    #def flatten(x: List[List[T]]) -> List[T]:\n",
    "    #    return [item for sublist in x for item in sublist]\n",
    "\n",
    "    from allennlp.data.token_indexers import PretrainedBertIndexer\n",
    "    token_indexer = PretrainedBertIndexer(\n",
    "        pretrained_model=config.model_type,\n",
    "        max_pieces=config.max_seq_len,\n",
    "        do_lowercase=True,\n",
    "     )\n",
    "    \n",
    "    @maybeshuffle\n",
    "    def token_extender(x: List[str]):\n",
    "        return x\n",
    "\n",
    "    if config.use_word_level_features or config.use_sentence_level_features:\n",
    "        raise ConfigurationError(\"BERT tokenization does not yet transfer to wlf and slf.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<allennlp.data.token_indexers.single_id_token_indexer.SingleIdTokenIndexer at 0x7f7208d4d3c8>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_indexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = ToxicDatasetJSONLReader(\n",
    "    token_extender = token_extender,\n",
    "    oov_token_swapper = OOVTokenSwapper(),\n",
    "    token_indexers={\"tokens\": token_indexer},\n",
    "    testing = config.testing,\n",
    "    label_smoothing_eps = config.label_smoothing_eps\n",
    "   )"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "159571it [00:24, 6629.92it/s]\n",
      "63978it [00:18, 3472.20it/s]\n"
     ]
    }
   ],
   "source": [
    "if config.val_ratio > 0.0:\n",
    "    \n",
    "    raise ConfigurationError(\"Validation dataset not processed.\")\n",
    "    \n",
    "    #train_ds, val_ds, test_ds = (reader.read(DATA_ROOT / fname) for fname in [\"train_wo_val.csv\",\n",
    "    #                                                                          \"val.csv\",\n",
    "    #                                                                          \"test_proced.csv\"])\n",
    "else:\n",
    "    \n",
    "    if config.use_bt:\n",
    "        raise ConfigurationError(\"BT dataset not processed.\")\n",
    "    \n",
    "    else:\n",
    "        train_ds, test_ds = (reader.read(DATA_ROOT / fname) for fname in [\"train_basic.jsonl\",\n",
    "                                                                          \"test_basic_aug.jsonl\"])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': ['Thank',\n",
       "  'you',\n",
       "  'for',\n",
       "  'understanding',\n",
       "  '.',\n",
       "  'I',\n",
       "  'think',\n",
       "  'very',\n",
       "  'highly',\n",
       "  'of',\n",
       "  'you',\n",
       "  'and',\n",
       "  'would',\n",
       "  'not',\n",
       "  'reverts',\n",
       "  'without',\n",
       "  'discussion',\n",
       "  '.'],\n",
       " '_token_indexers': {'tokens': <allennlp.data.token_indexers.single_id_token_indexer.SingleIdTokenIndexer at 0x7f7208d4d3c8>},\n",
       " '_indexed_tokens': None,\n",
       " '_indexer_name_to_indexed_token': None}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(test_ds[0].fields[\"tokens\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.use_augmented or config.discrete_mixup_ratio > 0.0:\n",
    "    raise ConfigurationError(\"Augmented dataset not processed.\")\n",
    "    # TODO: Handle data leak for validation!\n",
    "    #train_aug_ds = reader.read(DATA_ROOT / \"train_extra.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "159571"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': ['Explanation',\n",
       "  'Why',\n",
       "  'the',\n",
       "  'edits',\n",
       "  'made',\n",
       "  'under',\n",
       "  'my',\n",
       "  'username',\n",
       "  'Hardcore',\n",
       "  'Metallica',\n",
       "  'Fan',\n",
       "  'were',\n",
       "  'reverted',\n",
       "  '?',\n",
       "  'They',\n",
       "  'weren',\n",
       "  \"'\",\n",
       "  't',\n",
       "  'vandalisms',\n",
       "  ',',\n",
       "  'just',\n",
       "  'closure',\n",
       "  'on',\n",
       "  'some',\n",
       "  'Gas',\n",
       "  'after',\n",
       "  'I',\n",
       "  'voted',\n",
       "  'at',\n",
       "  'New',\n",
       "  'York',\n",
       "  'Dolls',\n",
       "  'FAC',\n",
       "  '.',\n",
       "  'And',\n",
       "  'please',\n",
       "  'don',\n",
       "  \"'\",\n",
       "  't',\n",
       "  'remove',\n",
       "  'the',\n",
       "  'template',\n",
       "  'from',\n",
       "  'the',\n",
       "  'talk',\n",
       "  'page',\n",
       "  'since',\n",
       "  'I',\n",
       "  \"'\",\n",
       "  'm',\n",
       "  'retired',\n",
       "  'now.89.205.38.27'],\n",
       " '_token_indexers': {'tokens': <allennlp.data.token_indexers.single_id_token_indexer.SingleIdTokenIndexer at 0x7f7208d4d3c8>},\n",
       " '_indexed_tokens': None,\n",
       " '_indexer_name_to_indexed_token': None}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(train_ds[0].fields[\"tokens\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.val_ratio > 0.0:\n",
    "    \n",
    "    train_labels = pd.read_csv(DATA_ROOT / \"train_wo_val.csv\")[label_cols].values\n",
    "    \n",
    "else:\n",
    "    train_labels = pd.read_csv(DATA_ROOT / \"train.csv\")[label_cols].values\n",
    "    \n",
    "if config.testing:\n",
    "    \n",
    "    train_labels = train_labels[:len(train_ds), :]\n",
    "    \n",
    "if config.use_augmented:\n",
    "    \n",
    "    train_aux_labels = pd.read_csv(DATA_ROOT / \"train_extra.csv\")[label_cols].values\n",
    "    \n",
    "    if config.testing: train_aux_labels = train_aux_labels[:len(train_ds), :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 223549/223549 [00:18<00:00, 11888.08it/s]\n"
     ]
    }
   ],
   "source": [
    "from allennlp.data.vocabulary import Vocabulary\n",
    "\n",
    "if \"bert\" in config.model_type:\n",
    "    vocab = Vocabulary()\n",
    "    \n",
    "elif config.model_type == \"standard\" or config.cache_elmo_embeddings:\n",
    "    \n",
    "    full_ds = train_ds + test_ds\n",
    "    \n",
    "    if config.val_ratio > 0.0: \n",
    "        full_ds = full_ds + val_ds\n",
    "        \n",
    "    vocab = Vocabulary.from_instances(full_ds, max_vocab_size=config.max_vocab_size)\n",
    "    \n",
    "else:\n",
    "    vocab = Vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.data.iterators import BucketIterator, DataIterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "class Sampler:\n",
    "    def sample(self, ds: List[Instance]) -> List[Instance]:\n",
    "        return ds\n",
    "    def sample_size(self, ds: List[Instance]) -> int:\n",
    "        return len(ds)\n",
    "\n",
    "class BiasedSampler(Sampler):\n",
    "    def __init__(self, mask: np.ndarray, n_splits: int):\n",
    "        self.mask = mask\n",
    "        self.n_splits = n_splits\n",
    "        self.pos = np.where(self.mask)[0]\n",
    "        self.neg = np.where(~self.mask)[0]\n",
    "        self._n_splits_iterated = 0\n",
    "        \n",
    "    def sample(self, ds: List[Instance]) -> List[Instance]:\n",
    "        if self._n_splits_iterated % self.n_splits == 0:\n",
    "            self.folds = KFold(n_splits=self.n_splits).split(self.neg)\n",
    "        _, neg_idxs = next(self.folds)\n",
    "        \n",
    "        p = np.random.permutation(len(self.pos) + len(neg_idxs))\n",
    "        smpl = np.r_[self.pos, self.neg[neg_idxs]][p]\n",
    "        \n",
    "        self._n_splits_iterated += 1\n",
    "        return [ds[i] for i in smpl]\n",
    "    \n",
    "    def sample_size(self, ds: List[Instance]) -> int:\n",
    "        \"\"\"Returns number of samples that would be returned upon a call to sample\"\"\"\n",
    "        # there might be a slight difference depending on the epoch, but it's okay\n",
    "        return len(self.pos) + len(self.neg) // self.n_splits "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScoredSampler:\n",
    "    def __init__(self, mask: np.ndarray, ratio: float):\n",
    "        self.mask = mask\n",
    "        self.ratio = ratio\n",
    "        self.n_samples = int(len(self.tgt) * self.ratio)\n",
    "        self.score = mask.astype(\"int\")\n",
    "    \n",
    "    def set_score(self, score: np.ndarray):\n",
    "        assert len(score) == len(self.tgt)\n",
    "        self.score = score\n",
    "    \n",
    "    def sample(self, ds: List[Instance]):\n",
    "        \"\"\"Sample top n targets sorted by score descending\"\"\"\n",
    "        smpl = np.arange(len(self.mask))[np.argsort(-self.score)][:self.n_samples]\n",
    "        return [ds[i] for i in smpl]\n",
    "    \n",
    "    def sample_size(self, ds: List[Instance]) -> int:\n",
    "        \"\"\"Returns number of samples that would be returned upon a call to sample\"\"\"\n",
    "        return self.n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import deque\n",
    "from overrides import overrides\n",
    "\n",
    "from allennlp.common.checks import ConfigurationError\n",
    "from allennlp.common.util import lazy_groups_of, add_noise_to_dict_values\n",
    "from allennlp.data.dataset import Batch\n",
    "from allennlp.data.instance import Instance\n",
    "from allennlp.data.iterators import DataIterator, BucketIterator, BasicIterator\n",
    "from allennlp.data.vocabulary import Vocabulary\n",
    "\n",
    "class SamplingIteratorMixin:\n",
    "    \"\"\"Uses Python's MRO to add sampling.\n",
    "    DANGER: This is pushing the limits of OOP and might lead to bugs\n",
    "    \"\"\"\n",
    "    def __init__(self, *args, sampler: Sampler=None, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.sampler = ifnone(sampler, Sampler())\n",
    "        \n",
    "    def get_num_batches(self, instances: List[Instance]):\n",
    "        return math.ceil(self.sampler.sample_size(instances) / self._batch_size)\n",
    "\n",
    "    def _create_batches(self, instances: Iterable[Instance], shuffle: bool) -> Iterable[Batch]:\n",
    "        yield from super()._create_batches(self.sampler.sample(instances), shuffle)\n",
    "\n",
    "# Caution: Inheritance must be in order: SamplingIteratorMixin, BucketIterator\n",
    "class CustomBucketIterator(SamplingIteratorMixin, BucketIterator): pass\n",
    "class CustomBasicIterator(SamplingIteratorMixin, BasicIterator): pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Allow for customization\n",
    "if config.neg_splits > 1:\n",
    "    if config.use_augmented:\n",
    "        full_trn_labels = np.concatenate([train_labels, train_aux_labels], axis=0)\n",
    "    else:\n",
    "        full_trn_labels = train_labels\n",
    "    sampler = BiasedSampler(full_trn_labels.sum(1) >= 1,\n",
    "                            config.neg_splits)\n",
    "else:\n",
    "    sampler = Sampler()\n",
    "if config.bucket:\n",
    "    iterator = CustomBucketIterator(\n",
    "        batch_size=config.batch_size, \n",
    "        biggest_batch_first=config.testing,\n",
    "        sorting_keys=[(\"tokens\", \"num_tokens\")],\n",
    "        max_instances_in_memory=config.batch_size * 2,\n",
    "        sampler=sampler,\n",
    "    )\n",
    "else:\n",
    "    # CAUTION: BasicIterator shuffles the dataset internally\n",
    "    # TODO: Either fix this bug or ensure evalutation can handle shuffle\n",
    "    # in the dataset order\n",
    "    iterator = CustomBasicIterator(\n",
    "        batch_size=config.batch_size, \n",
    "        max_instances_in_memory=config.batch_size * 2,\n",
    "        sampler=sampler,\n",
    "    )\n",
    "iterator.index_with(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(iterator(train_ds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': {'tokens': tensor([[  111,   594,    51,  ...,     0,     0,     0],\n",
       "          [    7,    92,   161,  ...,     0,     0,     0],\n",
       "          [ 8320,    32, 48729,  ...,     0,     0,     0],\n",
       "          ...,\n",
       "          [    5,   224,     3,  ...,   119,     2,     5],\n",
       "          [    5, 30802,  1742,  ...,     5,     0,     0],\n",
       "          [    5,  1374,   650,  ...,     5,     0,     0]])},\n",
       " 'word_level_features': tensor([[[0.1667, 0.0000, 1.0000],\n",
       "          [0.0000, 0.0000, 1.0000],\n",
       "          [0.0000, 0.0000, 1.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000]],\n",
       " \n",
       "         [[0.0000, 0.0000, 1.0000],\n",
       "          [0.0000, 0.0000, 1.0000],\n",
       "          [0.0000, 0.0000, 1.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000]],\n",
       " \n",
       "         [[0.0000, 0.1429, 1.0000],\n",
       "          [0.0000, 1.0000, 1.0000],\n",
       "          [0.1667, 0.0000, 1.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[0.0000, 1.0000, 1.0000],\n",
       "          [0.2000, 0.0000, 1.0000],\n",
       "          [0.0000, 0.0000, 1.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 1.0000],\n",
       "          [0.0000, 1.0000, 1.0000],\n",
       "          [0.0000, 1.0000, 1.0000]],\n",
       " \n",
       "         [[0.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 0.0000, 1.0000],\n",
       "          [0.2500, 0.0000, 1.0000],\n",
       "          ...,\n",
       "          [0.0000, 1.0000, 1.0000],\n",
       "          [0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000]],\n",
       " \n",
       "         [[0.0000, 1.0000, 1.0000],\n",
       "          [0.1667, 0.0000, 1.0000],\n",
       "          [0.0000, 1.0000, 1.0000],\n",
       "          ...,\n",
       "          [0.0000, 1.0000, 1.0000],\n",
       "          [0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000]]]),\n",
       " 'sentence_level_features': tensor([], size=(128, 0)),\n",
       " 'label': tensor([[0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [1., 1., 1., 0., 1., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [1., 0., 1., 0., 1., 0.],\n",
       "         [1., 0., 1., 0., 1., 1.],\n",
       "         [1., 0., 1., 0., 1., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [1., 1., 1., 0., 1., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [1., 0., 0., 1., 0., 0.],\n",
       "         [1., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [1., 0., 1., 0., 1., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [1., 0., 0., 0., 1., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [1., 0., 1., 0., 1., 1.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [1., 1., 1., 0., 1., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [1., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [1., 0., 1., 0., 1., 1.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [1., 0., 0., 0., 1., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [1., 0., 1., 0., 1., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [1., 0., 1., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [1., 0., 1., 1., 1., 1.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.]]),\n",
       " 'id': ['0037fe4f8f5cdcfb',\n",
       "  '008faa76dd3eb890',\n",
       "  '009a3333aa4ac011',\n",
       "  '008d3702b1ad0465',\n",
       "  '0023daf96917e0d0',\n",
       "  '000897889268bc93',\n",
       "  '003fa0c68deca750',\n",
       "  '0002bcb3da6cb337',\n",
       "  '006ca45465868e64',\n",
       "  '00a00eb46f78b1d2',\n",
       "  '006b888560bcdfcd',\n",
       "  '00a3073791a5feaa',\n",
       "  '009d0fe5e7ee720a',\n",
       "  '00908f946bd2fd05',\n",
       "  '009ce66bcd5de288',\n",
       "  '006a4e493b506d1e',\n",
       "  '00190820581d90ce',\n",
       "  '00472b8e2d38d1ea',\n",
       "  '0086998b34865f93',\n",
       "  '0057e30091cf3e81',\n",
       "  '004d912a00f79c89',\n",
       "  '00582dcd527c8d7d',\n",
       "  '0036e50f42d0b679',\n",
       "  '007a5055c23cc5a6',\n",
       "  '006e87872c8b370c',\n",
       "  '005fb1983ff191e9',\n",
       "  '006a263a08b593c5',\n",
       "  '001ffdcc3e7fb49c',\n",
       "  '004e59705689a5ed',\n",
       "  '0083ead5c8afc356',\n",
       "  '00040093b2687caa',\n",
       "  '002e2d3db2b597c4',\n",
       "  '005de39c51ef844a',\n",
       "  '00054a5e18b50dd4',\n",
       "  '004b97c80705a548',\n",
       "  '003bd094feef5263',\n",
       "  '001325b8b20ea8aa',\n",
       "  '002b90cc8a94c76b',\n",
       "  '008fdd0ca51cdadc',\n",
       "  '00025465d4725e87',\n",
       "  '004af8a71399a4dc',\n",
       "  '009df88828f523c4',\n",
       "  '009cb63b1bd5daf9',\n",
       "  '006eaaaca322e12d',\n",
       "  '004a789c03eda830',\n",
       "  '003217c3eb469ba9',\n",
       "  '0007e25b2121310b',\n",
       "  '000bfd0867774845',\n",
       "  '0020fd96ed3b8c8b',\n",
       "  '007810bde7d6ebd4',\n",
       "  '0084d905b6f19f2b',\n",
       "  '00128363e367d703',\n",
       "  '0036621e4c7e10b5',\n",
       "  '0091798f05a311af',\n",
       "  '000c6a3f0cd3ba8e',\n",
       "  '0083790fbfe5014d',\n",
       "  '0097dd5c29bf7a15',\n",
       "  '0010833a96e1f886',\n",
       "  '0040017ef6277334',\n",
       "  '001c419c445b5a59',\n",
       "  '0063f77040fbc7d4',\n",
       "  '00397b264deba890',\n",
       "  '0001d958c54c6e35',\n",
       "  '0011cc71398479c4',\n",
       "  '003910ffa2f50517',\n",
       "  '0044cf18cc2655b3',\n",
       "  '003f698d06c9b180',\n",
       "  '006cf8c9f4cc4d57',\n",
       "  '0020e7119b96eeeb',\n",
       "  '0071cd1ca07040ab',\n",
       "  '00803f08f55bdcad',\n",
       "  '00637960a7ec3436',\n",
       "  '00a1aabcab9d44a0',\n",
       "  '00229d44f41f3acb',\n",
       "  '007ecbb379c4a861',\n",
       "  '0009801bd85e5806',\n",
       "  '00604eb295a1dbf2',\n",
       "  '005ba6af463ab6cf',\n",
       "  '0060ef190ee10720',\n",
       "  '0035d638ba684122',\n",
       "  '000eefc67a2c930f',\n",
       "  '00031b1e95af7921',\n",
       "  '008e0818dde894fb',\n",
       "  '00562e78e0b34102',\n",
       "  '000103f0d9cfb60f',\n",
       "  '001c557175094f10',\n",
       "  '00958dec64c33224',\n",
       "  '0034d7c78cfa6dee',\n",
       "  '003caacc6ce6c9e9',\n",
       "  '008a990457283850',\n",
       "  '0048e4ed8a0af433',\n",
       "  '006fc8cfaa4faf0b',\n",
       "  '00686325bcc16080',\n",
       "  '006493e4e9c89cab',\n",
       "  '009371b0ef213487',\n",
       "  '0053978373606ba4',\n",
       "  '009a52daa8dbb767',\n",
       "  '00169857adbc989b',\n",
       "  '0050cb3bc226f94e',\n",
       "  '007094f2f9efe035',\n",
       "  '00290e2a171dd073',\n",
       "  '001dc38a83d420cf',\n",
       "  '002918ae66cc4bc2',\n",
       "  '0097b052f0b68a96',\n",
       "  '004fb9f655230909',\n",
       "  '006b94add72ed61c',\n",
       "  '004a23742282fee4',\n",
       "  '008a856f6691e051',\n",
       "  '0066dcf7d9ecd360',\n",
       "  '0073059e6433db47',\n",
       "  '0090c1f0788dd0e9',\n",
       "  '009a6b3e571ee155',\n",
       "  '0010307a3a50a353',\n",
       "  '004fd4fb5c47c29f',\n",
       "  '00a216c00b90ce88',\n",
       "  '001e89eb3f0b0915',\n",
       "  '00316bcc0d1bc6e0',\n",
       "  '006b77d41eb4bc08',\n",
       "  '0055208c660f7894',\n",
       "  '006fda507acd9769',\n",
       "  '00521847b93eba3b',\n",
       "  '0060c5c9030b2d14',\n",
       "  '00548d029d7f4783',\n",
       "  '003d77a20601cec1',\n",
       "  '0095756047a71716',\n",
       "  '002d6c9d9f85e81f',\n",
       "  '00280c0d0652b366',\n",
       "  '002a13f2896596fa']}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  111,   594,    51,  ...,     0,     0,     0],\n",
       "        [    7,    92,   161,  ...,     0,     0,     0],\n",
       "        [ 8320,    32, 48729,  ...,     0,     0,     0],\n",
       "        ...,\n",
       "        [    5,   224,     3,  ...,   119,     2,     5],\n",
       "        [    5, 30802,  1742,  ...,     5,     0,     0],\n",
       "        [    5,  1374,   650,  ...,     5,     0,     0]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"tokens\"][\"tokens\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 48])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"tokens\"][\"tokens\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.modules.token_embedders import Embedding\n",
    "from allennlp.modules.token_embedders.bert_token_embedder import BertEmbedder, PretrainedBertEmbedder\n",
    "from allennlp.modules.seq2vec_encoders import Seq2VecEncoder, PytorchSeq2VecWrapper\n",
    "from allennlp.modules.stacked_bidirectional_lstm import StackedBidirectionalLstm\n",
    "from allennlp.nn.util import get_text_field_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(Seq2VecEncoder):\n",
    "    def __init__(self, inp_sz, aug_sz=None,\n",
    "                 hidden_sz=None, out_sz=None, dim=1, eps=1e-9,\n",
    "                 return_attention=False, use_bias=True):\n",
    "        super().__init__()\n",
    "        self.inp_sz, self.dim, self.eps = inp_sz, dim, eps\n",
    "        self.out_sz = ifnone(out_sz, self.inp_sz)\n",
    "        self.return_attention = return_attention\n",
    "        self.l1 = nn.Linear(inp_sz, ifnone(inp_sz * 2, hidden_sz))\n",
    "        nn.init.xavier_uniform_(self.l1.weight.data)\n",
    "        nn.init.zeros_(self.l1.bias.data)\n",
    "        \n",
    "        vw = torch.zeros(ifnone(inp_sz * 2, hidden_sz), 1)\n",
    "        nn.init.xavier_uniform_(vw)        \n",
    "        self.vw = nn.Parameter(vw)\n",
    "        self.use_bias = use_bias\n",
    "        if self.use_bias: self.b = nn.Parameter(torch.zeros(1))\n",
    "    \n",
    "    @overrides\n",
    "    def get_input_dim(self) -> int:\n",
    "        return self.inp_sz\n",
    "    \n",
    "    @overrides\n",
    "    def get_output_dim(self) -> int:\n",
    "        return self.out_sz\n",
    "        \n",
    "    def forward(self, x, aug=None, mask=None):\n",
    "        e = torch.tanh(self.l1(x))\n",
    "        e = torch.einsum(\"bij,jk->bi\", [e, self.vw]) \n",
    "        if self.use_bias: e = e + self.b\n",
    "        a = torch.exp(e)\n",
    "        \n",
    "        if mask is not None: a = a.masked_fill(mask == 0, 0)\n",
    "\n",
    "        a = a / (torch.sum(a, dim=self.dim, keepdim=True) + self.eps)\n",
    "\n",
    "        weighted_input = x * a.unsqueeze(-1)\n",
    "        if self.return_attention:\n",
    "            return torch.sum(weighted_input, dim=1), a\n",
    "        else:\n",
    "            return torch.sum(weighted_input, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.modules.seq2seq_encoders import PytorchSeq2SeqWrapper, Seq2SeqEncoder\n",
    "from better_lstm import LSTM, VariationalDropout\n",
    "\n",
    "class MultiPooling(Seq2VecEncoder):\n",
    "    \"\"\"Does max and mean pooling over the temporal dimension\"\"\"\n",
    "    def __init__(self, input_sz: int):\n",
    "        super().__init__()\n",
    "        self.input_sz = input_sz\n",
    "        \n",
    "    @overrides\n",
    "    def get_input_dim(self) -> int:\n",
    "        return self.input_sz\n",
    "    \n",
    "    @overrides\n",
    "    def get_output_dim(self) -> int:\n",
    "        return self.input_sz * 2\n",
    "        \n",
    "    def forward(self, x, mask=None, aug=None):\n",
    "        max_, _ = torch.max(x, dim=1)\n",
    "        mean_ = torch.mean(x, dim=1)\n",
    "        return torch.cat([max_, mean_], dim=-1)\n",
    "\n",
    "class AugmentedMultiPool(MultiPooling):\n",
    "    def __init__(self, input_sz, aug_sz):\n",
    "        super().__init__(input_sz)\n",
    "        self.attn = Attention(input_sz, hidden_sz=input_sz, \n",
    "                              out_sz=input_sz)\n",
    "    @overrides\n",
    "    def get_output_dim(self) -> int:\n",
    "        return self.input_sz * 3\n",
    "    \n",
    "    def forward(self, x, mask=None, aug=None):\n",
    "        pooled = super().forward(x, mask=mask, aug=aug)\n",
    "        attn = self.attn(x, mask=mask, aug=None)\n",
    "        return torch.cat([pooled, attn], dim=-1)\n",
    "    \n",
    "class BiRNN(Seq2SeqEncoder):\n",
    "    def __init__(self, rnn_type, n_layers, embed_sz, hidden_sz, dropoutw=0.):\n",
    "        super().__init__()\n",
    "        self.rnn_type = rnn_type\n",
    "        self.n_layers = n_layers\n",
    "        self.embed_sz = embed_sz\n",
    "        self.hidden_sz = hidden_sz\n",
    "        in_szs = [embed_sz] + [hidden_sz * 2] * (n_layers - 1)\n",
    "        if rnn_type == \"lstm\":\n",
    "            rnns = [LSTM(embed_sz, hidden_sz, batch_first=True, num_layers=n_layers,\n",
    "                         bidirectional=True, dropoutw=dropoutw)]\n",
    "        else:\n",
    "            if dropoutw > 0.0:\n",
    "                warnings.warn(\"Weight dropout not currently supported with GRUs\")\n",
    "            rnns = [nn.GRU(embed_sz, hidden_sz, batch_first=True, num_layers=n_layers, \n",
    "                           bidirectional=True)]\n",
    "            for gru in rnns:\n",
    "                for name, param in gru.named_parameters():\n",
    "                    if \"weight_hh\" in name:\n",
    "                        nn.init.orthogonal_(param.data)\n",
    "                    elif \"weight_ih\" in name:\n",
    "                        nn.init.xavier_uniform_(param.data)\n",
    "                    elif \"bias\" in name:\n",
    "                        nn.init.zeros_(param.data)\n",
    "        self.rnns = nn.ModuleList([PytorchSeq2SeqWrapper(rnn) for rnn in rnns])\n",
    "        if config.use_attention_aux:\n",
    "            self.ln = nn.Linear(embed_sz, 64) # handle attention auxillary input here\n",
    "\n",
    "    @overrides\n",
    "    def get_input_dim(self) -> int:\n",
    "        return self.embed_sz\n",
    "    \n",
    "    @overrides\n",
    "    def get_output_dim(self) -> int:\n",
    "        if config.rnn_residual:\n",
    "            out_sz = self.hidden_sz * 2 * self.n_layers\n",
    "        else:\n",
    "            out_sz = self.hidden_sz * 2\n",
    "        if config.use_attention_aux: out_sz += 64\n",
    "        return out_sz\n",
    "    \n",
    "    def forward(self, embeds, mask=None):\n",
    "        x = embeds\n",
    "        outputs = []\n",
    "        for rnn in self.rnns:\n",
    "            x = rnn(x, mask=mask)\n",
    "            if config.rnn_residual:\n",
    "                outputs.append(x)\n",
    "        if config.rnn_residual:\n",
    "            x = torch.cat(outputs, dim=-1)\n",
    "\n",
    "        if config.use_attention_aux:\n",
    "            x = torch.cat([torch.tanh(self.ln(embeds)), x], dim=-1)\n",
    "        return x\n",
    "    \n",
    "class BiRNNEncoder(Seq2VecEncoder):\n",
    "    def __init__(self, rnn: Seq2SeqEncoder,\n",
    "                 pooler: Seq2VecEncoder,\n",
    "                 dropouti=0.0, dropoutr=0.0):\n",
    "        super().__init__()\n",
    "        self.dropouti = VariationalDropout(dropouti, batch_first=True)\n",
    "        self.rnn = rnn\n",
    "        self.dropouto = VariationalDropout(dropoutr, batch_first=True)\n",
    "        self.pool = pooler\n",
    "        \n",
    "    @overrides\n",
    "    def get_input_dim(self) -> int:\n",
    "        return self.rnn.get_input_dim()\n",
    "    \n",
    "    @overrides\n",
    "    def get_output_dim(self) -> int:\n",
    "        out_dim = self.pool.get_output_dim()\n",
    "        if config.use_sentence_level_features:\n",
    "            out_dim += len(sentence_level_features)\n",
    "        return out_dim\n",
    "    \n",
    "    def _init_hidden_state(self, bs:int):\n",
    "        if self.rnn.rnn_type == \"lstm\":\n",
    "            return torch.zeros(bs, self.hidden_sz), torch.zeros(bs, self.hidden_sz)\n",
    "        else:\n",
    "            return torch.zeros(bs, self.hidden_sz)\n",
    "    \n",
    "    @overrides\n",
    "    def forward(self, x: torch.Tensor, sentence_feats: torch.Tensor,\n",
    "                mask: Optional[torch.Tensor]=None) -> torch.Tensor:\n",
    "        seq = self.rnn(x, mask)\n",
    "        seq = self.dropouto(seq)\n",
    "        vec = self.pool(seq, aug=x, mask=mask)\n",
    "        if config.use_sentence_level_features:\n",
    "            return torch.cat([sentence_feats, vec], dim=-1)\n",
    "        else:\n",
    "            return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.training.metrics import CategoricalAccuracy, BooleanAccuracy, Metric\n",
    "\n",
    "def prod(x: Iterable):\n",
    "    acc = 1\n",
    "    for v in x: acc *= v\n",
    "    return acc\n",
    "\n",
    "class MultilabelAccuracy(Metric):\n",
    "    def __init__(self, thres=0.5):\n",
    "        \n",
    "        self.thres = 0.5\n",
    "        self.correct_count = 0\n",
    "        self.total_count = 0\n",
    "    \n",
    "    def __call__(self, logits: torch.FloatTensor, \n",
    "                 t: torch.LongTensor) -> float:\n",
    "        \n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        t = t.detach().cpu().numpy() >= (1 - config.label_smoothing_eps)\n",
    "        cc = ((logits >= self.thres) == t).sum()\n",
    "        tc = prod(logits.shape)\n",
    "        self.correct_count += cc\n",
    "        self.total_count += tc\n",
    "        return cc / tc\n",
    "    \n",
    "    def get_metric(self, reset: bool=False):\n",
    "        acc = self.correct_count / self.total_count\n",
    "        if reset:\n",
    "            self.reset()\n",
    "        return acc\n",
    "    \n",
    "    @overrides\n",
    "    def reset(self):\n",
    "        self.correct_count = 0\n",
    "        self.total_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.nn.util import move_to_device, has_tensor\n",
    "\n",
    "def permute(obj, p: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Given a structure (possibly) containing Tensors on the CPU,\n",
    "    permute all the Tensors\n",
    "    \"\"\"\n",
    "    if not has_tensor(obj):\n",
    "        return obj\n",
    "    elif isinstance(obj, torch.Tensor):\n",
    "        return obj[p]\n",
    "    elif isinstance(obj, dict):\n",
    "        return {key: permute(value, p) for key, value in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [permute(item, p) for item in obj]\n",
    "    elif isinstance(obj, tuple):\n",
    "        return tuple([permute(item, p) for item in obj])\n",
    "    else:\n",
    "        return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.functional as F\n",
    "\n",
    "class FocalLossWithLogits(nn.Module):\n",
    "    \"\"\"Borrowed from https://www.kaggle.com/c/tgs-salt-identification-challenge/discussion/65938\"\"\"\n",
    "    def __init__(self, alpha=1., gamma=2.):\n",
    "        super().__init__()\n",
    "        self.alpha,self.gamma = alpha,gamma\n",
    "        self._loss = nn.BCEWithLogitsLoss(reduction=\"none\")\n",
    "        \n",
    "    def forward(self, y, t):\n",
    "        bce_loss = self._loss(y, t)\n",
    "        pt = torch.exp(-bce_loss)\n",
    "        return (self.alpha * (1-pt) ** self.gamma * bce_loss).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "#Reference https://stackoverflow.com/a/5898031\n",
    "from itertools import chain, combinations\n",
    "def all_subsets(ss):\n",
    "    return chain(*map(lambda x: combinations(ss, x), range(0, len(ss)+1)))\n",
    "\n",
    "\n",
    "class MBernLossWithLogits(nn.Module):\n",
    "    \"\"\" Referece https://arxiv.org/pdf/1206.1874.pdf\n",
    "        We perform a simple trick of assigning each label\n",
    "        value combination to a separate category\n",
    "        Total number of categories is 2**config.n_classes\n",
    "    \"\"\"\n",
    "    def __init__(self, n_classes = config.n_classes):\n",
    "        super().__init__()\n",
    "        self.n_classes = n_classes\n",
    "        self.loss = nn.CrossEntropyLoss(reduction=\"mean\")\n",
    "        self._create_dicts()\n",
    "        self.n_cat = len(self.dict_cat)\n",
    "        \n",
    "    def _create_dicts(self):\n",
    "        pos = list(range(self.n_classes))\n",
    "        self.dict_cat = dict()\n",
    "        self.dict_cat_inv = dict()\n",
    "        for i, subset in enumerate(all_subsets(stuff)):\n",
    "            self.dict_cat[i] = subset\n",
    "            self.dict_cat_inv[subset] = i\n",
    "        \n",
    "    def _map_target(self,t_old):\n",
    "        \n",
    "        t_new = torch.zeros(t_old.size()[0],device=t_old.get_device(),dtype=torch.long)\n",
    "        \n",
    "        for i in range(t_old.size(0)):\n",
    "            p = (t_old[i,:]==1).nonzero().reshape(-1,).tolist()\n",
    "            t_new[i] = self.dict_cat_inv[tuple(p)]\n",
    "        \n",
    "        return t_new\n",
    "        \n",
    "    def get_marginal_probs(self,y):\n",
    "\n",
    "        probs = F.softmax(y,dim=1)\n",
    "        marginal_probs = torch.zeros(probs.size()[0],self.n_classes,device=probs.get_device())\n",
    "        \n",
    "        for i in range(probs.size(0)):\n",
    "            for j in range(probs.size(1)):\n",
    "                for p in self.dict_cat[j]:\n",
    "                    marginal_probs[i,p] += probs[i,j]\n",
    "                \n",
    "        return marginal_probs\n",
    "            \n",
    "    def forward(self, y, t):\n",
    "        t_new = self._map_target(t)\n",
    "        loss = self.loss(y, t_new)\n",
    "        return loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.models import Model\n",
    "from allennlp.modules.text_field_embedders import TextFieldEmbedder, BasicTextFieldEmbedder\n",
    "from torch.distributions.beta import Beta\n",
    "\n",
    "class BaselineModel(Model):\n",
    "    def __init__(self, word_embeddings: TextFieldEmbedder,\n",
    "                 encoder: Seq2VecEncoder,\n",
    "                 loss: nn.Module,\n",
    "                 out_sz: int=config.n_classes,\n",
    "                 multilabel: bool=True, \n",
    "                 dropouto=0.1,\n",
    "                 mixup_alpha: int=0.2):\n",
    "        super().__init__(vocab)\n",
    "        \n",
    "        self.word_embeddings = word_embeddings\n",
    "        self.encoder = encoder\n",
    "        feature_sz = self.encoder.get_output_dim()\n",
    "        \n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(feature_sz, 50),\n",
    "            nn.ELU(),\n",
    "            nn.Dropout(dropouto),\n",
    "            nn.Linear(50, out_sz),\n",
    "        )        \n",
    "        \n",
    "        self.loss = loss\n",
    "\n",
    "        self.multilabel = multilabel\n",
    "        self.lambda_sampler = Beta(torch.tensor([mixup_alpha]), torch.tensor([mixup_alpha]))\n",
    "        \n",
    "        if self.multilabel:\n",
    "            self.accuracy = MultilabelAccuracy()\n",
    "            self.per_label_acc = {c: MultilabelAccuracy() for c in label_cols}\n",
    "        \n",
    "        elif self.loss._get_name()==\"MBernLossWithLogits\":\n",
    "            self.accuracy = CategoricalAccuracy(thres=0.6224593312)\n",
    "        \n",
    "        else:\n",
    "            self.accuracy = CategoricalAccuracy()\n",
    "        \n",
    "        self.is_test_mode = False\n",
    "            \n",
    "    def test_mode(self, val=True):\n",
    "        self.is_test_mode = val\n",
    "        \n",
    "    def get_embeddings(self, toks: Dict[str, torch.Tensor],\n",
    "                       word_feats: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Encapsulates addition of word level features\"\"\"\n",
    "        embeddings = self.word_embeddings(toks)\n",
    "        if config.use_word_level_features:\n",
    "            embeddings = torch.cat([word_feats, embeddings], dim=-1)\n",
    "        return embeddings\n",
    "\n",
    "    def forward(self, tokens: Dict[str, torch.Tensor],\n",
    "                label: torch.Tensor,\n",
    "                word_level_features: torch.Tensor,\n",
    "                sentence_level_features: torch.Tensor,\n",
    "                **meta) -> torch.Tensor:\n",
    "        \n",
    "        if self.is_test_mode: tokens[\"tokens\"] *= 0\n",
    "        \n",
    "        mask = get_text_field_mask(tokens)\n",
    "        embeddings = self.get_embeddings(tokens, word_level_features)\n",
    "        \n",
    "        state = self.encoder(embeddings, \n",
    "                             sentence_feats=sentence_level_features, \n",
    "                             mask=mask)\n",
    "        \n",
    "        class_logits = self.projection(state)\n",
    "        \n",
    "        output = {\"class_logits\": class_logits}\n",
    "\n",
    "        if self.loss._get_name()==\"MBernLossWithLogits\":\n",
    "            output[\"marginal_probs\"] = self.loss.get_marginal_probs(class_logits)\n",
    "\n",
    "        if self.loss._get_name()==\"MBernLossWithLogits\":\n",
    "            output[\"accuracy\"] = self.accuracy(self.loss.get_marginal_probs(class_logits), label)\n",
    "        else:\n",
    "            output[\"accuracy\"] = self.accuracy(class_logits, label)\n",
    "        \n",
    "        output[\"loss\"] = self.loss(class_logits, label)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def mixup(self, tokens: Dict[str, torch.Tensor],\n",
    "              label: torch.Tensor,\n",
    "              word_level_features: torch.Tensor,\n",
    "              sentence_level_features: torch.Tensor,\n",
    "              **meta) -> TensorDict:\n",
    "        # generate new tokens and labels\n",
    "        bs = label.size(0)\n",
    "        shuf = torch.randperm(bs).to(label.device)\n",
    "        tokens2 = permute(tokens, shuf)\n",
    "        labels1, labels2 = label, permute(label, shuf)\n",
    "        # TODO: Think of how to handle this masking intelligently\n",
    "        mask1, mask2 = (get_text_field_mask(t) for t in (tokens, tokens2))\n",
    "        embs1, embs2 = (self.get_embeddings(t, word_level_features) for t in (tokens, tokens2))\n",
    "        # interpolate\n",
    "        ratios = self.lambda_sampler.sample((bs, 1)).to(label.device)\n",
    "        embs = ratios * embs1 + (1-ratios) * embs2\n",
    "        label = ratios.squeeze(2) * labels1 + (1-ratios.squeeze(2)) * labels2\n",
    "        \n",
    "        # remaining process is the same\n",
    "        # TODO: Handle stat feats\n",
    "        state = self.encoder(embs, sentence_level_features, mask1 * mask2) # TODO: Handle masking\n",
    "        class_logits = self.projection(state)\n",
    "        \n",
    "        output = {\"loss\": self.loss(class_logits, label)}\n",
    "        return output\n",
    "    \n",
    "    def get_metrics(self, reset: bool = False) -> Dict[str, float]:\n",
    "        return {\"accuracy\": self.accuracy.get_metric(reset)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.set(\"vocab_size\", min(vocab.get_vocab_size(), config.max_vocab_size))\n",
    "if config.model_type == \"standard\":\n",
    "    config.set(\"embedding_dim\", 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import warnings\n",
    "\n",
    "def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "def get_fasttext_embeddings(model_path: str, vocab: Vocabulary):\n",
    "    prog_bar = tqdm(open(model_path, encoding=\"utf8\", errors='ignore'))\n",
    "    prog_bar.set_description(\"Loading embeddings\")\n",
    "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in prog_bar\n",
    "                             if len(o)>100)\n",
    "    all_embs = np.stack(embeddings_index.values())\n",
    "\n",
    "    embeddings = np.zeros((config.vocab_size + 5, 300))\n",
    "    n_missing_tokens = 0\n",
    "    prog_bar = tqdm(vocab.get_index_to_token_vocabulary().items())\n",
    "    prog_bar.set_description(\"Creating matrix\")\n",
    "    for idx, token in prog_bar:\n",
    "        if idx == 0: continue # keep padding as all zeros\n",
    "        if idx == 1: continue # Treat unknown words as dropped words\n",
    "        if token == \"[MASK]\":\n",
    "            embeddings[idx, :] = np.random.randn(300) * 0.5\n",
    "        if token not in embeddings_index:\n",
    "            n_missing_tokens += 1\n",
    "            if n_missing_tokens < 10:\n",
    "                warnings.warn(f\"Token {token} not in embeddings: did you change preprocessing?\")\n",
    "            if n_missing_tokens == 10:\n",
    "                warnings.warn(f\"More than {n_missing_tokens} missing, supressing warnings\")\n",
    "        else:\n",
    "            embeddings[idx, :] = embeddings_index[token]\n",
    "    \n",
    "    print(\"MISSING tokens: \", n_missing_tokens)\n",
    "    if n_missing_tokens > 0:\n",
    "        warnings.warn(f\"{n_missing_tokens} in total are missing from embedding text file\")\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading embeddings: : 320356it [00:29, 10764.12it/s]\n",
      "/home/anna/neuralnlp/lib/python3.7/site-packages/ipykernel_launcher.py:11: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "Creating matrix: 100%|██████████| 252485/252485 [00:01<00:00, 237624.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MISSING tokens:  0\n",
      "[Loading embeddings] done in 32 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "with timer(\"Loading embeddings\"):\n",
    "    if config.model_type == \"standard\":\n",
    "        embedding_weights = get_fasttext_embeddings(config.ft_model_path, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomEmbedding(Embedding):\n",
    "    # TODO: Fix (make this decently efficient: currently allocating two embeddings)\n",
    "    def __init__(self, num_embeddings, embedding_dim,\n",
    "                 padding_index=None, max_norm=None, trainable=True,\n",
    "                 weight=None, dropout=0., scale=None):\n",
    "        super().__init__(num_embeddings, embedding_dim, weight=weight,\n",
    "                         padding_index=padding_index, max_norm=max_norm,\n",
    "                         trainable=trainable)\n",
    "        self.dropout = dropout\n",
    "        self.scale = scale\n",
    "        self.padding_idx = padding_index\n",
    "\n",
    "    def forward(self, words):\n",
    "        weight = self.weight\n",
    "        if self.dropout > 0.0 and self.training:\n",
    "            mask = weight.data.new().resize_((weight.size(0), 1)).bernoulli_(1 - self.dropout).expand_as(weight) / (1 - self.dropout)\n",
    "            masked_embed_weight = mask * weight\n",
    "        else:\n",
    "            masked_embed_weight = weight\n",
    "        if self.scale:\n",
    "            masked_embed_weight = scale.expand_as(masked_embed_weight) * masked_embed_weight\n",
    "\n",
    "        padding_idx = self.padding_idx\n",
    "        if padding_idx is None:\n",
    "            padding_idx = -1\n",
    "\n",
    "        X = torch.nn.functional.embedding(words, masked_embed_weight,\n",
    "            padding_idx, self.max_norm, self.norm_type,\n",
    "            self.scale_grad_by_freq, self.sparse\n",
    "          )\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.modules.text_field_embedders.text_field_embedder import TextFieldEmbedder\n",
    "from allennlp.modules.time_distributed import TimeDistributed\n",
    "\n",
    "# TODO: Implement\n",
    "class ElmoTextFieldEmbedder(TextFieldEmbedder):\n",
    "    # AllenNLP support for caching sucks by default\n",
    "    # so we have to write our own embedder to bypass this problem\n",
    "    def __init__(self,\n",
    "                 token_embedders: Dict[str, Any],\n",
    "                 embedder_to_indexer_map: Dict[str, List[str]] = None,\n",
    "                 allow_unmatched_keys: bool = False) -> None:\n",
    "        super().__init__()\n",
    "        self._token_embedders = token_embedders\n",
    "        self._embedder_to_indexer_map = embedder_to_indexer_map\n",
    "        for key, embedder in token_embedders.items():\n",
    "            name = 'token_embedder_%s' % key\n",
    "            self.add_module(name, embedder)\n",
    "        self._allow_unmatched_keys = allow_unmatched_keys\n",
    "    \n",
    "    @overrides\n",
    "    def get_output_dim(self) -> int:\n",
    "        output_dim = 0\n",
    "        for embedder in self._token_embedders.values():\n",
    "            output_dim += embedder.get_output_dim()\n",
    "        return output_dim\n",
    "    \n",
    "    def forward(self, text_field_input: Dict[str, torch.Tensor],\n",
    "                num_wrapping_dims: int = 0) -> torch.Tensor:\n",
    "        if self._token_embedders.keys() != text_field_input.keys():\n",
    "            if not self._allow_unmatched_keys:\n",
    "                message = \"Mismatched token keys: %s and %s\" % (str(self._token_embedders.keys()),\n",
    "                                                                str(text_field_input.keys()))\n",
    "                raise ConfigurationError(message)\n",
    "        embedded_representations = []\n",
    "        keys = sorted(self._token_embedders.keys())\n",
    "        for key in keys:\n",
    "            # If we pre-specified a mapping explictly, use that.\n",
    "            if self._embedder_to_indexer_map is not None:\n",
    "                tensors = [text_field_input[indexer_key] for\n",
    "                           indexer_key in self._embedder_to_indexer_map[key]]\n",
    "            else:\n",
    "                # otherwise, we assume the mapping between indexers and embedders\n",
    "                # is bijective and just use the key directly.\n",
    "                tensors = [text_field_input[key]]\n",
    "            # Note: need to use getattr here so that the pytorch voodoo\n",
    "            # with submodules works with multiple GPUs.\n",
    "            embedder = getattr(self, 'token_embedder_{}'.format(key))\n",
    "            for _ in range(num_wrapping_dims):\n",
    "                embedder = TimeDistributed(embedder)\n",
    "            # Force embedder to use word inputs\n",
    "            if key == \"tokens\":\n",
    "                token_vectors = embedder(tensors[0], \n",
    "                                         word_inputs=tensors[0] if config.cache_elmo_embeddings else None)\n",
    "            else:\n",
    "                token_vectors = embedder(*tensors)\n",
    "            embedded_representations.append(token_vectors)\n",
    "        return torch.cat(embedded_representations, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_pretrained_bert.modeling import BertModel\n",
    "from allennlp.modules.token_embedders.token_embedder import TokenEmbedder\n",
    "\n",
    "class CustomBertEmbedder(TokenEmbedder):\n",
    "    \"\"\"\n",
    "    A ``TokenEmbedder`` that produces BERT embeddings for your tokens.\n",
    "    Should be paired with a ``BertIndexer``, which produces wordpiece ids.\n",
    "    Sums last 4 hidden layers for now (might use scalar mix in the future)\n",
    "    \"\"\"\n",
    "    def __init__(self, pretrained_model: str,\n",
    "                 use_scalar_mix: bool = False,\n",
    "                 fine_tune: bool = False,\n",
    "                 n_hidden_layers: int = 4) -> None:\n",
    "        super().__init__()\n",
    "        if use_scalar_mix and fine_tune:\n",
    "            raise ConfigurationError(\"Choose mix or fine tuning\")\n",
    "        \n",
    "        self.bert_model = BertModel.from_pretrained(pretrained_model)\n",
    "        for param in self.bert_model.parameters():\n",
    "            param.requires_grad = fine_tune\n",
    "        self.output_dim = self.bert_model.config.hidden_size\n",
    "        self.n_hidden_layers = n_hidden_layers\n",
    "        if use_scalar_mix:\n",
    "            self._scalar_mix = ScalarMix(n_hidden_layers,\n",
    "                                         do_layer_norm=False)\n",
    "        else:\n",
    "            self._scalar_mix = None\n",
    "\n",
    "    def get_output_dim(self) -> int:\n",
    "        return self.output_dim\n",
    "\n",
    "    def forward(self,\n",
    "                input_ids: torch.LongTensor,\n",
    "                offsets: torch.LongTensor = None,\n",
    "                token_type_ids: torch.LongTensor = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_ids : ``torch.LongTensor``\n",
    "            The (batch_size, ..., max_sequence_length) tensor of wordpiece ids.\n",
    "        offsets : ``torch.LongTensor``, optional\n",
    "            The BERT embeddings are one per wordpiece. However it's possible/likely\n",
    "            you might want one per original token. In that case, ``offsets``\n",
    "            represents the indices of the desired wordpiece for each original token.\n",
    "            Depending on how your token indexer is configured, this could be the\n",
    "            position of the last wordpiece for each token, or it could be the position\n",
    "            of the first wordpiece for each token.\n",
    "\n",
    "            For example, if you had the sentence \"Definitely not\", and if the corresponding\n",
    "            wordpieces were [\"Def\", \"##in\", \"##ite\", \"##ly\", \"not\"], then the input_ids\n",
    "            would be 5 wordpiece ids, and the \"last wordpiece\" offsets would be [3, 4].\n",
    "            If offsets are provided, the returned tensor will contain only the wordpiece\n",
    "            embeddings at those positions, and (in particular) will contain one embedding\n",
    "            per token. If offsets are not provided, the entire tensor of wordpiece embeddings\n",
    "            will be returned.\n",
    "        token_type_ids : ``torch.LongTensor``, optional\n",
    "            If an input consists of two sentences (as in the BERT paper),\n",
    "            tokens from the first sentence should have type 0 and tokens from\n",
    "            the second sentence should have type 1.  If you don't provide this\n",
    "            (the default BertIndexer doesn't) then it's assumed to be all 0s.\n",
    "        \"\"\"\n",
    "        # pylint: disable=arguments-differ\n",
    "        if token_type_ids is None:\n",
    "            token_type_ids = torch.zeros_like(input_ids)\n",
    "\n",
    "        input_mask = (input_ids != 0).long()\n",
    "\n",
    "        # input_ids may have extra dimensions, so we reshape down to 2-d\n",
    "        # before calling the BERT model and then reshape back at the end.\n",
    "        all_encoder_layers, _ = self.bert_model(input_ids=nn_util.combine_initial_dims(input_ids),\n",
    "                                                token_type_ids=nn_util.combine_initial_dims(token_type_ids),\n",
    "                                                attention_mask=nn_util.combine_initial_dims(input_mask))\n",
    "        if self._scalar_mix is not None:\n",
    "            mix = self._scalar_mix(all_encoder_layers[-self.n_hidden_layers:], input_mask)\n",
    "        else:\n",
    "            mix = torch.stack(all_encoder_layers[-self.n_hidden_layers:]).mean(dim=0)\n",
    "\n",
    "        # At this point, mix is (batch_size * d1 * ... * dn, sequence_length, embedding_dim)\n",
    "\n",
    "        if offsets is None:\n",
    "            # Resize to (batch_size, d1, ..., dn, sequence_length, embedding_dim)\n",
    "            return nn_util.uncombine_initial_dims(mix, input_ids.size())\n",
    "        else:\n",
    "            # offsets is (batch_size, d1, ..., dn, orig_sequence_length)\n",
    "            offsets2d = nn_util.combine_initial_dims(offsets)\n",
    "            # now offsets is (batch_size * d1 * ... * dn, orig_sequence_length)\n",
    "            range_vector = nn_util.get_range_vector(offsets2d.size(0),\n",
    "                                                 device=nn_util.get_device_of(mix)).unsqueeze(1)\n",
    "            # selected embeddings is also (batch_size * d1 * ... * dn, orig_sequence_length)\n",
    "            selected_embeddings = mix[range_vector, offsets2d]\n",
    "\n",
    "            return util.uncombine_initial_dims(selected_embeddings, offsets.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.modules.text_field_embedders import BasicTextFieldEmbedder\n",
    "\n",
    "if config.model_type == \"standard\":\n",
    "    token_embedding = CustomEmbedding(num_embeddings=config.vocab_size + 5,\n",
    "                                      embedding_dim=config.embedding_dim,\n",
    "                                      trainable=not config.freeze_embeddings,\n",
    "                                      weight=torch.tensor(embedding_weights, dtype=torch.float),\n",
    "                                      dropout=config.dropoute, padding_index=0)\n",
    "    word_embeddings = BasicTextFieldEmbedder({\"tokens\": token_embedding})\n",
    "    \n",
    "elif \"elmo\" in config.model_type:\n",
    "    from allennlp.modules.token_embedders import ElmoTokenEmbedder\n",
    "    from allennlp.modules.elmo import Elmo\n",
    "\n",
    "    options_file = 'https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x1024_128_2048cnn_1xhighway/elmo_2x1024_128_2048cnn_1xhighway_options.json'\n",
    "    weight_file = 'https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x1024_128_2048cnn_1xhighway/elmo_2x1024_128_2048cnn_1xhighway_weights.hdf5'\n",
    "    \n",
    "    all_words_ordered = [w for i, w in sorted([(i, w) for w, i in vocab.get_token_to_index_vocabulary().items()])]\n",
    "    elmo_embedder = ElmoTokenEmbedder(\n",
    "        options_file, weight_file, dropout=0.5, # recommended value\n",
    "        vocab_to_cache=all_words_ordered if config.cache_elmo_embeddings else None,\n",
    "        do_layer_norm=False,\n",
    "        requires_grad=False,\n",
    "    )\n",
    "    # TODO: Find a way to skip character encodings\n",
    "    word_embeddings = ElmoTextFieldEmbedder({\"tokens\": elmo_embedder})\n",
    "    \n",
    "elif \"bert\" in config.model_type:\n",
    "    from allennlp.modules.token_embedders.bert_token_embedder import PretrainedBertEmbedder\n",
    "    bert_embedder = CustomBertEmbedder(\n",
    "            pretrained_model=config.model_type,\n",
    "            fine_tune=False, use_scalar_mix=False,\n",
    "    )\n",
    "    word_embeddings: TextFieldEmbedder = BasicTextFieldEmbedder({\"tokens\": bert_embedder},\n",
    "                                                                 # we'll be ignoring masks so we'll need to set this to True\n",
    "                                                                allow_unmatched_keys = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.pooling_type != \"bert_pool\":\n",
    "    embed_sz = word_embeddings.get_output_dim()\n",
    "    if config.use_word_level_features: \n",
    "        embed_sz += len(word_level_features)\n",
    "    rnn = BiRNN(rnn_type=rnn_type, n_layers=config.num_layers, \n",
    "                embed_sz=embed_sz, hidden_sz=config.hidden_sz, \n",
    "                dropoutw=config.dropoutw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.pooling_type != \"bert_pool\":\n",
    "    if config.pooling_type == \"attention\":\n",
    "        pooler = Attention(rnn.get_output_dim(), hidden_sz=rnn.get_output_dim(),\n",
    "                           out_sz=rnn.get_output_dim(), dim=1, \n",
    "                           use_bias=config.attention_bias)\n",
    "    elif config.pooling_type == \"multipool\":\n",
    "        pooler = MultiPooling(rnn.get_output_dim())\n",
    "    elif config.pooling_type == \"augmented_multipool\":\n",
    "        pooler = AugmentedMultiPool(rnn.get_output_dim(), \n",
    "                                    aug_sz=embed_sz)\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid pooling type {config.pooling_type}\")\n",
    "\n",
    "    encoder = BiRNNEncoder(\n",
    "            rnn,\n",
    "            pooler,\n",
    "            dropouti=config.dropouti,\n",
    "            dropoutr=config.dropoutr,\n",
    "        )\n",
    "else:\n",
    "    BERT_DIM = word_embeddings.get_output_dim()\n",
    "\n",
    "    class BertSentencePooler(Seq2VecEncoder):\n",
    "        def forward(self, embs: torch.tensor, \n",
    "                    mask: torch.tensor=None,\n",
    "                    **kwargs,\n",
    "                   ) -> torch.tensor:\n",
    "            # extract first token tensor\n",
    "            return embs[:, 0]\n",
    "\n",
    "        @overrides\n",
    "        def get_output_dim(self) -> int:\n",
    "            return BERT_DIM\n",
    "\n",
    "    encoder = BertSentencePooler(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.n_classes > 2:\n",
    "    if config.use_focal_loss:\n",
    "        loss = FocalLossWithLogits(config.focal_loss_alpha,\n",
    "                                   config.focal_loss_gamma)\n",
    "    elif config.use_mbern_loss:\n",
    "        loss = MBernLossWithLogits()\n",
    "        \n",
    "    else:\n",
    "        loss = nn.BCEWithLogitsLoss()\n",
    "else:\n",
    "    loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BaselineModel(\n",
    "    word_embeddings, \n",
    "    encoder, \n",
    "    loss,\n",
    "    out_sz=2**config.n_classes if use_mbern_loss else config.n_classes\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss._get_name()==\"MBernLossWithLogits\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaselineModel(\n",
       "  (word_embeddings): BasicTextFieldEmbedder(\n",
       "    (token_embedder_tokens): CustomEmbedding()\n",
       "  )\n",
       "  (encoder): BiRNNEncoder(\n",
       "    (dropouti): VariationalDropout()\n",
       "    (rnn): BiRNN(\n",
       "      (rnns): ModuleList(\n",
       "        (0): PytorchSeq2SeqWrapper(\n",
       "          (_module): LSTM(\n",
       "            300, 128, num_layers=2, batch_first=True, bidirectional=True\n",
       "            (input_drop): VariationalDropout()\n",
       "            (output_drop): VariationalDropout()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (dropouto): VariationalDropout()\n",
       "    (pool): AugmentedMultiPool(\n",
       "      (attn): Attention(\n",
       "        (l1): Linear(in_features=256, out_features=512, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (projection): Sequential(\n",
       "    (0): Linear(in_features=768, out_features=50, bias=True)\n",
       "    (1): ELU(alpha=1.0)\n",
       "    (2): Dropout(p=0.1)\n",
       "    (3): Linear(in_features=50, out_features=6, bias=True)\n",
       "  )\n",
       "  (loss): BCEWithLogitsLoss()\n",
       ")"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize bias according to prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.bias_init:\n",
    "    \n",
    "    class_bias = torch.zeros(2**config.n_classes if use_mbern_loss else config.n_classes)\n",
    "    \n",
    "    for i, _ in enumerate(label_cols):\n",
    "        p = train_labels[:, i].mean()\n",
    "        class_bias[i] = np.log(p / (1-p))\n",
    "\n",
    "    model.projection[-1].bias.data = class_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_GPU: model.cuda()\n",
    "else: model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), DATA_ROOT / \"tmp_model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic sanity checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = nn_util.move_to_device(batch, 0 if USE_GPU else -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = batch[\"tokens\"]\n",
    "labels = batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': {'tokens': tensor([[  111,   594,    51,  ...,     0,     0,     0],\n",
       "          [    7,    92,   161,  ...,     0,     0,     0],\n",
       "          [ 8320,    32, 48729,  ...,     0,     0,     0],\n",
       "          ...,\n",
       "          [    5,   224,     3,  ...,   119,     2,     5],\n",
       "          [    5, 30802,  1742,  ...,     5,     0,     0],\n",
       "          [    5,  1374,   650,  ...,     5,     0,     0]], device='cuda:0')},\n",
       " 'word_level_features': tensor([[[0.1667, 0.0000, 1.0000],\n",
       "          [0.0000, 0.0000, 1.0000],\n",
       "          [0.0000, 0.0000, 1.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000]],\n",
       " \n",
       "         [[0.0000, 0.0000, 1.0000],\n",
       "          [0.0000, 0.0000, 1.0000],\n",
       "          [0.0000, 0.0000, 1.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000]],\n",
       " \n",
       "         [[0.0000, 0.1429, 1.0000],\n",
       "          [0.0000, 1.0000, 1.0000],\n",
       "          [0.1667, 0.0000, 1.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[0.0000, 1.0000, 1.0000],\n",
       "          [0.2000, 0.0000, 1.0000],\n",
       "          [0.0000, 0.0000, 1.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 1.0000],\n",
       "          [0.0000, 1.0000, 1.0000],\n",
       "          [0.0000, 1.0000, 1.0000]],\n",
       " \n",
       "         [[0.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 0.0000, 1.0000],\n",
       "          [0.2500, 0.0000, 1.0000],\n",
       "          ...,\n",
       "          [0.0000, 1.0000, 1.0000],\n",
       "          [0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000]],\n",
       " \n",
       "         [[0.0000, 1.0000, 1.0000],\n",
       "          [0.1667, 0.0000, 1.0000],\n",
       "          [0.0000, 1.0000, 1.0000],\n",
       "          ...,\n",
       "          [0.0000, 1.0000, 1.0000],\n",
       "          [0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000]]], device='cuda:0'),\n",
       " 'sentence_level_features': tensor([], device='cuda:0', size=(128, 0)),\n",
       " 'label': tensor([[0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [1., 1., 1., 0., 1., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [1., 0., 1., 0., 1., 0.],\n",
       "         [1., 0., 1., 0., 1., 1.],\n",
       "         [1., 0., 1., 0., 1., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [1., 1., 1., 0., 1., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [1., 0., 0., 1., 0., 0.],\n",
       "         [1., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [1., 0., 1., 0., 1., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [1., 0., 0., 0., 1., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [1., 0., 1., 0., 1., 1.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [1., 1., 1., 0., 1., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [1., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [1., 0., 1., 0., 1., 1.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [1., 0., 0., 0., 1., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [1., 0., 1., 0., 1., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [1., 0., 1., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [1., 0., 1., 1., 1., 1.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0.]], device='cuda:0'),\n",
       " 'id': ['0037fe4f8f5cdcfb',\n",
       "  '008faa76dd3eb890',\n",
       "  '009a3333aa4ac011',\n",
       "  '008d3702b1ad0465',\n",
       "  '0023daf96917e0d0',\n",
       "  '000897889268bc93',\n",
       "  '003fa0c68deca750',\n",
       "  '0002bcb3da6cb337',\n",
       "  '006ca45465868e64',\n",
       "  '00a00eb46f78b1d2',\n",
       "  '006b888560bcdfcd',\n",
       "  '00a3073791a5feaa',\n",
       "  '009d0fe5e7ee720a',\n",
       "  '00908f946bd2fd05',\n",
       "  '009ce66bcd5de288',\n",
       "  '006a4e493b506d1e',\n",
       "  '00190820581d90ce',\n",
       "  '00472b8e2d38d1ea',\n",
       "  '0086998b34865f93',\n",
       "  '0057e30091cf3e81',\n",
       "  '004d912a00f79c89',\n",
       "  '00582dcd527c8d7d',\n",
       "  '0036e50f42d0b679',\n",
       "  '007a5055c23cc5a6',\n",
       "  '006e87872c8b370c',\n",
       "  '005fb1983ff191e9',\n",
       "  '006a263a08b593c5',\n",
       "  '001ffdcc3e7fb49c',\n",
       "  '004e59705689a5ed',\n",
       "  '0083ead5c8afc356',\n",
       "  '00040093b2687caa',\n",
       "  '002e2d3db2b597c4',\n",
       "  '005de39c51ef844a',\n",
       "  '00054a5e18b50dd4',\n",
       "  '004b97c80705a548',\n",
       "  '003bd094feef5263',\n",
       "  '001325b8b20ea8aa',\n",
       "  '002b90cc8a94c76b',\n",
       "  '008fdd0ca51cdadc',\n",
       "  '00025465d4725e87',\n",
       "  '004af8a71399a4dc',\n",
       "  '009df88828f523c4',\n",
       "  '009cb63b1bd5daf9',\n",
       "  '006eaaaca322e12d',\n",
       "  '004a789c03eda830',\n",
       "  '003217c3eb469ba9',\n",
       "  '0007e25b2121310b',\n",
       "  '000bfd0867774845',\n",
       "  '0020fd96ed3b8c8b',\n",
       "  '007810bde7d6ebd4',\n",
       "  '0084d905b6f19f2b',\n",
       "  '00128363e367d703',\n",
       "  '0036621e4c7e10b5',\n",
       "  '0091798f05a311af',\n",
       "  '000c6a3f0cd3ba8e',\n",
       "  '0083790fbfe5014d',\n",
       "  '0097dd5c29bf7a15',\n",
       "  '0010833a96e1f886',\n",
       "  '0040017ef6277334',\n",
       "  '001c419c445b5a59',\n",
       "  '0063f77040fbc7d4',\n",
       "  '00397b264deba890',\n",
       "  '0001d958c54c6e35',\n",
       "  '0011cc71398479c4',\n",
       "  '003910ffa2f50517',\n",
       "  '0044cf18cc2655b3',\n",
       "  '003f698d06c9b180',\n",
       "  '006cf8c9f4cc4d57',\n",
       "  '0020e7119b96eeeb',\n",
       "  '0071cd1ca07040ab',\n",
       "  '00803f08f55bdcad',\n",
       "  '00637960a7ec3436',\n",
       "  '00a1aabcab9d44a0',\n",
       "  '00229d44f41f3acb',\n",
       "  '007ecbb379c4a861',\n",
       "  '0009801bd85e5806',\n",
       "  '00604eb295a1dbf2',\n",
       "  '005ba6af463ab6cf',\n",
       "  '0060ef190ee10720',\n",
       "  '0035d638ba684122',\n",
       "  '000eefc67a2c930f',\n",
       "  '00031b1e95af7921',\n",
       "  '008e0818dde894fb',\n",
       "  '00562e78e0b34102',\n",
       "  '000103f0d9cfb60f',\n",
       "  '001c557175094f10',\n",
       "  '00958dec64c33224',\n",
       "  '0034d7c78cfa6dee',\n",
       "  '003caacc6ce6c9e9',\n",
       "  '008a990457283850',\n",
       "  '0048e4ed8a0af433',\n",
       "  '006fc8cfaa4faf0b',\n",
       "  '00686325bcc16080',\n",
       "  '006493e4e9c89cab',\n",
       "  '009371b0ef213487',\n",
       "  '0053978373606ba4',\n",
       "  '009a52daa8dbb767',\n",
       "  '00169857adbc989b',\n",
       "  '0050cb3bc226f94e',\n",
       "  '007094f2f9efe035',\n",
       "  '00290e2a171dd073',\n",
       "  '001dc38a83d420cf',\n",
       "  '002918ae66cc4bc2',\n",
       "  '0097b052f0b68a96',\n",
       "  '004fb9f655230909',\n",
       "  '006b94add72ed61c',\n",
       "  '004a23742282fee4',\n",
       "  '008a856f6691e051',\n",
       "  '0066dcf7d9ecd360',\n",
       "  '0073059e6433db47',\n",
       "  '0090c1f0788dd0e9',\n",
       "  '009a6b3e571ee155',\n",
       "  '0010307a3a50a353',\n",
       "  '004fd4fb5c47c29f',\n",
       "  '00a216c00b90ce88',\n",
       "  '001e89eb3f0b0915',\n",
       "  '00316bcc0d1bc6e0',\n",
       "  '006b77d41eb4bc08',\n",
       "  '0055208c660f7894',\n",
       "  '006fda507acd9769',\n",
       "  '00521847b93eba3b',\n",
       "  '0060c5c9030b2d14',\n",
       "  '00548d029d7f4783',\n",
       "  '003d77a20601cec1',\n",
       "  '0095756047a71716',\n",
       "  '002d6c9d9f85e81f',\n",
       "  '00280c0d0652b366',\n",
       "  '002a13f2896596fa']}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': tensor([[  111,   594,    51,  ...,     0,     0,     0],\n",
       "         [    7,    92,   161,  ...,     0,     0,     0],\n",
       "         [ 8320,    32, 48729,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [    5,   224,     3,  ...,   119,     2,     5],\n",
       "         [    5, 30802,  1742,  ...,     5,     0,     0],\n",
       "         [    5,  1374,   650,  ...,     5,     0,     0]], device='cuda:0')}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = get_text_field_mask(tokens)\n",
    "wlfs = batch[\"word_level_features\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = model.word_embeddings(tokens)\n",
    "if config.use_word_level_features:\n",
    "    embeddings = torch.cat([wlfs, embeddings], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.use_word_level_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 48, 300])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 48])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.pooling_type != \"bert_pool\":\n",
    "    encoded = model.encoder.rnn(embeddings, mask=mask)\n",
    "else:\n",
    "    encoded = model.encoder(embeddings, mask=mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 48, 256])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'class_logits': tensor([[-2.2577, -4.5900, -2.8909, -5.8121, -2.9632, -4.7485],\n",
       "         [-2.2325, -4.5813, -2.8891, -5.8298, -2.9568, -4.7607],\n",
       "         [-2.2595, -4.6086, -2.9058, -5.8175, -2.9542, -4.7554],\n",
       "         [-2.2757, -4.5816, -2.9039, -5.7885, -2.9263, -4.7371],\n",
       "         [-2.2427, -4.6033, -2.8645, -5.8060, -2.9718, -4.7410],\n",
       "         [-2.2814, -4.5923, -2.8709, -5.7930, -2.9211, -4.7174],\n",
       "         [-2.2315, -4.5704, -2.8700, -5.8031, -2.9623, -4.7312],\n",
       "         [-2.2708, -4.6066, -2.8753, -5.8080, -2.9527, -4.7345],\n",
       "         [-2.2811, -4.5994, -2.9071, -5.8461, -2.9711, -4.7133],\n",
       "         [-2.2945, -4.6181, -2.9336, -5.8493, -2.9504, -4.7339],\n",
       "         [-2.2460, -4.5801, -2.8722, -5.8322, -2.9232, -4.7707],\n",
       "         [-2.2681, -4.5697, -2.8853, -5.8304, -2.9509, -4.7360],\n",
       "         [-2.2588, -4.6254, -2.9144, -5.8347, -2.9430, -4.7494],\n",
       "         [-2.2769, -4.5507, -2.8892, -5.8274, -2.9188, -4.7415],\n",
       "         [-2.2920, -4.6192, -2.8885, -5.8067, -2.9533, -4.8046],\n",
       "         [-2.2577, -4.5647, -2.8951, -5.8006, -2.9410, -4.7531],\n",
       "         [-2.2649, -4.5637, -2.8761, -5.8150, -2.9690, -4.7139],\n",
       "         [-2.2945, -4.5688, -2.8857, -5.8281, -2.9050, -4.7940],\n",
       "         [-2.2170, -4.5623, -2.8725, -5.8119, -2.9537, -4.7138],\n",
       "         [-2.2510, -4.5279, -2.8546, -5.8560, -2.9732, -4.7166],\n",
       "         [-2.2976, -4.6189, -2.9186, -5.8669, -2.9439, -4.7429],\n",
       "         [-2.2897, -4.5609, -2.8831, -5.7965, -2.9566, -4.7371],\n",
       "         [-2.2538, -4.5760, -2.8639, -5.8173, -2.9381, -4.7162],\n",
       "         [-2.2965, -4.5623, -2.8900, -5.7622, -2.9352, -4.7304],\n",
       "         [-2.3007, -4.5631, -2.8746, -5.8157, -2.9309, -4.8048],\n",
       "         [-2.3092, -4.5895, -2.9493, -5.8081, -2.9399, -4.7541],\n",
       "         [-2.2635, -4.5552, -2.9249, -5.8844, -2.9622, -4.7395],\n",
       "         [-2.2549, -4.5742, -2.8532, -5.8481, -2.9816, -4.7348],\n",
       "         [-2.2597, -4.6368, -2.8999, -5.8520, -2.9505, -4.7817],\n",
       "         [-2.3161, -4.6272, -2.8585, -5.8414, -2.9399, -4.7238],\n",
       "         [-2.2840, -4.5677, -2.8857, -5.8276, -2.9586, -4.7574],\n",
       "         [-2.2998, -4.6126, -2.8751, -5.8145, -2.9327, -4.6986],\n",
       "         [-2.3122, -4.5587, -2.9557, -5.8391, -2.9838, -4.7386],\n",
       "         [-2.2754, -4.6039, -2.8278, -5.7905, -2.9169, -4.7201],\n",
       "         [-2.3002, -4.5556, -2.8514, -5.8283, -2.9421, -4.7316],\n",
       "         [-2.3129, -4.5899, -2.8845, -5.8321, -2.9670, -4.7615],\n",
       "         [-2.3124, -4.5903, -2.9487, -5.8162, -2.8937, -4.7108],\n",
       "         [-2.3089, -4.6303, -2.9176, -5.8431, -2.9493, -4.7757],\n",
       "         [-2.2703, -4.5939, -2.8536, -5.8288, -2.9676, -4.7639],\n",
       "         [-2.3099, -4.5989, -2.9008, -5.8508, -2.9317, -4.7373],\n",
       "         [-2.3455, -4.5949, -2.9167, -5.8472, -2.9730, -4.6728],\n",
       "         [-2.3009, -4.5872, -2.8947, -5.8561, -2.9221, -4.7038],\n",
       "         [-2.3138, -4.6115, -2.8617, -5.7951, -2.8974, -4.7245],\n",
       "         [-2.3329, -4.5705, -2.9517, -5.7942, -2.9622, -4.7494],\n",
       "         [-2.3402, -4.5882, -2.9050, -5.8537, -2.9270, -4.7471],\n",
       "         [-2.2594, -4.5217, -2.8777, -5.7855, -2.9555, -4.7845],\n",
       "         [-2.3430, -4.5427, -2.8576, -5.8352, -2.9147, -4.6855],\n",
       "         [-2.3057, -4.5648, -2.9293, -5.8185, -2.9537, -4.7862],\n",
       "         [-2.2952, -4.6013, -2.9041, -5.7999, -2.8989, -4.7259],\n",
       "         [-2.3388, -4.4920, -2.8446, -5.8054, -2.9475, -4.7512],\n",
       "         [-2.3311, -4.5505, -2.9227, -5.8365, -2.9352, -4.7893],\n",
       "         [-2.3404, -4.5513, -2.8735, -5.8450, -2.9360, -4.7564],\n",
       "         [-2.2754, -4.5074, -2.9177, -5.8279, -2.9360, -4.7113],\n",
       "         [-2.3564, -4.5653, -2.9299, -5.7717, -2.8891, -4.7797],\n",
       "         [-2.3367, -4.5829, -2.8750, -5.7361, -2.9030, -4.7114],\n",
       "         [-2.2797, -4.5686, -2.9358, -5.8002, -2.9341, -4.6903],\n",
       "         [-2.2819, -4.6165, -2.8977, -5.7842, -2.8731, -4.7315],\n",
       "         [-2.2951, -4.5826, -2.8594, -5.8006, -2.9377, -4.7698],\n",
       "         [-2.3191, -4.5521, -2.9422, -5.8531, -2.9502, -4.7591],\n",
       "         [-2.2963, -4.5776, -2.8774, -5.8377, -2.9224, -4.7603],\n",
       "         [-2.3107, -4.5079, -2.8793, -5.9278, -2.9687, -4.7581],\n",
       "         [-2.2921, -4.6172, -2.8659, -5.9048, -2.9244, -4.8244],\n",
       "         [-2.3324, -4.5731, -2.9100, -5.8546, -2.9484, -4.7649],\n",
       "         [-2.3062, -4.5229, -2.8771, -5.8237, -2.9748, -4.6592],\n",
       "         [-2.2361, -4.5404, -2.8741, -5.8281, -2.9477, -4.7193],\n",
       "         [-2.2924, -4.5772, -2.9128, -5.8752, -3.0148, -4.7500],\n",
       "         [-2.3290, -4.5802, -2.9454, -5.8208, -2.8783, -4.7080],\n",
       "         [-2.2442, -4.5152, -2.8776, -5.8063, -2.9279, -4.6898],\n",
       "         [-2.3383, -4.5144, -2.9055, -5.8593, -2.9174, -4.8050],\n",
       "         [-2.2680, -4.5615, -2.8881, -5.7941, -2.9370, -4.7027],\n",
       "         [-2.3730, -4.5663, -2.9150, -5.8725, -2.9259, -4.7409],\n",
       "         [-2.2466, -4.5773, -2.8970, -5.8017, -2.8847, -4.7772],\n",
       "         [-2.3174, -4.5388, -2.9519, -5.8608, -2.8773, -4.7846],\n",
       "         [-2.3204, -4.5919, -2.9710, -5.9173, -3.0280, -4.8619],\n",
       "         [-2.2838, -4.6033, -2.8952, -5.8498, -2.8986, -4.8283],\n",
       "         [-2.2924, -4.5601, -2.9080, -5.8992, -2.9991, -4.8805],\n",
       "         [-2.3846, -4.4984, -2.9465, -5.8353, -2.9432, -4.6751],\n",
       "         [-2.3388, -4.6299, -2.9380, -5.8650, -2.9502, -4.7782],\n",
       "         [-2.3079, -4.6545, -2.9079, -5.8060, -2.9114, -4.8151],\n",
       "         [-2.3630, -4.5919, -2.9078, -5.8412, -2.8910, -4.7777],\n",
       "         [-2.3708, -4.5736, -2.9172, -5.8216, -2.9289, -4.7294],\n",
       "         [-2.2633, -4.5442, -2.8497, -5.9179, -2.9722, -4.8171],\n",
       "         [-2.3151, -4.4947, -2.8804, -5.8482, -2.9238, -4.7224],\n",
       "         [-2.3834, -4.5350, -2.9701, -5.8581, -2.9418, -4.7557],\n",
       "         [-2.3165, -4.5916, -2.8278, -5.8547, -2.9646, -4.8201],\n",
       "         [-2.2796, -4.5151, -2.9129, -5.8047, -2.8809, -4.7527],\n",
       "         [-2.3345, -4.5304, -2.8844, -5.8129, -2.9020, -4.7320],\n",
       "         [-2.3735, -4.5460, -2.8810, -5.8082, -2.9625, -4.6863],\n",
       "         [-2.4021, -4.5460, -2.9214, -5.9684, -2.9566, -4.6986],\n",
       "         [-2.2730, -4.5906, -2.8903, -5.8105, -2.9627, -4.7362],\n",
       "         [-2.2533, -4.5754, -2.9040, -5.8435, -2.9217, -4.8100],\n",
       "         [-2.2102, -4.5327, -2.8493, -5.8174, -2.9228, -4.8065],\n",
       "         [-2.2476, -4.5454, -3.0042, -5.8968, -2.8956, -4.7914],\n",
       "         [-2.3897, -4.6224, -2.9828, -5.8546, -2.8697, -4.6784],\n",
       "         [-2.3700, -4.4066, -2.9572, -5.9076, -3.0222, -4.7737],\n",
       "         [-2.4267, -4.4763, -3.0791, -5.9473, -2.9277, -4.8199],\n",
       "         [-2.3689, -4.6189, -3.0331, -5.7894, -2.8898, -4.7299],\n",
       "         [-2.3815, -4.5348, -3.0341, -5.8742, -2.8178, -4.6856],\n",
       "         [-2.3067, -4.5797, -2.9563, -5.8479, -2.9477, -4.7441],\n",
       "         [-2.3099, -4.6253, -2.9174, -5.9348, -2.8996, -4.7789],\n",
       "         [-2.3792, -4.5233, -2.9889, -5.8327, -2.9066, -4.7123],\n",
       "         [-2.2953, -4.5429, -2.9041, -5.8174, -2.9111, -4.8002],\n",
       "         [-2.3082, -4.6281, -2.9784, -5.8627, -2.9456, -4.7837],\n",
       "         [-2.3036, -4.6566, -2.8200, -5.7597, -2.9933, -4.9462],\n",
       "         [-2.3485, -4.6221, -2.9175, -5.9036, -2.9198, -4.7812],\n",
       "         [-2.3197, -4.5226, -2.9144, -5.8939, -2.9404, -4.8290],\n",
       "         [-2.2584, -4.5589, -2.9709, -5.9500, -3.0136, -4.8695],\n",
       "         [-2.3155, -4.5651, -2.9003, -5.8555, -3.0084, -4.7336],\n",
       "         [-2.4406, -4.4864, -2.9894, -5.8795, -2.8680, -4.7206],\n",
       "         [-2.3002, -4.5246, -2.9114, -5.8684, -2.9745, -4.6412],\n",
       "         [-2.3220, -4.5360, -2.9163, -5.8201, -2.8770, -4.7686],\n",
       "         [-2.3967, -4.5641, -2.9760, -5.9042, -2.8991, -4.7208],\n",
       "         [-2.3087, -4.5640, -2.9653, -5.8614, -2.8717, -4.7726],\n",
       "         [-2.3470, -4.5913, -2.9448, -5.8836, -2.9746, -4.7754],\n",
       "         [-2.3780, -4.4966, -2.9500, -5.8525, -2.9442, -4.7764],\n",
       "         [-2.3960, -4.5674, -3.0151, -5.8854, -2.9722, -4.6782],\n",
       "         [-2.3912, -4.5219, -2.9010, -5.9723, -2.9312, -4.8937],\n",
       "         [-2.3530, -4.4702, -2.8554, -5.7697, -2.9183, -4.7089],\n",
       "         [-2.2939, -4.5915, -2.8479, -5.8213, -2.9399, -4.7415],\n",
       "         [-2.4594, -4.6117, -2.9900, -6.0027, -3.0074, -4.8150],\n",
       "         [-2.4182, -4.6559, -3.0270, -5.9493, -2.9308, -4.8960],\n",
       "         [-2.3936, -4.5283, -2.9305, -5.9395, -2.9493, -4.7378],\n",
       "         [-2.3575, -4.5276, -3.0307, -6.0015, -2.9242, -4.7848],\n",
       "         [-2.4261, -4.4607, -2.9394, -5.7839, -2.9249, -4.7121],\n",
       "         [-2.5010, -4.5926, -2.9772, -5.8217, -2.9658, -4.7767],\n",
       "         [-2.4244, -4.6016, -2.9633, -5.8524, -2.9235, -4.8212],\n",
       "         [-2.2686, -4.5162, -2.8904, -5.7869, -3.0209, -4.7987],\n",
       "         [-2.3564, -4.5448, -2.9437, -5.7727, -2.8409, -4.7537]],\n",
       "        device='cuda:0', grad_fn=<AddmmBackward>),\n",
       " 'accuracy': 0.93359375,\n",
       " 'loss': tensor(0.2403, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(**batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = model(**batch)[\"loss\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2402, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"label\"].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"tokens\"][\"tokens\"].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.mixup(**batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.training import Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StopTraining(Exception): pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NanWeightMonitor(Callback):\n",
    "    def on_backward_end(self, data):\n",
    "        for name, param in self.trainer.model.named_parameters():\n",
    "            if torch.isnan(param.data).any() or torch.isinf(param.data).any():\n",
    "                raise StopTraining(f\"Nan/Inf weights in param {name}: \\n {param}\")\n",
    "try:\n",
    "    import jupyter_slack\n",
    "    can_notify = True\n",
    "except:\n",
    "    jupyter_slack = None\n",
    "    can_notify = False\n",
    "\n",
    "class SlackNotification(Callback):\n",
    "    def __init__(self, silent):\n",
    "        self.silent = silent\n",
    "    def on_train_end(self, data):\n",
    "        if not self.silent and can_notify:\n",
    "            try:\n",
    "                jupyter_slack.notify_self(f\"Finished training with state {self.trainer._state}\")\n",
    "            except: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TensorboardCallback(Callback):\n",
    "    \"\"\"For now, delegate all processing to the trainer's own methods\"\"\"\n",
    "    def on_batch_begin(self):\n",
    "        self._log_histograms_this_batch = \\\n",
    "        self.trainer._histogram_interval is not None and (\n",
    "            self.trainer._batch_num_total % self.trainer._histogram_interval == 0)\n",
    "    \n",
    "    def on_backward_end(self, loss):\n",
    "        if self._log_histograms_this_batch:\n",
    "            # get the magnitude of parameter updates for logging\n",
    "            # We need a copy of current parameters to compute magnitude of updates,\n",
    "            # and copy them to CPU so large models won't go OOM on the GPU.\n",
    "            self.param_updates = {\n",
    "                name: param.detach().cpu().clone()\n",
    "                for name, param in self.trainer.model.named_parameters()\n",
    "            }\n",
    "    \n",
    "    def on_step_end(self, loss):\n",
    "        if self._log_histograms_this_batch:\n",
    "            for name, param in self.trainer.model.named_parameters():\n",
    "                self.param_updates[name].sub_(param.detach().cpu())\n",
    "                update_norm = torch.norm(self.param_updates[name].view(-1, ))\n",
    "                param_norm = torch.norm(param.view(-1, )).cpu()\n",
    "                self.trainer._tensorboard.add_train_scalar(\n",
    "                    \"gradient_update/\" + name,\n",
    "                     update_norm / (param_norm + 1e-7),\n",
    "                     batch_num_total\n",
    "                )\n",
    "            self.param_updates = {} # release memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mixup(Callback):\n",
    "    \"\"\"Does mixup in embedding space\n",
    "    TODO: Figure out how to best handle masking...\n",
    "    \"\"\"\n",
    "    def __init__(self, weight: float, \n",
    "                 batch_iterator: Optional[Iterable[TensorDict]]=None):\n",
    "        self.weight = weight\n",
    "        self.batch_iterator = batch_iterator\n",
    "        \n",
    "    def on_batch_loss(self, batch: TensorDict, for_training=True):\n",
    "        if for_training and self.weight > 0.:\n",
    "            # use mixup iterator if exists\n",
    "            if self.batch_iterator is not None: batch = next(self.batch_iterator)\n",
    "            mixup_output_dict = self.model.mixup(**batch)\n",
    "            return mixup_output_dict[\"loss\"] * self.weight\n",
    "        else:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscreteMixup(Callback):\n",
    "    \"\"\"Mixes up and concatenates sentences within a batch\"\"\"\n",
    "    def __init__(self, weight: float, \n",
    "                 batch_iterator: Optional[Iterable[TensorDict]]=None):\n",
    "        self.weight = weight\n",
    "        self.batch_iterator = batch_iterator\n",
    "    \n",
    "    def on_batch_loss(self, batch: TensorDict, for_training=True):\n",
    "        if for_training and self.weight > 0.:\n",
    "            # use mixup iterator if exists\n",
    "            if self.batch_iterator is not None: \n",
    "                batch = next(self.batch_iterator)\n",
    "                \n",
    "            # create permutation\n",
    "            tokens, label = batch[\"tokens\"], batch[\"label\"]\n",
    "            bs = label.size(0)\n",
    "            shuf = torch.randperm(bs).to(label.device)\n",
    "            tokens2 = permute(tokens, shuf)\n",
    "            labels2 = permute(label, shuf)\n",
    "            \n",
    "            # join the sentences\n",
    "            n_tokens1 = get_text_field_mask(tokens).sum(1)\n",
    "            n_tokens2 = get_text_field_mask(tokens2).sum(1)\n",
    "            maxlen = min(config.max_seq_len, (n_tokens1 + n_tokens2).sum())\n",
    "            # TODO: Is there a faster way?\n",
    "            new_tokens = torch.zeros(bs, maxlen, \n",
    "                                     dtype=torch.long).to(label.device)\n",
    "            for i, (t1, t2) in enumerate(zip(tokens[\"tokens\"], tokens2[\"tokens\"])):\n",
    "                l1, l2 = n_tokens1[i].item(), n_tokens2[i].item()\n",
    "                new_tokens[i, :l1] = t1 # TODO: Fairly divide the capacity\n",
    "                new_tokens[i, l1:min(maxlen, l1+l2)] = \\\n",
    "                    t2[:min(maxlen-l1, l2)]\n",
    "            \n",
    "            # compute loss on new batch\n",
    "            new_batch = {k: v for k, v in batch.items()}\n",
    "            new_batch[\"tokens\"] = {\"tokens\": new_tokens}\n",
    "            new_batch[\"label\"] = new_label\n",
    "            return model(**new_batch)[\"loss\"] * self.weight\n",
    "        else:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearEpochDropoutSchedule:\n",
    "    def __init__(self, n_epochs, start_do, end_do):\n",
    "        self.n_epochs = n_epochs\n",
    "        self.start_do = start_do\n",
    "        self.end_do = end_do\n",
    "        if n_epochs > 1:\n",
    "            self.delta_do = (end_do - start_do) / (n_epochs - 1)\n",
    "        else:\n",
    "            self.delta_do = 0 # cannot change dropout if only one epoch\n",
    "\n",
    "    def __call__(self, epochs, batches_this_epoch, batches_total):\n",
    "        return self.start_do + self.delta_do * epochs\n",
    "\n",
    "class DropoutScheduler(Callback):\n",
    "    def __init__(self, \n",
    "                 module: nn.Module,\n",
    "                 schedule: Callable[[int, int, int], float]):\n",
    "        self._module = module\n",
    "        self._schedule = schedule\n",
    "        self.epoch = 0\n",
    "    \n",
    "    def on_batch_end(self, data):\n",
    "        self._module.dropout = self._schedule(self.epoch, \n",
    "                                              data[\"batches_this_epoch\"],\n",
    "                                              data[\"batch_num_total\"],\n",
    "                                             )\n",
    "    def on_epoch_end(self, data):\n",
    "        # handle per-epoch schedules\n",
    "        self.epoch += 1\n",
    "        self._module.dropout = self._schedule(self.epoch, -1, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test performance when input is all 0s\n",
    "- If our initialization works decently, the loss should barely/not move and accuracy should stay constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.training import TrainerWithCallbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.debugging:\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config.lr)\n",
    "    model.test_mode()\n",
    "    trainer = TrainerWithCallbacks(\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        iterator=iterator,\n",
    "        train_dataset=train_ds[:256],\n",
    "        cuda_device=0 if USE_GPU else -1,\n",
    "        num_epochs=5,\n",
    "    )\n",
    "    metrics = trainer.train()\n",
    "    model.load_state_dict(torch.load(DATA_ROOT / \"tmp_model.pth\"))\n",
    "    model.test_mode(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test performance on a small batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.debugging:\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config.lr)\n",
    "    state_dict = deepcopy(model.state_dict())\n",
    "    trainer = TrainerWithCallbacks(\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        iterator=iterator,\n",
    "        train_dataset=train_ds[:256],\n",
    "        cuda_device=0 if USE_GPU else -1,\n",
    "        num_epochs=50,\n",
    "    )\n",
    "    metrics = trainer.train()\n",
    "    model.load_state_dict(torch.load(DATA_ROOT / \"tmp_model.pth\"))\n",
    "    metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import math\n",
    "\n",
    "class LRFinder(Callback):\n",
    "    def __init__(self):\n",
    "        self.losses = []\n",
    "        self.lrs = []\n",
    "        self.best_loss = 1e9\n",
    "\n",
    "    def on_step_end(self, loss, **kwargs):\n",
    "        # Log the learning rate\n",
    "        self.losses.append(loss.item())\n",
    "        self.lrs.append(self.trainer.optimizer.state_dict()['param_groups'][0][\"lr\"])\n",
    "\n",
    "    def plot_loss(self, n_skip_beginning=10, n_skip_end=5):\n",
    "        plt.ylabel(\"loss\")\n",
    "        plt.xlabel(\"learning rate (log scale)\")\n",
    "        plt.plot(self.lrs[n_skip_beginning:-n_skip_end], self.losses[n_skip_beginning:-n_skip_end])\n",
    "        plt.xscale('log')\n",
    "\n",
    "    def plot_loss_change(self, sma=1, n_skip_beginning=10, n_skip_end=5, y_lim=(-0.01, 0.01)):\n",
    "        assert sma >= 1\n",
    "        derivatives = [0] * sma\n",
    "        for i in range(sma, len(self.lrs)):\n",
    "            derivative = (self.losses[i] - self.losses[i - sma]) / sma\n",
    "            derivatives.append(derivative)\n",
    "\n",
    "        plt.ylabel(\"rate of loss change\")\n",
    "        plt.xlabel(\"learning rate (log scale)\")\n",
    "        plt.plot(self.lrs[n_skip_beginning:-n_skip_end], derivatives[n_skip_beginning:-n_skip_end])\n",
    "        plt.xscale('log')\n",
    "        plt.ylim(y_lim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.find_lr:\n",
    "    from copy import deepcopy\n",
    "    \n",
    "    class ExponentialIncrease(torch.optim.lr_scheduler._LRScheduler):\n",
    "        def __init__(self, optimizer: torch.optim.Optimizer, n_iters: int,\n",
    "                     lr_start=1e-6, lr_end=2.0) -> None:\n",
    "            self.n_iters = n_iters\n",
    "            self.steps = 0\n",
    "            self.lr_start = lr_start\n",
    "            self.gamma = (lr_end / lr_start) ** (1 / n_iters)\n",
    "            super().__init__(optimizer)\n",
    "        def step(self, epoch=None): pass\n",
    "        def step_batch(self, epoch=None):\n",
    "            self.steps += 1\n",
    "            if epoch is None: epoch = self.last_epoch + 1\n",
    "            self.last_epoch = epoch\n",
    "            for param_group, learning_rate in zip(self.optimizer.param_groups, self.get_lr()):\n",
    "                param_group['lr'] = learning_rate\n",
    "        def get_lr(self):\n",
    "            return [self.lr_start * (self.gamma ** self.steps) for _ in self.base_lrs]\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-6)\n",
    "    lr_finder = LRFinder()\n",
    "    trainer = CustomTrainer(\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        iterator=iterator,\n",
    "        train_dataset=train_ds,\n",
    "        learning_rate_scheduler=ExponentialIncrease(optimizer, \n",
    "                                                    iterator.get_num_batches(train_ds)),\n",
    "        cuda_device=0 if USE_GPU else -1,\n",
    "        num_epochs=1,\n",
    "        callbacks=[lr_finder],\n",
    "    )\n",
    "    trainer.train()\n",
    "    model.load_state_dict(torch.load(DATA_ROOT / \"tmp_model.pth\"))\n",
    "    del model_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.find_lr:\n",
    "    lr_finder.plot_loss(n_skip_beginning=0, n_skip_end=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actual Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(DATA_ROOT / \"tmp_model.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"elmo\" in config.model_type:\n",
    "    # apply a small amount of l2 regularization to scalar params as recommended\n",
    "    # here: https://github.com/allenai/allennlp/blob/master/tutorials/how_to/elmo.md\n",
    "    pgroup_1 = [p for nm, p in model.named_parameters() if \"scalar_parameters\" not in nm and p.requires_grad]\n",
    "    pgroup_2 = [p for nm, p in model.named_parameters() if \"scalar_parameters\" in nm]\n",
    "    optimizer = optim.Adam([{\"params\": pgroup_1}, {\"params\": pgroup_2, \"weight_decay\": 0.001}], lr=config.lr, weight_decay=config.weight_decay)\n",
    "else:\n",
    "    optimizer = optim.Adam(model.parameters(), \n",
    "                           lr=config.lr, weight_decay=config.weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1006437"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def _prod(args):\n",
    "    acc = 1\n",
    "    for a in args: acc *= a\n",
    "    return acc\n",
    "num_trainable_params = sum([_prod(p.shape) for p in model.parameters() if p.requires_grad])\n",
    "num_trainable_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.training.learning_rate_schedulers import SlantedTriangular, CosineWithRestarts\n",
    "if config.lr_schedule == \"slanted_triangular\":\n",
    "    lr_sched = SlantedTriangular(optimizer, \n",
    "                                 num_epochs=config.epochs, \n",
    "                                 num_steps_per_epoch=iterator.get_num_batches(train_ds))\n",
    "elif config.lr_scheduler == \"cosine_annealing\":\n",
    "    lr_sched = optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer, T_max=iterator.get_num_batches(train_ds) * config.epochs,\n",
    "    )\n",
    "elif config.lr_scheduler is None:\n",
    "    lr_sched = None\n",
    "else:\n",
    "    raise ConfigurationError(f\"Invalid lr schedule {config.lr_scheduler} passed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_options = {\n",
    "    # TODO: Add appropriate learning rate scheduler\n",
    "    \"should_log_parameter_statistics\": True,\n",
    "    \"should_log_learning_rate\": True,\n",
    "    \"num_epochs\": config.epochs,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [NanWeightMonitor(), \n",
    "             SlackNotification(silent=config.testing)]\n",
    "if config.dropoute_max is not None:\n",
    "    callbacks.append(DropoutScheduler(\n",
    "        model.word_embeddings.token_embedder_tokens,\n",
    "        LinearEpochDropoutSchedule(config.epochs, config.dropoute, config.dropoute_max)\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (os.environ[\"IS_COLAB\"] != \"True\" and not config.testing):\n",
    "    SER_DIR = DATA_ROOT / \"ckpts\" / RUN_ID\n",
    "else:\n",
    "    SER_DIR = None\n",
    "\n",
    "trainer = TrainerWithCallbacks(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    iterator=iterator,\n",
    "    train_dataset=train_ds + train_aug_ds if config.use_augmented else train_ds,\n",
    "    validation_dataset=val_ds if config.val_ratio > 0.0 else None,\n",
    "    serialization_dir=SER_DIR,\n",
    "    cuda_device=0 if USE_GPU else -1,\n",
    "    gradient_accumulation_steps=config.batch_size // config.computational_batch_size,\n",
    "    callbacks=callbacks,\n",
    "    learning_rate_scheduler=lr_sched,\n",
    "    **training_options,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9757, loss: 0.0698 ||: 100%|██████████| 1247/1247 [05:28<00:00,  3.64it/s]\n",
      "accuracy: 0.9807, loss: 0.0514 ||: 100%|██████████| 1247/1247 [05:09<00:00,  3.80it/s]\n",
      "accuracy: 0.9814, loss: 0.0482 ||: 100%|██████████| 1247/1247 [05:11<00:00,  4.01it/s]\n",
      "accuracy: 0.9822, loss: 0.0454 ||: 100%|██████████| 1247/1247 [05:10<00:00,  3.62it/s]\n"
     ]
    }
   ],
   "source": [
    "metrics = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2402, device='cuda:0', grad_fn=<BinaryCrossEntropyWithLogitsBackward>)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'best_epoch': 3,\n",
       " 'peak_cpu_memory_MB': 5289.18,\n",
       " 'peak_gpu_0_memory_MB': 9238,\n",
       " 'training_duration': '00:21:06',\n",
       " 'training_start_epoch': 0,\n",
       " 'training_epochs': 3,\n",
       " 'epoch': 3,\n",
       " 'training_accuracy': 0.9821876573228636,\n",
       " 'training_loss': 0.045409041629400654,\n",
       " 'training_cpu_memory_MB': 5289.18,\n",
       " 'training_gpu_0_memory_MB': 9238}"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('word_embeddings.token_embedder_tokens.weight', Parameter containing:\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1111, -0.0014, -0.1778,  ...,  0.0634, -0.1216,  0.0393],\n",
      "        ...,\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "       device='cuda:0'))\n",
      "('encoder.rnn.rnns.0._module.weight_ih_l0', Parameter containing:\n",
      "tensor([[-0.0591, -0.0404, -0.0407,  ..., -0.0363,  0.0482, -0.0078],\n",
      "        [ 0.1964, -0.2119,  0.0575,  ..., -0.0491, -0.0206, -0.1791],\n",
      "        [ 0.1202, -0.1705,  0.1510,  ..., -0.0923,  0.0381,  0.0026],\n",
      "        ...,\n",
      "        [ 0.0358, -0.1772,  0.0225,  ..., -0.0005,  0.0669,  0.1155],\n",
      "        [-0.0146, -0.0623,  0.0395,  ...,  0.0158, -0.0169, -0.0934],\n",
      "        [ 0.1298,  0.0503,  0.0350,  ..., -0.0831, -0.1419, -0.0659]],\n",
      "       device='cuda:0', requires_grad=True))\n",
      "('encoder.rnn.rnns.0._module.weight_hh_l0', Parameter containing:\n",
      "tensor([[ 0.0226, -0.0272, -0.0560,  ...,  0.0336, -0.0505, -0.0277],\n",
      "        [ 0.0192,  0.1087,  0.0312,  ...,  0.1149, -0.0767, -0.0560],\n",
      "        [-0.0333,  0.0572,  0.0222,  ...,  0.0275, -0.0720,  0.0445],\n",
      "        ...,\n",
      "        [ 0.0093, -0.0225, -0.0797,  ..., -0.0300,  0.0042,  0.0483],\n",
      "        [-0.0097, -0.0361, -0.0633,  ..., -0.0235, -0.0204, -0.0125],\n",
      "        [-0.0279,  0.0111, -0.0415,  ...,  0.0723,  0.1115, -0.0436]],\n",
      "       device='cuda:0', requires_grad=True))\n",
      "('encoder.rnn.rnns.0._module.bias_ih_l0', Parameter containing:\n",
      "tensor([-3.9429e-02, -7.6675e-02, -3.0517e-02, -8.2215e-02, -1.7898e-01,\n",
      "        -2.4429e-02, -1.9998e-01, -1.4744e-01, -3.5129e-02, -5.8508e-02,\n",
      "        -9.7958e-02, -2.8068e-02, -4.5746e-02, -9.5107e-02, -4.4395e-02,\n",
      "        -8.2502e-02, -1.0299e-01, -1.0870e-01, -1.7972e-02, -2.8474e-02,\n",
      "        -1.2911e-01, -8.5318e-02, -7.3375e-02, -7.2390e-02, -3.3995e-02,\n",
      "        -8.1988e-02, -1.3540e-01, -2.6190e-02, -5.0064e-02, -5.9123e-02,\n",
      "        -2.8218e-02, -3.3251e-02, -4.9564e-02, -5.6765e-02, -3.2508e-02,\n",
      "        -6.7309e-02, -4.9888e-02, -2.7571e-02, -6.0838e-02, -1.1960e-01,\n",
      "        -1.2602e-01, -3.1511e-02, -8.8344e-02, -6.2722e-02, -5.3994e-02,\n",
      "        -5.7004e-02, -1.0528e-01, -2.9556e-02, -3.8796e-02, -1.7227e-03,\n",
      "        -4.6062e-02, -9.4500e-02, -3.3926e-02, -7.8582e-02, -7.4078e-02,\n",
      "        -8.2808e-02, -8.8489e-02, -8.1383e-02, -1.0883e-01, -7.8056e-02,\n",
      "        -4.1394e-02, -5.3332e-02, -4.7367e-02, -4.8379e-02, -9.5682e-02,\n",
      "        -4.5209e-02, -4.9778e-02, -6.1437e-02, -4.2289e-02, -1.3461e-02,\n",
      "        -6.8967e-02, -1.3684e-01, -9.0106e-02, -2.8859e-02, -5.6417e-02,\n",
      "        -5.8346e-02, -6.7707e-02, -6.3762e-02, -2.1142e-02, -5.5886e-02,\n",
      "        -9.3663e-02, -6.6032e-02, -4.0673e-02, -7.6903e-02, -5.4237e-02,\n",
      "        -7.9155e-02, -4.1498e-02, -5.2200e-02, -1.6950e-01, -4.9284e-02,\n",
      "        -8.2694e-02, -6.1485e-02, -8.2871e-02, -1.5437e-02, -5.4740e-02,\n",
      "        -6.7538e-02, -7.1663e-02, -5.8584e-02, -4.3831e-03, -7.6444e-02,\n",
      "        -6.8985e-03, -3.9176e-02, -4.8411e-02, -2.7422e-02, -1.7901e-01,\n",
      "        -8.9476e-02, -5.1814e-02, -8.5685e-02, -3.0536e-02, -6.0513e-02,\n",
      "        -1.6259e-02, -6.6816e-02, -5.3968e-02, -4.6641e-03, -7.8095e-02,\n",
      "        -8.5151e-02, -1.1836e-01, -8.8473e-02, -3.7337e-02, -1.4358e-01,\n",
      "        -1.7356e-02, -6.9898e-02, -8.2477e-02, -8.9030e-02, -2.9454e-02,\n",
      "        -9.4028e-02, -3.6833e-02, -6.3376e-02,  9.6190e-01,  9.9312e-01,\n",
      "         1.0227e+00,  1.0345e+00,  1.1287e+00,  8.9218e-01,  1.1632e+00,\n",
      "         1.0997e+00,  9.7124e-01,  9.4209e-01,  1.0566e+00,  9.4333e-01,\n",
      "         9.7941e-01,  9.8204e-01,  9.4557e-01,  1.0226e+00,  9.8387e-01,\n",
      "         1.1043e+00,  9.6305e-01,  9.6564e-01,  9.5319e-01,  9.9057e-01,\n",
      "         8.9805e-01,  9.8418e-01,  9.9979e-01,  9.6609e-01,  9.7383e-01,\n",
      "         9.6520e-01,  9.7927e-01,  9.5305e-01,  9.6520e-01,  9.3239e-01,\n",
      "         9.4509e-01,  9.5193e-01,  9.5181e-01,  9.5832e-01,  9.8415e-01,\n",
      "         9.8004e-01,  9.5001e-01,  9.8109e-01,  1.0967e+00,  9.6274e-01,\n",
      "         1.0534e+00,  1.0342e+00,  9.7842e-01,  9.5668e-01,  9.5687e-01,\n",
      "         1.0237e+00,  9.6591e-01,  9.7109e-01,  9.7352e-01,  9.9542e-01,\n",
      "         9.2791e-01,  1.0197e+00,  9.9046e-01,  1.0411e+00,  1.1523e+00,\n",
      "         9.8752e-01,  9.7288e-01,  9.5119e-01,  9.6877e-01,  9.8879e-01,\n",
      "         9.0778e-01,  9.8684e-01,  9.5681e-01,  1.1015e+00,  9.8010e-01,\n",
      "         9.6639e-01,  9.8390e-01,  9.2130e-01,  9.4231e-01,  9.8668e-01,\n",
      "         9.9237e-01,  9.5983e-01,  9.7482e-01,  9.9054e-01,  9.9005e-01,\n",
      "         1.0727e+00,  9.7161e-01,  9.2585e-01,  9.5966e-01,  9.6771e-01,\n",
      "         9.2963e-01,  9.2911e-01,  1.0665e+00,  1.0004e+00,  9.2682e-01,\n",
      "         9.6585e-01,  1.1166e+00,  9.7334e-01,  9.8623e-01,  9.8479e-01,\n",
      "         1.0207e+00,  9.7477e-01,  1.0447e+00,  9.4175e-01,  9.9054e-01,\n",
      "         9.4777e-01,  1.0004e+00,  9.2516e-01,  9.5666e-01,  9.3715e-01,\n",
      "         9.9591e-01,  9.8056e-01,  1.0003e+00,  9.8223e-01,  9.8950e-01,\n",
      "         9.8424e-01,  9.7700e-01,  9.6085e-01,  9.2676e-01,  9.9078e-01,\n",
      "         9.5936e-01,  9.1341e-01,  9.7641e-01,  1.0083e+00,  1.0008e+00,\n",
      "         9.6840e-01,  9.6835e-01,  1.0530e+00,  1.0126e+00,  9.7117e-01,\n",
      "         9.5422e-01,  1.1123e+00,  9.9199e-01,  1.0513e+00,  9.7353e-01,\n",
      "         9.8600e-01,  6.3608e-03, -6.8275e-03, -2.6691e-05, -2.5214e-02,\n",
      "        -8.8800e-03, -1.1169e-02, -4.4239e-02, -2.0765e-02,  7.2747e-04,\n",
      "        -9.2773e-03, -9.3314e-03, -2.3304e-03, -5.0654e-05, -5.3885e-03,\n",
      "         7.6826e-03, -1.0724e-02,  9.1406e-03, -7.5523e-03,  8.2332e-03,\n",
      "        -1.9583e-02, -1.8372e-02,  1.1305e-02, -3.4188e-03, -6.2857e-03,\n",
      "         2.2825e-02, -5.3481e-03, -7.1477e-03,  6.7920e-03, -6.5696e-03,\n",
      "        -6.1779e-03, -3.7557e-03,  3.7840e-03, -1.6767e-02,  1.6854e-02,\n",
      "         3.5707e-03, -1.7986e-03, -3.1167e-03,  1.2799e-03,  7.5252e-03,\n",
      "        -1.3351e-03, -1.0984e-02, -6.1474e-03, -1.4008e-03,  3.9166e-03,\n",
      "         9.2137e-03, -6.1091e-03, -2.7673e-03,  8.0226e-03,  3.0295e-03,\n",
      "        -9.8294e-03,  1.5211e-02,  1.1923e-02, -4.5443e-05, -1.7092e-02,\n",
      "        -2.0061e-03, -3.9936e-03,  7.7110e-03,  3.8355e-03,  1.5387e-02,\n",
      "        -7.9338e-03, -3.1389e-03,  1.1788e-02, -1.1984e-02,  2.6547e-02,\n",
      "        -1.3364e-03,  5.6152e-04,  1.5824e-02, -4.0205e-03,  1.8833e-03,\n",
      "        -6.0348e-03,  1.3993e-02, -6.1880e-03, -6.0916e-03, -5.3456e-03,\n",
      "         1.1055e-04,  2.2651e-02, -7.4242e-04,  2.3490e-02, -1.1656e-02,\n",
      "         2.3730e-02,  1.4986e-02, -2.2914e-02,  7.7088e-03,  5.1834e-03,\n",
      "         4.8514e-03, -1.6618e-02, -1.4297e-02,  1.6948e-02, -5.1085e-03,\n",
      "         3.4685e-04, -2.4138e-03,  1.5651e-02, -7.4490e-03,  2.0969e-02,\n",
      "         5.2045e-03, -6.3339e-03, -7.5359e-04, -2.4682e-04, -1.9266e-02,\n",
      "         4.4376e-03, -6.5449e-04,  1.0252e-02, -1.4603e-02, -2.6474e-03,\n",
      "         1.6860e-02,  2.7789e-03,  2.1256e-04,  5.0544e-03,  9.5766e-04,\n",
      "         1.1406e-02, -4.1432e-03,  2.4487e-03, -2.5225e-03,  1.7870e-03,\n",
      "         1.5047e-02,  1.1843e-02, -2.4624e-03, -1.7102e-02, -1.0970e-02,\n",
      "        -1.1032e-02, -8.0429e-03,  5.5866e-03,  1.3090e-02,  7.6486e-03,\n",
      "         1.4332e-02,  1.2697e-02,  2.2847e-04, -1.3257e-02, -3.3777e-02,\n",
      "        -2.0888e-02, -5.2062e-02, -1.9341e-02, -1.2705e-02,  6.5222e-02,\n",
      "         1.6430e-02, -1.5285e-02, -2.2350e-02, -5.2651e-02, -3.6692e-02,\n",
      "        -3.5743e-02, -6.4894e-02,  1.3612e-02, -1.7334e-02, -2.7380e-02,\n",
      "        -6.7107e-02, -2.8184e-02, -2.0244e-02, -1.1677e-02, -1.6043e-02,\n",
      "        -3.7601e-02,  1.4188e-02, -3.9523e-02, -3.5856e-02, -4.0628e-02,\n",
      "        -2.1407e-02, -7.4298e-03, -3.5725e-02, -5.2432e-02, -1.1991e-02,\n",
      "        -5.9466e-04, -4.0866e-02, -3.8565e-02, -3.7789e-02, -2.1499e-02,\n",
      "        -3.4081e-02, -2.2685e-02, -5.3258e-02, -2.3264e-02, -8.8069e-03,\n",
      "        -3.2376e-02, -5.3764e-02, -3.4813e-02, -2.5288e-02, -2.3747e-02,\n",
      "        -3.3318e-03, -3.4688e-02,  2.4242e-02,  3.0080e-02, -3.3596e-02,\n",
      "         1.5733e-03, -4.1779e-02, -5.7108e-02, -5.4541e-02, -4.5340e-02,\n",
      "        -3.6718e-02, -4.8938e-02, -1.2887e-02, -3.9855e-02, -2.5386e-02,\n",
      "        -4.5175e-02, -4.5078e-03, -4.9408e-02, -2.2913e-02,  1.4474e-03,\n",
      "        -2.4807e-02, -2.9405e-02,  4.6355e-03, -1.1345e-02, -6.9099e-02,\n",
      "         1.0791e-02,  2.5544e-03, -7.9411e-03,  1.6921e-02, -4.8283e-02,\n",
      "        -4.8845e-02, -6.3784e-02, -9.8538e-03, -4.9795e-02, -2.8382e-03,\n",
      "        -7.5886e-02, -1.0782e-02, -4.2701e-02, -3.8270e-02, -7.0936e-02,\n",
      "        -2.8296e-02, -4.6562e-02, -2.4045e-02,  6.1797e-03, -3.4430e-02,\n",
      "        -9.6949e-03, -4.1435e-02, -3.6929e-03, -3.4557e-02, -5.7038e-02,\n",
      "        -5.2532e-03,  2.7518e-02,  7.1000e-02, -6.7857e-02, -3.3298e-02,\n",
      "        -3.3754e-02, -6.1375e-02, -8.8613e-04,  3.9810e-02, -3.2754e-02,\n",
      "        -2.7245e-02, -5.0485e-03, -2.5385e-02, -3.9920e-02, -3.1993e-02,\n",
      "        -3.1411e-02, -6.2690e-02,  1.9253e-02,  1.3154e-02, -2.7695e-03,\n",
      "         1.1242e-02, -5.1329e-02, -1.5737e-02, -3.9446e-02, -3.6926e-02,\n",
      "         9.2007e-03, -2.5434e-02, -4.3208e-02, -1.5462e-02,  5.5983e-02,\n",
      "        -3.0894e-02, -4.0267e-02], device='cuda:0', requires_grad=True))\n",
      "('encoder.rnn.rnns.0._module.bias_hh_l0', Parameter containing:\n",
      "tensor([-3.9429e-02, -7.6675e-02, -3.0517e-02, -8.2215e-02, -1.7898e-01,\n",
      "        -2.4429e-02, -1.9998e-01, -1.4744e-01, -3.5129e-02, -5.8508e-02,\n",
      "        -9.7958e-02, -2.8068e-02, -4.5746e-02, -9.5107e-02, -4.4395e-02,\n",
      "        -8.2502e-02, -1.0299e-01, -1.0870e-01, -1.7972e-02, -2.8474e-02,\n",
      "        -1.2911e-01, -8.5318e-02, -7.3375e-02, -7.2390e-02, -3.3995e-02,\n",
      "        -8.1988e-02, -1.3540e-01, -2.6190e-02, -5.0064e-02, -5.9123e-02,\n",
      "        -2.8218e-02, -3.3251e-02, -4.9564e-02, -5.6765e-02, -3.2508e-02,\n",
      "        -6.7309e-02, -4.9888e-02, -2.7571e-02, -6.0838e-02, -1.1960e-01,\n",
      "        -1.2602e-01, -3.1511e-02, -8.8344e-02, -6.2722e-02, -5.3994e-02,\n",
      "        -5.7004e-02, -1.0528e-01, -2.9556e-02, -3.8796e-02, -1.7227e-03,\n",
      "        -4.6062e-02, -9.4500e-02, -3.3926e-02, -7.8582e-02, -7.4078e-02,\n",
      "        -8.2808e-02, -8.8489e-02, -8.1383e-02, -1.0883e-01, -7.8056e-02,\n",
      "        -4.1394e-02, -5.3332e-02, -4.7367e-02, -4.8379e-02, -9.5682e-02,\n",
      "        -4.5209e-02, -4.9778e-02, -6.1437e-02, -4.2289e-02, -1.3461e-02,\n",
      "        -6.8967e-02, -1.3684e-01, -9.0106e-02, -2.8859e-02, -5.6417e-02,\n",
      "        -5.8346e-02, -6.7707e-02, -6.3762e-02, -2.1142e-02, -5.5886e-02,\n",
      "        -9.3663e-02, -6.6032e-02, -4.0673e-02, -7.6903e-02, -5.4237e-02,\n",
      "        -7.9155e-02, -4.1498e-02, -5.2200e-02, -1.6950e-01, -4.9284e-02,\n",
      "        -8.2694e-02, -6.1485e-02, -8.2871e-02, -1.5437e-02, -5.4740e-02,\n",
      "        -6.7538e-02, -7.1663e-02, -5.8584e-02, -4.3831e-03, -7.6444e-02,\n",
      "        -6.8985e-03, -3.9176e-02, -4.8411e-02, -2.7422e-02, -1.7901e-01,\n",
      "        -8.9476e-02, -5.1814e-02, -8.5685e-02, -3.0536e-02, -6.0513e-02,\n",
      "        -1.6259e-02, -6.6816e-02, -5.3968e-02, -4.6641e-03, -7.8095e-02,\n",
      "        -8.5151e-02, -1.1836e-01, -8.8473e-02, -3.7337e-02, -1.4358e-01,\n",
      "        -1.7356e-02, -6.9898e-02, -8.2477e-02, -8.9030e-02, -2.9454e-02,\n",
      "        -9.4028e-02, -3.6833e-02, -6.3376e-02,  9.6190e-01,  9.9312e-01,\n",
      "         1.0227e+00,  1.0345e+00,  1.1287e+00,  8.9218e-01,  1.1632e+00,\n",
      "         1.0997e+00,  9.7124e-01,  9.4209e-01,  1.0566e+00,  9.4333e-01,\n",
      "         9.7941e-01,  9.8204e-01,  9.4557e-01,  1.0226e+00,  9.8387e-01,\n",
      "         1.1043e+00,  9.6305e-01,  9.6564e-01,  9.5319e-01,  9.9057e-01,\n",
      "         8.9805e-01,  9.8418e-01,  9.9979e-01,  9.6609e-01,  9.7383e-01,\n",
      "         9.6520e-01,  9.7927e-01,  9.5305e-01,  9.6520e-01,  9.3239e-01,\n",
      "         9.4509e-01,  9.5193e-01,  9.5181e-01,  9.5832e-01,  9.8415e-01,\n",
      "         9.8004e-01,  9.5001e-01,  9.8109e-01,  1.0967e+00,  9.6274e-01,\n",
      "         1.0534e+00,  1.0342e+00,  9.7842e-01,  9.5668e-01,  9.5687e-01,\n",
      "         1.0237e+00,  9.6591e-01,  9.7109e-01,  9.7352e-01,  9.9542e-01,\n",
      "         9.2791e-01,  1.0197e+00,  9.9046e-01,  1.0411e+00,  1.1523e+00,\n",
      "         9.8752e-01,  9.7288e-01,  9.5119e-01,  9.6877e-01,  9.8879e-01,\n",
      "         9.0778e-01,  9.8684e-01,  9.5681e-01,  1.1015e+00,  9.8010e-01,\n",
      "         9.6639e-01,  9.8390e-01,  9.2130e-01,  9.4231e-01,  9.8668e-01,\n",
      "         9.9237e-01,  9.5983e-01,  9.7482e-01,  9.9054e-01,  9.9005e-01,\n",
      "         1.0727e+00,  9.7161e-01,  9.2585e-01,  9.5966e-01,  9.6771e-01,\n",
      "         9.2963e-01,  9.2911e-01,  1.0665e+00,  1.0004e+00,  9.2682e-01,\n",
      "         9.6585e-01,  1.1166e+00,  9.7334e-01,  9.8623e-01,  9.8479e-01,\n",
      "         1.0207e+00,  9.7477e-01,  1.0447e+00,  9.4175e-01,  9.9054e-01,\n",
      "         9.4777e-01,  1.0004e+00,  9.2516e-01,  9.5666e-01,  9.3715e-01,\n",
      "         9.9591e-01,  9.8056e-01,  1.0003e+00,  9.8223e-01,  9.8950e-01,\n",
      "         9.8424e-01,  9.7700e-01,  9.6085e-01,  9.2676e-01,  9.9078e-01,\n",
      "         9.5936e-01,  9.1341e-01,  9.7641e-01,  1.0083e+00,  1.0008e+00,\n",
      "         9.6840e-01,  9.6835e-01,  1.0530e+00,  1.0126e+00,  9.7117e-01,\n",
      "         9.5422e-01,  1.1123e+00,  9.9199e-01,  1.0513e+00,  9.7353e-01,\n",
      "         9.8600e-01,  6.3608e-03, -6.8275e-03, -2.6691e-05, -2.5214e-02,\n",
      "        -8.8800e-03, -1.1169e-02, -4.4239e-02, -2.0765e-02,  7.2747e-04,\n",
      "        -9.2773e-03, -9.3314e-03, -2.3304e-03, -5.0654e-05, -5.3885e-03,\n",
      "         7.6826e-03, -1.0724e-02,  9.1406e-03, -7.5523e-03,  8.2332e-03,\n",
      "        -1.9583e-02, -1.8372e-02,  1.1305e-02, -3.4188e-03, -6.2857e-03,\n",
      "         2.2825e-02, -5.3481e-03, -7.1477e-03,  6.7920e-03, -6.5696e-03,\n",
      "        -6.1779e-03, -3.7557e-03,  3.7840e-03, -1.6767e-02,  1.6854e-02,\n",
      "         3.5707e-03, -1.7986e-03, -3.1167e-03,  1.2799e-03,  7.5252e-03,\n",
      "        -1.3351e-03, -1.0984e-02, -6.1474e-03, -1.4008e-03,  3.9166e-03,\n",
      "         9.2137e-03, -6.1091e-03, -2.7673e-03,  8.0226e-03,  3.0295e-03,\n",
      "        -9.8294e-03,  1.5211e-02,  1.1923e-02, -4.5443e-05, -1.7092e-02,\n",
      "        -2.0061e-03, -3.9936e-03,  7.7110e-03,  3.8355e-03,  1.5387e-02,\n",
      "        -7.9338e-03, -3.1389e-03,  1.1788e-02, -1.1984e-02,  2.6547e-02,\n",
      "        -1.3364e-03,  5.6152e-04,  1.5824e-02, -4.0205e-03,  1.8833e-03,\n",
      "        -6.0348e-03,  1.3993e-02, -6.1880e-03, -6.0916e-03, -5.3456e-03,\n",
      "         1.1055e-04,  2.2651e-02, -7.4242e-04,  2.3490e-02, -1.1656e-02,\n",
      "         2.3730e-02,  1.4986e-02, -2.2914e-02,  7.7088e-03,  5.1834e-03,\n",
      "         4.8514e-03, -1.6618e-02, -1.4297e-02,  1.6948e-02, -5.1085e-03,\n",
      "         3.4685e-04, -2.4138e-03,  1.5651e-02, -7.4490e-03,  2.0969e-02,\n",
      "         5.2045e-03, -6.3339e-03, -7.5359e-04, -2.4682e-04, -1.9266e-02,\n",
      "         4.4376e-03, -6.5449e-04,  1.0252e-02, -1.4603e-02, -2.6474e-03,\n",
      "         1.6860e-02,  2.7789e-03,  2.1256e-04,  5.0544e-03,  9.5766e-04,\n",
      "         1.1406e-02, -4.1432e-03,  2.4487e-03, -2.5225e-03,  1.7870e-03,\n",
      "         1.5047e-02,  1.1843e-02, -2.4624e-03, -1.7102e-02, -1.0970e-02,\n",
      "        -1.1032e-02, -8.0429e-03,  5.5866e-03,  1.3090e-02,  7.6486e-03,\n",
      "         1.4332e-02,  1.2697e-02,  2.2847e-04, -1.3257e-02, -3.3777e-02,\n",
      "        -2.0888e-02, -5.2062e-02, -1.9341e-02, -1.2705e-02,  6.5222e-02,\n",
      "         1.6430e-02, -1.5285e-02, -2.2350e-02, -5.2651e-02, -3.6692e-02,\n",
      "        -3.5743e-02, -6.4894e-02,  1.3612e-02, -1.7334e-02, -2.7380e-02,\n",
      "        -6.7107e-02, -2.8184e-02, -2.0244e-02, -1.1677e-02, -1.6043e-02,\n",
      "        -3.7601e-02,  1.4188e-02, -3.9523e-02, -3.5856e-02, -4.0628e-02,\n",
      "        -2.1407e-02, -7.4298e-03, -3.5725e-02, -5.2432e-02, -1.1991e-02,\n",
      "        -5.9466e-04, -4.0866e-02, -3.8565e-02, -3.7789e-02, -2.1499e-02,\n",
      "        -3.4081e-02, -2.2685e-02, -5.3258e-02, -2.3264e-02, -8.8069e-03,\n",
      "        -3.2376e-02, -5.3764e-02, -3.4813e-02, -2.5288e-02, -2.3747e-02,\n",
      "        -3.3318e-03, -3.4688e-02,  2.4242e-02,  3.0080e-02, -3.3596e-02,\n",
      "         1.5733e-03, -4.1779e-02, -5.7108e-02, -5.4541e-02, -4.5340e-02,\n",
      "        -3.6718e-02, -4.8938e-02, -1.2887e-02, -3.9855e-02, -2.5386e-02,\n",
      "        -4.5175e-02, -4.5078e-03, -4.9408e-02, -2.2913e-02,  1.4474e-03,\n",
      "        -2.4807e-02, -2.9405e-02,  4.6355e-03, -1.1345e-02, -6.9099e-02,\n",
      "         1.0791e-02,  2.5544e-03, -7.9411e-03,  1.6921e-02, -4.8283e-02,\n",
      "        -4.8845e-02, -6.3784e-02, -9.8538e-03, -4.9795e-02, -2.8382e-03,\n",
      "        -7.5886e-02, -1.0782e-02, -4.2701e-02, -3.8270e-02, -7.0936e-02,\n",
      "        -2.8296e-02, -4.6562e-02, -2.4045e-02,  6.1797e-03, -3.4430e-02,\n",
      "        -9.6949e-03, -4.1435e-02, -3.6929e-03, -3.4557e-02, -5.7038e-02,\n",
      "        -5.2532e-03,  2.7518e-02,  7.1000e-02, -6.7857e-02, -3.3298e-02,\n",
      "        -3.3754e-02, -6.1375e-02, -8.8613e-04,  3.9810e-02, -3.2754e-02,\n",
      "        -2.7245e-02, -5.0485e-03, -2.5385e-02, -3.9920e-02, -3.1993e-02,\n",
      "        -3.1411e-02, -6.2690e-02,  1.9253e-02,  1.3154e-02, -2.7695e-03,\n",
      "         1.1242e-02, -5.1329e-02, -1.5737e-02, -3.9446e-02, -3.6926e-02,\n",
      "         9.2007e-03, -2.5434e-02, -4.3208e-02, -1.5462e-02,  5.5983e-02,\n",
      "        -3.0894e-02, -4.0267e-02], device='cuda:0', requires_grad=True))\n",
      "('encoder.rnn.rnns.0._module.weight_ih_l0_reverse', Parameter containing:\n",
      "tensor([[ 0.0272,  0.1115, -0.1181,  ...,  0.0075, -0.0671,  0.0938],\n",
      "        [-0.1819, -0.1950, -0.2218,  ..., -0.0463,  0.1269,  0.0012],\n",
      "        [ 0.1002,  0.0168,  0.0064,  ...,  0.0129, -0.1065,  0.1340],\n",
      "        ...,\n",
      "        [ 0.0591,  0.1516,  0.0222,  ..., -0.0892, -0.0013, -0.0623],\n",
      "        [ 0.0951, -0.0094,  0.0820,  ..., -0.0710,  0.0076, -0.1164],\n",
      "        [-0.0219,  0.1327,  0.0223,  ..., -0.0766,  0.1436, -0.2140]],\n",
      "       device='cuda:0', requires_grad=True))\n",
      "('encoder.rnn.rnns.0._module.weight_hh_l0_reverse', Parameter containing:\n",
      "tensor([[ 0.1229, -0.0880,  0.0012,  ...,  0.2095,  0.1638, -0.1301],\n",
      "        [-0.0046,  0.0083, -0.0391,  ...,  0.0976, -0.0738, -0.0401],\n",
      "        [ 0.0004, -0.0222,  0.0416,  ...,  0.0148, -0.0309, -0.0826],\n",
      "        ...,\n",
      "        [-0.0571, -0.0363, -0.0492,  ...,  0.0626, -0.0303, -0.0679],\n",
      "        [ 0.0276, -0.0061, -0.0191,  ...,  0.1083,  0.0584, -0.0768],\n",
      "        [-0.0789, -0.0066,  0.0057,  ...,  0.0412, -0.0724, -0.0335]],\n",
      "       device='cuda:0', requires_grad=True))\n",
      "('encoder.rnn.rnns.0._module.bias_ih_l0_reverse', Parameter containing:\n",
      "tensor([-1.6430e-02, -2.3647e-02, -5.8874e-02, -3.5127e-02, -4.9584e-02,\n",
      "        -1.8121e-02, -1.6207e-02, -5.2718e-02,  6.5738e-03, -4.2491e-02,\n",
      "        -5.0442e-02, -5.6720e-02, -6.4200e-02, -2.2251e-02, -3.5629e-02,\n",
      "         4.8634e-03, -2.0227e-02,  5.9722e-03, -3.5033e-03, -2.7051e-02,\n",
      "        -2.3682e-02, -6.6308e-02, -5.3278e-03,  1.0359e-02, -5.4198e-02,\n",
      "         1.3154e-02, -5.6572e-02, -1.5889e-01, -1.9319e-02, -3.9547e-02,\n",
      "        -3.0697e-02, -4.9150e-02, -5.8690e-02, -8.9445e-03, -7.6593e-02,\n",
      "        -2.0401e-02, -6.6431e-03, -5.6966e-03, -5.5811e-02, -4.3464e-02,\n",
      "        -4.4225e-02, -2.4914e-02, -1.4934e-02, -1.4309e-02, -2.8936e-02,\n",
      "        -2.2750e-02, -6.6544e-02,  9.9260e-03, -2.2020e-02, -9.8528e-02,\n",
      "         4.4181e-03,  9.1313e-04, -2.8355e-02, -2.1686e-02, -4.4683e-02,\n",
      "        -1.4740e-02, -7.2657e-02, -4.1263e-02, -1.9053e-02, -4.9915e-02,\n",
      "        -4.5019e-02, -2.5031e-02, -2.7816e-02, -4.1949e-02,  4.6948e-03,\n",
      "        -1.0750e-01, -2.4244e-02, -1.8207e-02,  1.0062e-04, -1.5013e-02,\n",
      "        -7.0723e-03, -1.6815e-02, -1.4967e-02, -4.8462e-02, -2.2805e-02,\n",
      "        -5.1995e-02, -9.1742e-03, -4.5087e-02, -8.8650e-03, -2.0923e-02,\n",
      "        -1.5591e-02, -2.4127e-02, -3.9675e-02, -6.3298e-02, -2.9757e-02,\n",
      "        -2.9878e-02, -2.5627e-02, -1.8334e-02, -4.9329e-02, -5.7126e-02,\n",
      "        -1.7477e-02, -4.0333e-02, -3.0876e-02, -6.7560e-02, -1.9083e-01,\n",
      "         1.6060e-02, -6.2537e-02, -5.0071e-02, -2.2192e-02, -1.4775e-02,\n",
      "        -8.3626e-02, -5.0227e-02, -1.5213e-02, -3.9887e-02, -6.5658e-03,\n",
      "        -1.8767e-02, -4.4065e-02,  2.9701e-02,  4.7448e-03, -2.9003e-02,\n",
      "        -1.3554e-02, -4.8376e-02, -3.8938e-02, -5.1260e-02, -2.2052e-02,\n",
      "        -3.5308e-02,  8.9805e-03, -1.9088e-02,  4.2938e-03, -3.4418e-02,\n",
      "        -2.1253e-02, -2.9951e-02, -1.4470e-02, -5.1250e-02, -4.0758e-02,\n",
      "        -4.2941e-02, -1.7897e-02, -7.8620e-02,  1.0259e+00,  1.0138e+00,\n",
      "         9.2399e-01,  1.0030e+00,  9.7802e-01,  1.0152e+00,  9.6721e-01,\n",
      "         9.6171e-01,  1.0278e+00,  1.0322e+00,  9.7085e-01,  9.6183e-01,\n",
      "         9.3982e-01,  1.0192e+00,  9.8685e-01,  1.0477e+00,  9.8492e-01,\n",
      "         1.0267e+00,  1.0289e+00,  9.9706e-01,  1.0170e+00,  9.5510e-01,\n",
      "         1.0338e+00,  1.0375e+00,  9.5423e-01,  1.0163e+00,  9.7217e-01,\n",
      "         1.1026e+00,  1.0187e+00,  9.9646e-01,  9.8140e-01,  9.8851e-01,\n",
      "         9.9259e-01,  1.0235e+00,  1.0356e+00,  1.0075e+00,  1.0408e+00,\n",
      "         1.0379e+00,  9.7932e-01,  1.0028e+00,  9.2213e-01,  1.0235e+00,\n",
      "         9.9726e-01,  1.0063e+00,  9.6722e-01,  9.8415e-01,  9.6059e-01,\n",
      "         1.0542e+00,  9.6654e-01,  9.7932e-01,  1.0053e+00,  1.0219e+00,\n",
      "         1.0108e+00,  1.0071e+00,  9.8938e-01,  1.0268e+00,  1.0084e+00,\n",
      "         9.9717e-01,  1.0008e+00,  9.2099e-01,  9.6160e-01,  9.9824e-01,\n",
      "         9.7391e-01,  9.8268e-01,  1.0253e+00,  1.0117e+00,  9.5068e-01,\n",
      "         9.9680e-01,  1.0437e+00,  1.0203e+00,  1.0067e+00,  1.0199e+00,\n",
      "         9.9780e-01,  1.0049e+00,  9.9307e-01,  9.8935e-01,  1.0152e+00,\n",
      "         9.6268e-01,  1.0210e+00,  1.0203e+00,  9.9189e-01,  9.9267e-01,\n",
      "         1.0077e+00,  9.6401e-01,  9.8171e-01,  9.9149e-01,  1.0242e+00,\n",
      "         9.9314e-01,  9.5399e-01,  9.6198e-01,  1.0023e+00,  9.9005e-01,\n",
      "         9.7090e-01,  9.7271e-01,  1.0599e+00,  1.0656e+00,  9.9560e-01,\n",
      "         9.6715e-01,  9.6307e-01,  1.0399e+00,  9.8086e-01,  9.7291e-01,\n",
      "         1.0066e+00,  9.5145e-01,  1.0277e+00,  9.8539e-01,  9.9635e-01,\n",
      "         1.0231e+00,  1.0121e+00,  9.8825e-01,  1.0317e+00,  9.6341e-01,\n",
      "         9.8564e-01,  9.6438e-01,  9.8403e-01,  1.0353e+00,  1.0637e+00,\n",
      "         1.0184e+00,  1.0354e+00,  9.9156e-01,  1.0170e+00,  9.8877e-01,\n",
      "         1.0178e+00,  9.6680e-01,  9.2502e-01,  9.7810e-01,  9.9749e-01,\n",
      "         9.8726e-01, -2.4497e-03,  1.5687e-02,  2.2976e-02,  8.2615e-03,\n",
      "        -6.2175e-03, -6.4530e-03,  1.5199e-02, -1.2234e-02,  1.9139e-02,\n",
      "        -8.7816e-04, -1.3262e-02,  2.5508e-02,  1.4353e-02,  1.4872e-02,\n",
      "         1.3627e-04, -1.1251e-02,  1.6252e-02,  1.0180e-02, -1.6296e-02,\n",
      "        -2.4946e-02,  3.4409e-02,  3.6241e-03,  1.0829e-02,  5.1180e-03,\n",
      "        -1.8792e-03,  1.4673e-02, -2.4562e-02, -2.6305e-03,  2.2598e-02,\n",
      "         1.2818e-02,  1.8663e-03,  2.6303e-02,  7.3548e-03, -1.2422e-02,\n",
      "         6.8143e-04, -2.5080e-02, -7.3553e-03,  1.2475e-03, -5.5269e-03,\n",
      "         1.3093e-02,  2.1411e-02, -1.8700e-02,  1.8619e-02, -1.7697e-02,\n",
      "         5.2040e-03,  1.8497e-02, -1.9421e-02,  2.0288e-03, -1.1542e-02,\n",
      "        -1.8688e-02, -1.3942e-02, -1.2165e-02, -1.4062e-03, -2.7653e-03,\n",
      "        -1.3265e-02,  1.1524e-02, -3.4946e-03, -1.0112e-02,  2.7421e-02,\n",
      "        -1.7368e-02, -8.8891e-03, -1.6654e-02, -1.3739e-02, -1.2106e-02,\n",
      "         6.4275e-03,  1.0605e-02, -8.2966e-03,  9.4543e-03, -1.9852e-03,\n",
      "        -1.5047e-02, -2.3277e-02,  1.2247e-02, -2.1511e-02,  3.8976e-03,\n",
      "        -1.5763e-02, -4.3174e-03,  1.4501e-02, -4.6752e-04, -8.1155e-03,\n",
      "        -2.9970e-02,  9.4238e-03, -1.2305e-02,  3.9775e-02, -2.0023e-04,\n",
      "         8.9248e-03, -2.5286e-02,  1.3641e-02,  1.1524e-02,  1.9487e-02,\n",
      "         2.0014e-02, -9.8491e-03,  6.3805e-03, -2.2603e-03,  4.8588e-03,\n",
      "         6.8065e-03,  1.0000e-02, -1.0209e-02,  3.3527e-04, -3.7277e-03,\n",
      "        -7.8741e-03,  6.7780e-03, -8.4294e-03, -7.4810e-03, -1.1669e-02,\n",
      "        -3.6524e-02, -5.6108e-03, -7.5048e-03, -8.5426e-03,  1.5736e-02,\n",
      "         2.5371e-02, -1.1586e-02,  2.3254e-02,  1.5535e-02, -1.2152e-02,\n",
      "        -2.3627e-02, -3.4685e-02,  2.3355e-02,  1.2974e-02,  2.0011e-02,\n",
      "         2.9052e-03,  1.2073e-02, -1.2662e-02, -1.7631e-02, -1.3122e-02,\n",
      "        -6.2189e-04, -1.8168e-02, -3.3908e-02,  2.0868e-02, -1.8997e-02,\n",
      "         2.3377e-03, -6.1979e-02, -4.8806e-02, -3.7646e-02, -3.5958e-02,\n",
      "        -3.7846e-02, -6.7464e-02, -2.0573e-02, -3.8986e-02, -1.3593e-02,\n",
      "        -6.1960e-02, -6.9424e-02, -2.3325e-02, -3.3977e-02, -4.6261e-03,\n",
      "        -2.8055e-02, -1.7704e-02, -5.9797e-02, -3.9141e-02, -2.1094e-02,\n",
      "        -3.0427e-02, -2.9419e-02, -2.6121e-02, -2.3435e-02, -1.2337e-02,\n",
      "        -1.7356e-02, -2.2389e-02, -2.1636e-02, -4.8107e-02, -3.1114e-02,\n",
      "        -3.9922e-02,  2.6467e-02, -2.4440e-02, -1.4311e-02, -5.9236e-02,\n",
      "        -1.3029e-02,  1.7723e-03, -4.7230e-02, -3.0030e-02, -4.6055e-02,\n",
      "        -6.7083e-02, -6.4043e-02, -1.8413e-02, -3.9473e-02,  1.3383e-02,\n",
      "        -6.1697e-02,  2.2465e-03, -1.7784e-02,  3.9109e-02,  1.5799e-02,\n",
      "        -3.4482e-02, -1.6063e-02, -4.8181e-02, -1.6273e-02, -1.1227e-02,\n",
      "         1.7690e-02, -2.0415e-02, -6.1920e-02, -5.2899e-02, -3.4614e-02,\n",
      "        -5.0297e-02, -5.7330e-02, -3.9105e-02, -2.0973e-02,  2.8097e-03,\n",
      "        -9.7379e-03, -4.6004e-02, -1.0953e-02, -2.2731e-02, -6.0926e-02,\n",
      "         9.6831e-03, -5.2334e-02, -3.8989e-02, -3.4984e-02, -3.1548e-02,\n",
      "        -2.0256e-02, -2.1713e-02, -3.2254e-02, -4.9591e-02, -1.7539e-02,\n",
      "        -4.8485e-02, -4.9594e-02, -2.3151e-02, -3.6674e-02, -5.9348e-02,\n",
      "        -4.4492e-02, -5.7914e-03, -5.7746e-02, -5.0604e-02, -1.6330e-02,\n",
      "        -2.7253e-02, -2.4748e-02, -1.1075e-02, -4.3165e-03, -4.0434e-03,\n",
      "        -1.2373e-02, -5.9700e-02,  2.4330e-02, -2.0731e-02,  4.2917e-02,\n",
      "        -4.6155e-02, -4.1943e-02, -6.8624e-02, -2.9911e-02, -2.2791e-02,\n",
      "        -1.8013e-02, -1.9070e-02, -4.8326e-02, -2.7661e-02, -1.3450e-02,\n",
      "        -5.1403e-02, -5.9634e-02, -4.2071e-02, -3.8958e-02, -3.9211e-02,\n",
      "        -9.8344e-03, -3.0545e-02, -2.1596e-02, -7.1786e-03, -6.3863e-02,\n",
      "        -5.0893e-02, -4.2807e-02, -4.6094e-02, -5.3600e-02, -4.8054e-02,\n",
      "        -5.4254e-02,  5.0701e-03], device='cuda:0', requires_grad=True))\n",
      "('encoder.rnn.rnns.0._module.bias_hh_l0_reverse', Parameter containing:\n",
      "tensor([-1.6430e-02, -2.3647e-02, -5.8874e-02, -3.5127e-02, -4.9584e-02,\n",
      "        -1.8121e-02, -1.6207e-02, -5.2718e-02,  6.5738e-03, -4.2491e-02,\n",
      "        -5.0442e-02, -5.6720e-02, -6.4200e-02, -2.2251e-02, -3.5629e-02,\n",
      "         4.8634e-03, -2.0227e-02,  5.9722e-03, -3.5033e-03, -2.7051e-02,\n",
      "        -2.3682e-02, -6.6308e-02, -5.3278e-03,  1.0359e-02, -5.4198e-02,\n",
      "         1.3154e-02, -5.6572e-02, -1.5889e-01, -1.9319e-02, -3.9547e-02,\n",
      "        -3.0697e-02, -4.9150e-02, -5.8690e-02, -8.9445e-03, -7.6593e-02,\n",
      "        -2.0401e-02, -6.6431e-03, -5.6966e-03, -5.5811e-02, -4.3464e-02,\n",
      "        -4.4225e-02, -2.4914e-02, -1.4934e-02, -1.4309e-02, -2.8936e-02,\n",
      "        -2.2750e-02, -6.6544e-02,  9.9260e-03, -2.2020e-02, -9.8528e-02,\n",
      "         4.4181e-03,  9.1313e-04, -2.8355e-02, -2.1686e-02, -4.4683e-02,\n",
      "        -1.4740e-02, -7.2657e-02, -4.1263e-02, -1.9053e-02, -4.9915e-02,\n",
      "        -4.5019e-02, -2.5031e-02, -2.7816e-02, -4.1949e-02,  4.6948e-03,\n",
      "        -1.0750e-01, -2.4244e-02, -1.8207e-02,  1.0062e-04, -1.5013e-02,\n",
      "        -7.0723e-03, -1.6815e-02, -1.4967e-02, -4.8462e-02, -2.2805e-02,\n",
      "        -5.1995e-02, -9.1742e-03, -4.5087e-02, -8.8650e-03, -2.0923e-02,\n",
      "        -1.5591e-02, -2.4127e-02, -3.9675e-02, -6.3298e-02, -2.9757e-02,\n",
      "        -2.9878e-02, -2.5627e-02, -1.8334e-02, -4.9329e-02, -5.7126e-02,\n",
      "        -1.7477e-02, -4.0333e-02, -3.0876e-02, -6.7560e-02, -1.9083e-01,\n",
      "         1.6060e-02, -6.2537e-02, -5.0071e-02, -2.2192e-02, -1.4775e-02,\n",
      "        -8.3626e-02, -5.0227e-02, -1.5213e-02, -3.9887e-02, -6.5658e-03,\n",
      "        -1.8767e-02, -4.4065e-02,  2.9701e-02,  4.7448e-03, -2.9003e-02,\n",
      "        -1.3554e-02, -4.8376e-02, -3.8938e-02, -5.1260e-02, -2.2052e-02,\n",
      "        -3.5308e-02,  8.9805e-03, -1.9088e-02,  4.2938e-03, -3.4418e-02,\n",
      "        -2.1253e-02, -2.9951e-02, -1.4470e-02, -5.1250e-02, -4.0758e-02,\n",
      "        -4.2941e-02, -1.7897e-02, -7.8620e-02,  1.0259e+00,  1.0138e+00,\n",
      "         9.2399e-01,  1.0030e+00,  9.7802e-01,  1.0152e+00,  9.6721e-01,\n",
      "         9.6171e-01,  1.0278e+00,  1.0322e+00,  9.7085e-01,  9.6183e-01,\n",
      "         9.3982e-01,  1.0192e+00,  9.8685e-01,  1.0477e+00,  9.8492e-01,\n",
      "         1.0267e+00,  1.0289e+00,  9.9706e-01,  1.0170e+00,  9.5510e-01,\n",
      "         1.0338e+00,  1.0375e+00,  9.5423e-01,  1.0163e+00,  9.7217e-01,\n",
      "         1.1026e+00,  1.0187e+00,  9.9646e-01,  9.8140e-01,  9.8851e-01,\n",
      "         9.9259e-01,  1.0235e+00,  1.0356e+00,  1.0075e+00,  1.0408e+00,\n",
      "         1.0379e+00,  9.7932e-01,  1.0028e+00,  9.2213e-01,  1.0235e+00,\n",
      "         9.9726e-01,  1.0063e+00,  9.6722e-01,  9.8415e-01,  9.6059e-01,\n",
      "         1.0542e+00,  9.6654e-01,  9.7932e-01,  1.0053e+00,  1.0219e+00,\n",
      "         1.0108e+00,  1.0071e+00,  9.8938e-01,  1.0268e+00,  1.0084e+00,\n",
      "         9.9717e-01,  1.0008e+00,  9.2099e-01,  9.6160e-01,  9.9824e-01,\n",
      "         9.7391e-01,  9.8268e-01,  1.0253e+00,  1.0117e+00,  9.5068e-01,\n",
      "         9.9680e-01,  1.0437e+00,  1.0203e+00,  1.0067e+00,  1.0199e+00,\n",
      "         9.9780e-01,  1.0049e+00,  9.9307e-01,  9.8935e-01,  1.0152e+00,\n",
      "         9.6268e-01,  1.0210e+00,  1.0203e+00,  9.9189e-01,  9.9267e-01,\n",
      "         1.0077e+00,  9.6401e-01,  9.8171e-01,  9.9149e-01,  1.0242e+00,\n",
      "         9.9314e-01,  9.5399e-01,  9.6198e-01,  1.0023e+00,  9.9005e-01,\n",
      "         9.7090e-01,  9.7271e-01,  1.0599e+00,  1.0656e+00,  9.9560e-01,\n",
      "         9.6715e-01,  9.6307e-01,  1.0399e+00,  9.8086e-01,  9.7291e-01,\n",
      "         1.0066e+00,  9.5145e-01,  1.0277e+00,  9.8539e-01,  9.9635e-01,\n",
      "         1.0231e+00,  1.0121e+00,  9.8825e-01,  1.0317e+00,  9.6341e-01,\n",
      "         9.8564e-01,  9.6438e-01,  9.8403e-01,  1.0353e+00,  1.0637e+00,\n",
      "         1.0184e+00,  1.0354e+00,  9.9156e-01,  1.0170e+00,  9.8877e-01,\n",
      "         1.0178e+00,  9.6680e-01,  9.2502e-01,  9.7810e-01,  9.9749e-01,\n",
      "         9.8726e-01, -2.4497e-03,  1.5687e-02,  2.2976e-02,  8.2615e-03,\n",
      "        -6.2175e-03, -6.4530e-03,  1.5199e-02, -1.2234e-02,  1.9139e-02,\n",
      "        -8.7816e-04, -1.3262e-02,  2.5508e-02,  1.4353e-02,  1.4872e-02,\n",
      "         1.3627e-04, -1.1251e-02,  1.6252e-02,  1.0180e-02, -1.6296e-02,\n",
      "        -2.4946e-02,  3.4409e-02,  3.6241e-03,  1.0829e-02,  5.1180e-03,\n",
      "        -1.8792e-03,  1.4673e-02, -2.4562e-02, -2.6305e-03,  2.2598e-02,\n",
      "         1.2818e-02,  1.8663e-03,  2.6303e-02,  7.3548e-03, -1.2422e-02,\n",
      "         6.8143e-04, -2.5080e-02, -7.3553e-03,  1.2475e-03, -5.5269e-03,\n",
      "         1.3093e-02,  2.1411e-02, -1.8700e-02,  1.8619e-02, -1.7697e-02,\n",
      "         5.2040e-03,  1.8497e-02, -1.9421e-02,  2.0288e-03, -1.1542e-02,\n",
      "        -1.8688e-02, -1.3942e-02, -1.2165e-02, -1.4062e-03, -2.7653e-03,\n",
      "        -1.3265e-02,  1.1524e-02, -3.4946e-03, -1.0112e-02,  2.7421e-02,\n",
      "        -1.7368e-02, -8.8891e-03, -1.6654e-02, -1.3739e-02, -1.2106e-02,\n",
      "         6.4275e-03,  1.0605e-02, -8.2966e-03,  9.4543e-03, -1.9852e-03,\n",
      "        -1.5047e-02, -2.3277e-02,  1.2247e-02, -2.1511e-02,  3.8976e-03,\n",
      "        -1.5763e-02, -4.3174e-03,  1.4501e-02, -4.6752e-04, -8.1155e-03,\n",
      "        -2.9970e-02,  9.4238e-03, -1.2305e-02,  3.9775e-02, -2.0023e-04,\n",
      "         8.9248e-03, -2.5286e-02,  1.3641e-02,  1.1524e-02,  1.9487e-02,\n",
      "         2.0014e-02, -9.8491e-03,  6.3805e-03, -2.2603e-03,  4.8588e-03,\n",
      "         6.8065e-03,  1.0000e-02, -1.0209e-02,  3.3527e-04, -3.7277e-03,\n",
      "        -7.8741e-03,  6.7780e-03, -8.4294e-03, -7.4810e-03, -1.1669e-02,\n",
      "        -3.6524e-02, -5.6108e-03, -7.5048e-03, -8.5426e-03,  1.5736e-02,\n",
      "         2.5371e-02, -1.1586e-02,  2.3254e-02,  1.5535e-02, -1.2152e-02,\n",
      "        -2.3627e-02, -3.4685e-02,  2.3355e-02,  1.2974e-02,  2.0011e-02,\n",
      "         2.9052e-03,  1.2073e-02, -1.2662e-02, -1.7631e-02, -1.3122e-02,\n",
      "        -6.2189e-04, -1.8168e-02, -3.3908e-02,  2.0868e-02, -1.8997e-02,\n",
      "         2.3377e-03, -6.1979e-02, -4.8806e-02, -3.7646e-02, -3.5958e-02,\n",
      "        -3.7846e-02, -6.7464e-02, -2.0573e-02, -3.8986e-02, -1.3593e-02,\n",
      "        -6.1960e-02, -6.9424e-02, -2.3325e-02, -3.3977e-02, -4.6261e-03,\n",
      "        -2.8055e-02, -1.7704e-02, -5.9797e-02, -3.9141e-02, -2.1094e-02,\n",
      "        -3.0427e-02, -2.9419e-02, -2.6121e-02, -2.3435e-02, -1.2337e-02,\n",
      "        -1.7356e-02, -2.2389e-02, -2.1636e-02, -4.8107e-02, -3.1114e-02,\n",
      "        -3.9922e-02,  2.6467e-02, -2.4440e-02, -1.4311e-02, -5.9236e-02,\n",
      "        -1.3029e-02,  1.7723e-03, -4.7230e-02, -3.0030e-02, -4.6055e-02,\n",
      "        -6.7083e-02, -6.4043e-02, -1.8413e-02, -3.9473e-02,  1.3383e-02,\n",
      "        -6.1697e-02,  2.2465e-03, -1.7784e-02,  3.9109e-02,  1.5799e-02,\n",
      "        -3.4482e-02, -1.6063e-02, -4.8181e-02, -1.6273e-02, -1.1227e-02,\n",
      "         1.7690e-02, -2.0415e-02, -6.1920e-02, -5.2899e-02, -3.4614e-02,\n",
      "        -5.0297e-02, -5.7330e-02, -3.9105e-02, -2.0973e-02,  2.8097e-03,\n",
      "        -9.7379e-03, -4.6004e-02, -1.0953e-02, -2.2731e-02, -6.0926e-02,\n",
      "         9.6831e-03, -5.2334e-02, -3.8989e-02, -3.4984e-02, -3.1548e-02,\n",
      "        -2.0256e-02, -2.1713e-02, -3.2254e-02, -4.9591e-02, -1.7539e-02,\n",
      "        -4.8485e-02, -4.9594e-02, -2.3151e-02, -3.6674e-02, -5.9348e-02,\n",
      "        -4.4492e-02, -5.7914e-03, -5.7746e-02, -5.0604e-02, -1.6330e-02,\n",
      "        -2.7253e-02, -2.4748e-02, -1.1075e-02, -4.3165e-03, -4.0434e-03,\n",
      "        -1.2373e-02, -5.9700e-02,  2.4330e-02, -2.0731e-02,  4.2917e-02,\n",
      "        -4.6155e-02, -4.1943e-02, -6.8624e-02, -2.9911e-02, -2.2791e-02,\n",
      "        -1.8013e-02, -1.9070e-02, -4.8326e-02, -2.7661e-02, -1.3450e-02,\n",
      "        -5.1403e-02, -5.9634e-02, -4.2071e-02, -3.8958e-02, -3.9211e-02,\n",
      "        -9.8344e-03, -3.0545e-02, -2.1596e-02, -7.1786e-03, -6.3863e-02,\n",
      "        -5.0893e-02, -4.2807e-02, -4.6094e-02, -5.3600e-02, -4.8054e-02,\n",
      "        -5.4254e-02,  5.0701e-03], device='cuda:0', requires_grad=True))\n",
      "('encoder.rnn.rnns.0._module.weight_ih_l1', Parameter containing:\n",
      "tensor([[ 0.0172,  0.1612,  0.0023,  ...,  0.1113,  0.0209,  0.0497],\n",
      "        [ 0.0343,  0.0199, -0.1295,  ...,  0.0668,  0.0975, -0.1018],\n",
      "        [ 0.0302,  0.0028, -0.0579,  ...,  0.0656, -0.0354, -0.0209],\n",
      "        ...,\n",
      "        [-0.1043, -0.0162, -0.0512,  ...,  0.0835,  0.1328, -0.1345],\n",
      "        [-0.0422, -0.0717, -0.0253,  ..., -0.0695, -0.0299, -0.1159],\n",
      "        [ 0.0765, -0.0547, -0.0482,  ...,  0.1553,  0.0815, -0.1646]],\n",
      "       device='cuda:0', requires_grad=True))\n",
      "('encoder.rnn.rnns.0._module.weight_hh_l1', Parameter containing:\n",
      "tensor([[-0.0895,  0.0491, -0.1933,  ..., -0.0118, -0.0155,  0.0547],\n",
      "        [-0.0162,  0.0374, -0.0327,  ..., -0.0809, -0.0558, -0.0605],\n",
      "        [ 0.0260, -0.0187, -0.0039,  ...,  0.1461,  0.1688,  0.0092],\n",
      "        ...,\n",
      "        [ 0.0165,  0.0538,  0.0463,  ...,  0.0107,  0.0508, -0.0956],\n",
      "        [-0.0956,  0.0304, -0.0022,  ..., -0.0658, -0.0573,  0.0109],\n",
      "        [-0.0742, -0.0175, -0.0348,  ..., -0.0561,  0.0208, -0.0214]],\n",
      "       device='cuda:0', requires_grad=True))\n",
      "('encoder.rnn.rnns.0._module.bias_ih_l1', Parameter containing:\n",
      "tensor([-8.0518e-02, -1.3433e-01, -2.3195e-02, -1.0644e-01, -3.4327e-02,\n",
      "        -5.1880e-02, -5.7010e-02,  1.7729e-02, -7.7342e-02, -1.4085e-01,\n",
      "        -5.6687e-02, -8.3152e-02, -1.0917e-01, -9.7213e-02, -5.2768e-02,\n",
      "        -4.0167e-02, -1.6460e-01, -1.1321e-01, -1.5402e-01, -2.0479e-01,\n",
      "         1.8468e-02, -7.8376e-02, -1.5049e-01, -6.4075e-02, -1.7154e-01,\n",
      "         3.3348e-02, -1.4805e-01, -8.1813e-02, -9.2930e-02, -1.8597e-01,\n",
      "         5.7674e-03, -8.4534e-02, -1.1906e-01,  2.5993e-03, -1.2163e-01,\n",
      "        -1.0049e-01, -8.3789e-02, -8.5912e-02, -7.0190e-02,  2.5169e-02,\n",
      "        -6.8652e-02, -8.8500e-02, -1.0795e-01,  2.7978e-02, -1.3993e-01,\n",
      "        -2.2713e-02, -3.8854e-02, -5.1608e-02,  4.1099e-02, -8.6031e-02,\n",
      "         5.0402e-02, -5.0968e-02, -7.1112e-02, -7.6791e-02, -6.0977e-02,\n",
      "        -6.0093e-02, -2.5274e-02, -9.3872e-02, -8.2333e-02, -5.0562e-03,\n",
      "        -1.2203e-01, -7.3263e-03, -4.9046e-02, -1.0753e-01, -5.6924e-02,\n",
      "        -5.3863e-02,  1.0365e-02, -6.4304e-02, -7.8429e-02,  1.1206e-01,\n",
      "        -8.7389e-02,  1.2118e-03, -1.1664e-02, -4.2459e-02, -2.4757e-02,\n",
      "        -6.4116e-02, -1.8170e-02,  1.3118e-01, -1.3061e-01,  1.8556e-02,\n",
      "        -9.1018e-02, -9.1636e-03, -2.3652e-02, -8.7351e-02, -8.9017e-02,\n",
      "        -3.8056e-02, -9.8196e-02, -7.5254e-02, -8.3730e-02, -9.2699e-02,\n",
      "        -4.1000e-02, -1.1427e-01, -5.9305e-02, -4.7747e-02, -8.1849e-02,\n",
      "         1.1185e-02, -7.6840e-02, -3.9353e-02, -1.3634e-01, -5.1901e-02,\n",
      "        -1.2891e-01, -6.0744e-02, -3.0458e-02, -9.4096e-02, -5.8314e-02,\n",
      "        -7.3634e-02, -1.4716e-01, -7.1715e-02,  4.5674e-02, -6.1096e-02,\n",
      "        -1.0951e-01, -1.3322e-01, -2.5306e-02, -1.1714e-01,  1.4615e-01,\n",
      "        -4.8830e-02, -1.2793e-01,  1.2219e-02, -8.9387e-02, -5.8373e-02,\n",
      "        -8.0319e-02, -4.0880e-02, -8.0960e-02, -1.5125e-02,  3.9602e-02,\n",
      "         2.0709e-02, -2.1428e-02, -1.5198e-01,  9.5687e-01,  8.9304e-01,\n",
      "         9.0548e-01,  8.7748e-01,  8.5392e-01,  9.7394e-01,  8.8459e-01,\n",
      "         9.9153e-01,  9.8879e-01,  9.0881e-01,  8.6406e-01,  9.8651e-01,\n",
      "         8.7826e-01,  9.5418e-01,  9.3348e-01,  9.9894e-01,  8.9001e-01,\n",
      "         8.8099e-01,  8.8597e-01,  1.1614e+00,  1.0244e+00,  9.3334e-01,\n",
      "         8.9824e-01,  9.2964e-01,  8.6768e-01,  1.0048e+00,  8.8578e-01,\n",
      "         8.7767e-01,  9.4436e-01,  1.0155e+00,  1.0275e+00,  9.2122e-01,\n",
      "         9.1488e-01,  9.4072e-01,  7.9411e-01,  9.3980e-01,  8.3060e-01,\n",
      "         8.8924e-01,  9.6503e-01,  1.0132e+00,  9.7515e-01,  8.5438e-01,\n",
      "         1.0124e+00,  9.4855e-01,  9.3802e-01,  1.0114e+00,  9.7155e-01,\n",
      "         8.9656e-01,  9.7179e-01,  1.0015e+00,  1.0154e+00,  9.1886e-01,\n",
      "         1.0210e+00,  8.9085e-01,  9.3593e-01,  9.3160e-01,  9.4279e-01,\n",
      "         8.9889e-01,  9.3454e-01,  1.0622e+00,  9.2403e-01,  9.4754e-01,\n",
      "         9.6528e-01,  9.5576e-01,  9.2268e-01,  9.5747e-01,  1.0437e+00,\n",
      "         9.4196e-01,  9.4946e-01,  7.7721e-01,  9.0337e-01,  9.0696e-01,\n",
      "         9.2209e-01,  9.1278e-01,  8.0458e-01,  1.0138e+00,  9.3083e-01,\n",
      "         1.0403e+00,  9.3014e-01,  1.0404e+00,  9.1293e-01,  9.0905e-01,\n",
      "         8.5866e-01,  9.1161e-01,  1.0346e+00,  9.6298e-01,  9.6843e-01,\n",
      "         9.1162e-01,  9.3401e-01,  8.8346e-01,  1.0100e+00,  9.1094e-01,\n",
      "         9.2728e-01,  9.0193e-01,  8.9207e-01,  9.9586e-01,  9.1594e-01,\n",
      "         1.0005e+00,  9.5065e-01,  8.7319e-01,  8.2916e-01,  8.9040e-01,\n",
      "         9.1365e-01,  9.8078e-01,  9.6608e-01,  9.4662e-01,  9.0794e-01,\n",
      "         1.0930e+00,  1.0455e+00,  8.1261e-01,  9.2194e-01,  8.8760e-01,\n",
      "         1.1557e+00,  9.2155e-01,  1.1470e+00,  9.5680e-01,  7.9605e-01,\n",
      "         9.0735e-01,  9.5575e-01,  9.3270e-01,  9.4564e-01,  9.5527e-01,\n",
      "         1.1330e+00,  1.0109e+00,  1.0434e+00,  9.1461e-01,  1.0005e+00,\n",
      "         8.8521e-01, -2.3167e-02, -2.3705e-02,  3.1747e-02,  1.5802e-02,\n",
      "         1.2062e-02, -2.7106e-02, -2.9513e-02,  1.4714e-02, -3.3746e-02,\n",
      "         7.8789e-03,  9.4780e-03, -2.7908e-02,  5.8009e-02, -1.8803e-03,\n",
      "         1.8573e-02,  1.3563e-02, -1.5002e-02,  2.4688e-02, -4.1851e-02,\n",
      "         4.6492e-03,  2.9786e-02, -2.0265e-02, -3.8572e-02, -3.2468e-02,\n",
      "        -1.6515e-02,  9.4740e-03, -4.7193e-02,  2.3077e-02,  1.8820e-02,\n",
      "        -2.0598e-02, -5.5673e-02, -1.1249e-03, -2.0126e-03, -8.7772e-03,\n",
      "         1.7514e-03, -7.1532e-03, -1.8227e-02, -2.4236e-03,  9.4320e-03,\n",
      "        -3.6229e-02,  1.1604e-02, -4.3300e-03, -2.6699e-03,  2.2104e-02,\n",
      "        -4.2974e-03, -3.5808e-02,  2.9136e-02, -2.5767e-02,  7.4210e-03,\n",
      "        -5.0870e-03, -5.5488e-02,  3.3844e-02, -1.5914e-02, -3.4476e-03,\n",
      "         6.4557e-03, -4.3384e-03, -3.1648e-03,  3.3866e-03, -2.3048e-02,\n",
      "        -3.1855e-02, -3.6947e-02, -3.9204e-02,  2.2164e-02, -1.2270e-02,\n",
      "         1.1461e-05,  3.2595e-03,  1.7761e-02, -2.3691e-03, -1.5655e-02,\n",
      "         2.2262e-02, -2.5517e-02,  2.5611e-03, -1.1691e-03,  3.1172e-02,\n",
      "        -5.9275e-02, -1.2436e-02, -1.3136e-03, -8.7008e-02, -4.4102e-02,\n",
      "        -4.2588e-02, -5.7948e-03,  4.1833e-02,  2.8148e-02, -1.3522e-02,\n",
      "        -1.0071e-01,  3.2450e-02, -1.7205e-03, -3.8482e-02, -3.0701e-02,\n",
      "         3.5540e-02, -1.7645e-02, -6.4378e-03, -8.4665e-03,  3.4484e-02,\n",
      "        -2.1053e-02,  4.8142e-02,  3.9214e-03, -4.7688e-03,  1.5698e-02,\n",
      "         3.1600e-02, -3.4526e-02,  4.8593e-03,  3.8012e-02,  8.6952e-03,\n",
      "        -2.6423e-02,  3.7434e-03, -3.8508e-02,  1.4269e-02, -3.3329e-02,\n",
      "        -2.6013e-02, -2.5513e-02, -5.2429e-03, -7.3821e-02,  5.9501e-03,\n",
      "         5.8908e-02,  1.2968e-02, -1.6865e-02, -4.8539e-02,  2.9368e-02,\n",
      "         1.5604e-02, -3.5939e-02,  1.8059e-02, -4.2940e-02, -1.8447e-02,\n",
      "         7.0085e-02,  1.1646e-02, -4.0217e-02, -4.5452e-02, -1.6548e-01,\n",
      "        -8.3495e-02, -1.1628e-01, -8.9400e-02, -4.7456e-02, -7.4979e-02,\n",
      "        -5.9481e-02, -8.1287e-02, -1.4892e-01, -1.0340e-01, -4.7581e-02,\n",
      "        -1.2767e-01, -6.7231e-02, -8.4829e-02, -4.6879e-02, -1.0436e-01,\n",
      "        -1.4648e-01, -1.7583e-01, -1.2540e-01, -7.2279e-02, -1.9324e-01,\n",
      "        -1.0337e-01, -1.0027e-01, -3.6302e-02, -1.1521e-01, -5.8837e-02,\n",
      "        -9.1128e-02, -1.1001e-01, -9.5462e-02, -9.4765e-02, -1.5050e-01,\n",
      "        -6.0476e-02, -1.0349e-01, -1.2485e-01, -5.4723e-02, -6.4032e-02,\n",
      "        -8.9262e-02, -1.1119e-01, -5.0153e-02, -1.1035e-01, -8.6775e-02,\n",
      "        -7.8538e-02, -1.7639e-01, -8.2184e-02, -8.1771e-02, -9.5934e-02,\n",
      "        -1.0920e-01, -5.9821e-02, -6.1197e-02, -1.7877e-01, -1.4812e-01,\n",
      "        -8.3264e-02, -1.6990e-01, -1.0686e-01, -4.1834e-02, -1.1516e-01,\n",
      "        -7.1363e-02, -1.6481e-02, -5.2185e-02, -1.0096e-01, -6.7494e-02,\n",
      "        -1.3358e-01, -1.6669e-01, -4.3221e-02, -2.4841e-02, -5.3434e-02,\n",
      "        -1.4640e-01, -8.4728e-02, -1.1859e-01,  2.5740e-02, -8.3214e-02,\n",
      "        -1.1892e-01, -7.0382e-02, -9.4068e-02, -8.7005e-02, -4.8120e-02,\n",
      "        -4.3820e-02,  1.4966e-02, -9.5106e-02, -7.0447e-02, -1.0379e-01,\n",
      "        -1.3501e-01, -2.4433e-01, -5.9365e-02, -1.5334e-01, -5.8608e-02,\n",
      "        -2.9963e-02, -5.4198e-02, -3.7639e-02, -1.0668e-01, -1.3947e-01,\n",
      "        -1.1738e-01, -2.7673e-02, -7.0802e-02, -8.9687e-02, -2.1380e-01,\n",
      "        -5.4295e-02, -1.3093e-01, -3.0292e-02, -1.4404e-01, -7.3640e-02,\n",
      "        -5.8759e-02, -9.3818e-02, -1.1271e-01, -7.2344e-02, -8.4293e-02,\n",
      "        -8.8958e-02, -1.5516e-01, -8.6807e-02, -5.2022e-02, -7.3256e-02,\n",
      "        -7.2312e-02, -6.3104e-02, -7.8099e-02, -8.0089e-02, -1.4207e-01,\n",
      "        -2.0654e-01,  6.5955e-03, -6.0044e-02, -1.3036e-01, -1.5688e-01,\n",
      "        -9.6034e-02, -7.2661e-02, -5.8872e-02, -1.0304e-01, -8.2085e-02,\n",
      "        -8.9980e-02, -1.2321e-01], device='cuda:0', requires_grad=True))\n",
      "('encoder.rnn.rnns.0._module.bias_hh_l1', Parameter containing:\n",
      "tensor([-8.0518e-02, -1.3433e-01, -2.3195e-02, -1.0644e-01, -3.4327e-02,\n",
      "        -5.1880e-02, -5.7010e-02,  1.7729e-02, -7.7342e-02, -1.4085e-01,\n",
      "        -5.6687e-02, -8.3152e-02, -1.0917e-01, -9.7213e-02, -5.2768e-02,\n",
      "        -4.0167e-02, -1.6460e-01, -1.1321e-01, -1.5402e-01, -2.0479e-01,\n",
      "         1.8468e-02, -7.8376e-02, -1.5049e-01, -6.4075e-02, -1.7154e-01,\n",
      "         3.3348e-02, -1.4805e-01, -8.1813e-02, -9.2930e-02, -1.8597e-01,\n",
      "         5.7674e-03, -8.4534e-02, -1.1906e-01,  2.5993e-03, -1.2163e-01,\n",
      "        -1.0049e-01, -8.3789e-02, -8.5912e-02, -7.0190e-02,  2.5169e-02,\n",
      "        -6.8652e-02, -8.8500e-02, -1.0795e-01,  2.7978e-02, -1.3993e-01,\n",
      "        -2.2713e-02, -3.8854e-02, -5.1608e-02,  4.1099e-02, -8.6031e-02,\n",
      "         5.0402e-02, -5.0968e-02, -7.1112e-02, -7.6791e-02, -6.0977e-02,\n",
      "        -6.0093e-02, -2.5274e-02, -9.3872e-02, -8.2333e-02, -5.0562e-03,\n",
      "        -1.2203e-01, -7.3263e-03, -4.9046e-02, -1.0753e-01, -5.6924e-02,\n",
      "        -5.3863e-02,  1.0365e-02, -6.4304e-02, -7.8429e-02,  1.1206e-01,\n",
      "        -8.7389e-02,  1.2118e-03, -1.1664e-02, -4.2459e-02, -2.4757e-02,\n",
      "        -6.4116e-02, -1.8170e-02,  1.3118e-01, -1.3061e-01,  1.8556e-02,\n",
      "        -9.1018e-02, -9.1636e-03, -2.3652e-02, -8.7351e-02, -8.9017e-02,\n",
      "        -3.8056e-02, -9.8196e-02, -7.5254e-02, -8.3730e-02, -9.2699e-02,\n",
      "        -4.1000e-02, -1.1427e-01, -5.9305e-02, -4.7747e-02, -8.1849e-02,\n",
      "         1.1185e-02, -7.6840e-02, -3.9353e-02, -1.3634e-01, -5.1901e-02,\n",
      "        -1.2891e-01, -6.0744e-02, -3.0458e-02, -9.4096e-02, -5.8314e-02,\n",
      "        -7.3634e-02, -1.4716e-01, -7.1715e-02,  4.5674e-02, -6.1096e-02,\n",
      "        -1.0951e-01, -1.3322e-01, -2.5306e-02, -1.1714e-01,  1.4615e-01,\n",
      "        -4.8830e-02, -1.2793e-01,  1.2219e-02, -8.9387e-02, -5.8373e-02,\n",
      "        -8.0319e-02, -4.0880e-02, -8.0960e-02, -1.5125e-02,  3.9602e-02,\n",
      "         2.0709e-02, -2.1428e-02, -1.5198e-01,  9.5687e-01,  8.9304e-01,\n",
      "         9.0548e-01,  8.7748e-01,  8.5392e-01,  9.7394e-01,  8.8459e-01,\n",
      "         9.9153e-01,  9.8879e-01,  9.0881e-01,  8.6406e-01,  9.8651e-01,\n",
      "         8.7826e-01,  9.5418e-01,  9.3348e-01,  9.9894e-01,  8.9001e-01,\n",
      "         8.8099e-01,  8.8597e-01,  1.1614e+00,  1.0244e+00,  9.3334e-01,\n",
      "         8.9824e-01,  9.2964e-01,  8.6768e-01,  1.0048e+00,  8.8578e-01,\n",
      "         8.7767e-01,  9.4436e-01,  1.0155e+00,  1.0275e+00,  9.2122e-01,\n",
      "         9.1488e-01,  9.4072e-01,  7.9411e-01,  9.3980e-01,  8.3060e-01,\n",
      "         8.8924e-01,  9.6503e-01,  1.0132e+00,  9.7515e-01,  8.5438e-01,\n",
      "         1.0124e+00,  9.4855e-01,  9.3802e-01,  1.0114e+00,  9.7155e-01,\n",
      "         8.9656e-01,  9.7179e-01,  1.0015e+00,  1.0154e+00,  9.1886e-01,\n",
      "         1.0210e+00,  8.9085e-01,  9.3593e-01,  9.3160e-01,  9.4279e-01,\n",
      "         8.9889e-01,  9.3454e-01,  1.0622e+00,  9.2403e-01,  9.4754e-01,\n",
      "         9.6528e-01,  9.5576e-01,  9.2268e-01,  9.5747e-01,  1.0437e+00,\n",
      "         9.4196e-01,  9.4946e-01,  7.7721e-01,  9.0337e-01,  9.0696e-01,\n",
      "         9.2209e-01,  9.1278e-01,  8.0458e-01,  1.0138e+00,  9.3083e-01,\n",
      "         1.0403e+00,  9.3014e-01,  1.0404e+00,  9.1293e-01,  9.0905e-01,\n",
      "         8.5866e-01,  9.1161e-01,  1.0346e+00,  9.6298e-01,  9.6843e-01,\n",
      "         9.1162e-01,  9.3401e-01,  8.8346e-01,  1.0100e+00,  9.1094e-01,\n",
      "         9.2728e-01,  9.0193e-01,  8.9207e-01,  9.9586e-01,  9.1594e-01,\n",
      "         1.0005e+00,  9.5065e-01,  8.7319e-01,  8.2916e-01,  8.9040e-01,\n",
      "         9.1365e-01,  9.8078e-01,  9.6608e-01,  9.4662e-01,  9.0794e-01,\n",
      "         1.0930e+00,  1.0455e+00,  8.1261e-01,  9.2194e-01,  8.8760e-01,\n",
      "         1.1557e+00,  9.2155e-01,  1.1470e+00,  9.5680e-01,  7.9605e-01,\n",
      "         9.0735e-01,  9.5575e-01,  9.3270e-01,  9.4564e-01,  9.5527e-01,\n",
      "         1.1330e+00,  1.0109e+00,  1.0434e+00,  9.1461e-01,  1.0005e+00,\n",
      "         8.8521e-01, -2.3167e-02, -2.3705e-02,  3.1747e-02,  1.5802e-02,\n",
      "         1.2062e-02, -2.7106e-02, -2.9513e-02,  1.4714e-02, -3.3746e-02,\n",
      "         7.8789e-03,  9.4780e-03, -2.7908e-02,  5.8009e-02, -1.8803e-03,\n",
      "         1.8573e-02,  1.3563e-02, -1.5002e-02,  2.4688e-02, -4.1851e-02,\n",
      "         4.6492e-03,  2.9786e-02, -2.0265e-02, -3.8572e-02, -3.2468e-02,\n",
      "        -1.6515e-02,  9.4740e-03, -4.7193e-02,  2.3077e-02,  1.8820e-02,\n",
      "        -2.0598e-02, -5.5673e-02, -1.1249e-03, -2.0126e-03, -8.7772e-03,\n",
      "         1.7514e-03, -7.1532e-03, -1.8227e-02, -2.4236e-03,  9.4320e-03,\n",
      "        -3.6229e-02,  1.1604e-02, -4.3300e-03, -2.6699e-03,  2.2104e-02,\n",
      "        -4.2974e-03, -3.5808e-02,  2.9136e-02, -2.5767e-02,  7.4210e-03,\n",
      "        -5.0870e-03, -5.5488e-02,  3.3844e-02, -1.5914e-02, -3.4476e-03,\n",
      "         6.4557e-03, -4.3384e-03, -3.1648e-03,  3.3866e-03, -2.3048e-02,\n",
      "        -3.1855e-02, -3.6947e-02, -3.9204e-02,  2.2164e-02, -1.2270e-02,\n",
      "         1.1461e-05,  3.2595e-03,  1.7761e-02, -2.3691e-03, -1.5655e-02,\n",
      "         2.2262e-02, -2.5517e-02,  2.5611e-03, -1.1691e-03,  3.1172e-02,\n",
      "        -5.9275e-02, -1.2436e-02, -1.3136e-03, -8.7008e-02, -4.4102e-02,\n",
      "        -4.2588e-02, -5.7948e-03,  4.1833e-02,  2.8148e-02, -1.3522e-02,\n",
      "        -1.0071e-01,  3.2450e-02, -1.7205e-03, -3.8482e-02, -3.0701e-02,\n",
      "         3.5540e-02, -1.7645e-02, -6.4378e-03, -8.4665e-03,  3.4484e-02,\n",
      "        -2.1053e-02,  4.8142e-02,  3.9214e-03, -4.7688e-03,  1.5698e-02,\n",
      "         3.1600e-02, -3.4526e-02,  4.8593e-03,  3.8012e-02,  8.6952e-03,\n",
      "        -2.6423e-02,  3.7434e-03, -3.8508e-02,  1.4269e-02, -3.3329e-02,\n",
      "        -2.6013e-02, -2.5513e-02, -5.2429e-03, -7.3821e-02,  5.9501e-03,\n",
      "         5.8908e-02,  1.2968e-02, -1.6865e-02, -4.8539e-02,  2.9368e-02,\n",
      "         1.5604e-02, -3.5939e-02,  1.8059e-02, -4.2940e-02, -1.8447e-02,\n",
      "         7.0085e-02,  1.1646e-02, -4.0217e-02, -4.5452e-02, -1.6548e-01,\n",
      "        -8.3495e-02, -1.1628e-01, -8.9400e-02, -4.7456e-02, -7.4979e-02,\n",
      "        -5.9481e-02, -8.1287e-02, -1.4892e-01, -1.0340e-01, -4.7581e-02,\n",
      "        -1.2767e-01, -6.7231e-02, -8.4829e-02, -4.6879e-02, -1.0436e-01,\n",
      "        -1.4648e-01, -1.7583e-01, -1.2540e-01, -7.2279e-02, -1.9324e-01,\n",
      "        -1.0337e-01, -1.0027e-01, -3.6302e-02, -1.1521e-01, -5.8837e-02,\n",
      "        -9.1128e-02, -1.1001e-01, -9.5462e-02, -9.4765e-02, -1.5050e-01,\n",
      "        -6.0476e-02, -1.0349e-01, -1.2485e-01, -5.4723e-02, -6.4032e-02,\n",
      "        -8.9262e-02, -1.1119e-01, -5.0153e-02, -1.1035e-01, -8.6775e-02,\n",
      "        -7.8538e-02, -1.7639e-01, -8.2184e-02, -8.1771e-02, -9.5934e-02,\n",
      "        -1.0920e-01, -5.9821e-02, -6.1197e-02, -1.7877e-01, -1.4812e-01,\n",
      "        -8.3264e-02, -1.6990e-01, -1.0686e-01, -4.1834e-02, -1.1516e-01,\n",
      "        -7.1363e-02, -1.6481e-02, -5.2185e-02, -1.0096e-01, -6.7494e-02,\n",
      "        -1.3358e-01, -1.6669e-01, -4.3221e-02, -2.4841e-02, -5.3434e-02,\n",
      "        -1.4640e-01, -8.4728e-02, -1.1859e-01,  2.5740e-02, -8.3214e-02,\n",
      "        -1.1892e-01, -7.0382e-02, -9.4068e-02, -8.7005e-02, -4.8120e-02,\n",
      "        -4.3820e-02,  1.4966e-02, -9.5106e-02, -7.0447e-02, -1.0379e-01,\n",
      "        -1.3501e-01, -2.4433e-01, -5.9365e-02, -1.5334e-01, -5.8608e-02,\n",
      "        -2.9963e-02, -5.4198e-02, -3.7639e-02, -1.0668e-01, -1.3947e-01,\n",
      "        -1.1738e-01, -2.7673e-02, -7.0802e-02, -8.9687e-02, -2.1380e-01,\n",
      "        -5.4295e-02, -1.3093e-01, -3.0292e-02, -1.4404e-01, -7.3640e-02,\n",
      "        -5.8759e-02, -9.3818e-02, -1.1271e-01, -7.2344e-02, -8.4293e-02,\n",
      "        -8.8958e-02, -1.5516e-01, -8.6807e-02, -5.2022e-02, -7.3256e-02,\n",
      "        -7.2312e-02, -6.3104e-02, -7.8099e-02, -8.0089e-02, -1.4207e-01,\n",
      "        -2.0654e-01,  6.5955e-03, -6.0044e-02, -1.3036e-01, -1.5688e-01,\n",
      "        -9.6034e-02, -7.2661e-02, -5.8872e-02, -1.0304e-01, -8.2085e-02,\n",
      "        -8.9980e-02, -1.2321e-01], device='cuda:0', requires_grad=True))\n",
      "('encoder.rnn.rnns.0._module.weight_ih_l1_reverse', Parameter containing:\n",
      "tensor([[ 0.0678, -0.0400, -0.0619,  ...,  0.1764,  0.1591, -0.1468],\n",
      "        [-0.1141, -0.0031, -0.0311,  ...,  0.1605, -0.0323, -0.1644],\n",
      "        [ 0.1219,  0.0440, -0.0568,  ..., -0.1189, -0.0204,  0.1387],\n",
      "        ...,\n",
      "        [-0.1268,  0.0328,  0.0733,  ...,  0.0769,  0.0726, -0.1075],\n",
      "        [-0.0500, -0.1239,  0.0356,  ..., -0.0529,  0.0064, -0.1044],\n",
      "        [ 0.0469,  0.0559,  0.0037,  ...,  0.0653,  0.0467, -0.0536]],\n",
      "       device='cuda:0', requires_grad=True))\n",
      "('encoder.rnn.rnns.0._module.weight_hh_l1_reverse', Parameter containing:\n",
      "tensor([[-0.2292,  0.1027,  0.1160,  ..., -0.0548,  0.0164,  0.0477],\n",
      "        [-0.0598,  0.0227,  0.3024,  ...,  0.0581,  0.0896, -0.0768],\n",
      "        [ 0.0071, -0.0876, -0.1154,  ...,  0.0376, -0.0345,  0.1316],\n",
      "        ...,\n",
      "        [ 0.0456,  0.1282,  0.1075,  ...,  0.0108,  0.0836, -0.0060],\n",
      "        [-0.0053, -0.0499,  0.0247,  ...,  0.0488, -0.0239, -0.0243],\n",
      "        [ 0.0081,  0.0167,  0.0456,  ..., -0.0083,  0.0229, -0.0405]],\n",
      "       device='cuda:0', requires_grad=True))\n",
      "('encoder.rnn.rnns.0._module.bias_ih_l1_reverse', Parameter containing:\n",
      "tensor([-8.5194e-02,  5.0748e-02, -4.2345e-02, -8.9201e-03, -1.2594e-01,\n",
      "         1.7850e-02,  1.4268e-02, -8.2107e-02,  1.0559e-01,  3.1797e-02,\n",
      "        -3.8011e-02, -9.8749e-03, -6.3035e-02, -4.3352e-02, -6.0415e-02,\n",
      "        -1.7647e-02, -3.3878e-02,  8.7495e-02, -5.6274e-02, -2.0285e-02,\n",
      "        -7.3616e-02, -1.6452e-02, -6.3552e-02, -5.4365e-02,  3.4121e-02,\n",
      "        -5.7629e-02, -1.2424e-01, -4.4267e-02,  9.4388e-03,  2.8660e-02,\n",
      "        -1.1146e-01, -1.0919e-01, -5.5710e-02,  3.3862e-02,  2.7875e-03,\n",
      "        -5.7307e-02, -7.6660e-03, -3.0281e-02, -6.5681e-02, -3.3209e-02,\n",
      "        -8.8059e-02, -6.0699e-03, -3.0260e-02, -4.0956e-02, -1.3897e-02,\n",
      "        -7.7983e-02, -6.7759e-02, -3.6184e-02,  4.2259e-02,  2.0130e-02,\n",
      "        -9.0917e-03, -8.6644e-03, -2.6129e-02, -5.0649e-02, -5.4276e-02,\n",
      "        -1.6026e-02, -9.1224e-03, -4.8640e-02, -1.1548e-01, -7.2881e-02,\n",
      "        -8.4433e-03,  9.4591e-03, -7.3279e-02, -7.9007e-02,  3.5890e-02,\n",
      "        -4.1443e-02, -1.0185e-01, -7.0682e-03,  1.9676e-03, -5.3172e-02,\n",
      "        -1.1073e-01,  2.3811e-02, -4.5280e-02, -3.9587e-03, -6.8454e-02,\n",
      "        -7.2889e-02, -7.3230e-03, -4.8429e-02,  7.8387e-02, -3.4647e-02,\n",
      "        -6.0389e-02, -7.5968e-02, -9.8876e-02, -2.4399e-02, -7.7866e-02,\n",
      "        -8.3291e-02, -1.4140e-01, -1.0745e-01, -4.6692e-02, -4.5490e-03,\n",
      "        -5.1385e-02, -1.1350e-01, -1.2480e-02,  2.8452e-02, -6.1673e-02,\n",
      "        -2.4638e-02,  1.9051e-02, -1.6957e-02, -2.4066e-02, -4.9330e-02,\n",
      "        -5.8934e-02, -5.1905e-03, -1.2178e-01,  5.8719e-02, -1.1871e-01,\n",
      "        -5.5876e-02, -8.1458e-02, -2.0993e-02, -5.9440e-02, -1.6258e-01,\n",
      "        -7.4437e-02,  6.1088e-02, -4.4021e-02,  6.7462e-03, -5.1425e-02,\n",
      "        -5.1184e-03, -1.3289e-01, -5.5719e-02, -8.9919e-02,  5.7741e-03,\n",
      "         1.9624e-02, -3.4006e-02, -4.6006e-02, -9.0814e-02, -8.7417e-02,\n",
      "        -2.4229e-02, -5.3843e-02, -5.6102e-02,  1.0810e+00,  9.6548e-01,\n",
      "         1.0327e+00,  9.3848e-01,  8.7390e-01,  9.9234e-01,  9.3967e-01,\n",
      "         8.3122e-01,  1.0445e+00,  1.0282e+00,  9.5764e-01,  9.2140e-01,\n",
      "         1.0389e+00,  9.7235e-01,  9.6408e-01,  8.5322e-01,  8.2607e-01,\n",
      "         9.8306e-01,  9.2006e-01,  9.9426e-01,  9.4097e-01,  9.5562e-01,\n",
      "         9.4873e-01,  9.4476e-01,  1.0726e+00,  9.5022e-01,  9.3246e-01,\n",
      "         9.3657e-01,  8.8288e-01,  1.0615e+00,  9.1758e-01,  9.3764e-01,\n",
      "         9.5291e-01,  1.0102e+00,  9.6668e-01,  9.7533e-01,  1.0386e+00,\n",
      "         9.9499e-01,  9.6557e-01,  8.5403e-01,  9.1489e-01,  1.0159e+00,\n",
      "         8.4977e-01,  9.6358e-01,  9.2550e-01,  9.7680e-01,  9.3521e-01,\n",
      "         9.2931e-01,  8.9567e-01,  1.0772e+00,  1.0024e+00,  8.6580e-01,\n",
      "         9.3738e-01,  1.0129e+00,  9.5295e-01,  9.6479e-01,  9.1282e-01,\n",
      "         8.7494e-01,  9.1950e-01,  1.0174e+00,  7.9923e-01,  7.9274e-01,\n",
      "         8.7110e-01,  9.7718e-01,  1.0171e+00,  9.4445e-01,  1.0443e+00,\n",
      "         1.0188e+00,  1.0532e+00,  8.5124e-01,  9.3951e-01,  1.0234e+00,\n",
      "         9.5625e-01,  9.1405e-01,  9.7411e-01,  9.0941e-01,  9.2810e-01,\n",
      "         9.6760e-01,  1.0467e+00,  9.5590e-01,  1.0540e+00,  8.9165e-01,\n",
      "         9.4323e-01,  9.8542e-01,  9.1518e-01,  9.2224e-01,  8.8912e-01,\n",
      "         9.1553e-01,  1.0429e+00,  8.7050e-01,  9.3991e-01,  9.9075e-01,\n",
      "         1.0153e+00,  8.6923e-01,  8.6238e-01,  9.8481e-01,  1.0011e+00,\n",
      "         8.1820e-01,  1.0136e+00,  9.5089e-01,  9.1416e-01,  9.6352e-01,\n",
      "         9.1127e-01,  1.1243e+00,  9.2045e-01,  1.1010e+00,  9.3505e-01,\n",
      "         9.5784e-01,  1.0304e+00,  1.0024e+00,  1.0193e+00,  1.0027e+00,\n",
      "         9.4980e-01,  8.5995e-01,  9.5968e-01,  8.6697e-01,  9.9105e-01,\n",
      "         9.3728e-01,  9.5841e-01,  1.0495e+00,  1.0790e+00,  9.4038e-01,\n",
      "         1.0016e+00,  9.0738e-01,  9.0829e-01,  9.7873e-01,  1.0011e+00,\n",
      "         8.7539e-01, -2.6814e-02,  1.7090e-03,  6.5089e-03, -2.5157e-02,\n",
      "        -3.8511e-02,  1.7454e-02,  4.6543e-03, -4.5388e-02, -1.7894e-02,\n",
      "         6.4569e-03, -3.0575e-02,  2.0534e-02,  2.0942e-02,  5.1053e-04,\n",
      "        -1.2404e-02, -2.5091e-02, -3.5209e-03,  6.6831e-02, -2.5032e-02,\n",
      "        -1.0898e-02,  2.2232e-02,  2.1761e-02,  7.0685e-03,  4.2037e-03,\n",
      "        -4.1356e-02,  3.9474e-03,  2.8118e-03, -1.1670e-02, -3.8580e-02,\n",
      "        -9.1252e-03, -2.2567e-02,  2.5050e-02,  1.4644e-03,  3.5647e-02,\n",
      "        -6.4453e-02, -2.7878e-02,  5.2735e-03,  3.1311e-02,  6.6675e-03,\n",
      "        -2.2180e-02, -4.5190e-04, -3.8080e-02,  2.5343e-02,  2.4318e-02,\n",
      "         1.0065e-02, -2.8401e-02, -2.2435e-02, -3.0955e-03, -8.6043e-02,\n",
      "        -2.6397e-02, -2.7660e-02, -4.6037e-02, -2.9842e-02,  1.1239e-04,\n",
      "         2.0865e-02, -2.4601e-02, -2.1017e-02, -2.5588e-02,  2.2877e-02,\n",
      "        -3.6845e-02,  2.6317e-02, -1.4098e-03, -2.7017e-02,  3.1445e-02,\n",
      "         9.8702e-03, -6.2134e-03,  5.5325e-02, -3.6057e-03, -2.2163e-02,\n",
      "         2.0476e-02,  2.7060e-03,  1.5106e-02,  4.7512e-03,  1.3311e-02,\n",
      "        -2.1150e-02, -3.6651e-02,  3.8230e-02,  1.7490e-03, -7.6503e-02,\n",
      "         2.3328e-02,  2.9566e-03, -4.1404e-02,  1.5676e-02,  1.0020e-02,\n",
      "        -9.6596e-03,  1.2744e-02, -5.1453e-02, -5.3052e-02,  2.5795e-03,\n",
      "        -6.4977e-03,  3.2390e-02, -1.5457e-02, -1.6475e-02, -1.2376e-02,\n",
      "        -1.1771e-02,  2.1142e-04,  1.8905e-02, -1.5262e-04,  2.0617e-02,\n",
      "         2.7840e-03, -2.4044e-02, -6.8746e-03,  2.0943e-04, -7.0919e-02,\n",
      "         2.3181e-02, -6.2171e-02, -5.4543e-04, -2.1738e-02,  2.0512e-02,\n",
      "        -8.2106e-03, -3.0736e-02,  3.8836e-02,  5.8306e-02, -4.1853e-03,\n",
      "         7.8189e-03, -1.3104e-02,  1.9061e-02,  2.8891e-02, -4.4002e-02,\n",
      "         5.8864e-03, -4.5037e-02, -2.0881e-02, -1.0878e-02, -2.5542e-02,\n",
      "         1.9232e-02, -8.0950e-02,  1.3532e-02, -1.2474e-02, -1.5060e-01,\n",
      "        -2.0253e-02, -4.6761e-02, -2.2804e-02, -1.1165e-01, -8.7994e-02,\n",
      "        -7.2182e-02, -9.8677e-02, -1.1599e-01, -6.8070e-02, -1.3728e-03,\n",
      "        -3.5718e-02, -8.3189e-02, -7.7364e-02, -7.9933e-02, -6.7202e-02,\n",
      "        -4.0220e-02, -1.4792e-02, -3.6608e-02, -5.3133e-02, -1.2673e-01,\n",
      "        -4.6986e-02, -6.3220e-02, -4.1274e-02, -7.3179e-02, -7.9192e-02,\n",
      "        -1.5744e-01, -9.5800e-02, -5.7112e-02, -8.0274e-02, -8.4491e-02,\n",
      "        -9.3302e-02, -4.4109e-02, -1.1122e-01, -6.5557e-02, -3.0388e-02,\n",
      "        -5.1858e-02, -1.3953e-01, -7.1740e-02, -6.1417e-02, -1.3685e-01,\n",
      "        -8.5980e-02, -5.1758e-02, -7.8130e-02, -3.3013e-02, -1.2844e-01,\n",
      "        -6.0793e-02, -8.8730e-02, -1.0569e-01,  9.8135e-03, -6.0705e-02,\n",
      "        -8.3302e-03, -8.4765e-02, -7.0114e-02, -6.5947e-02, -9.0714e-02,\n",
      "        -2.9939e-02, -7.2307e-02, -1.0836e-01, -1.4012e-01, -3.2584e-02,\n",
      "        -5.4277e-02, -4.4188e-02, -1.2672e-01, -8.8080e-02, -5.8405e-02,\n",
      "        -1.6399e-01, -1.1573e-01, -8.0764e-02, -1.2052e-01, -5.9525e-02,\n",
      "        -5.4104e-02, -1.0982e-01, -4.0574e-02, -6.0802e-02, -4.3820e-02,\n",
      "        -8.0018e-02, -1.1223e-01, -6.1934e-02, -6.4585e-02, -1.2785e-01,\n",
      "        -4.3723e-02, -1.0090e-01, -2.0742e-02, -8.6903e-02, -7.4653e-02,\n",
      "        -1.4329e-01, -1.1244e-01, -1.0008e-01, -6.1255e-02, -6.3625e-02,\n",
      "        -1.1185e-01, -4.1158e-02, -7.7259e-03, -7.6541e-02, -5.9322e-02,\n",
      "        -8.0740e-02, -5.1919e-02, -5.9182e-02, -1.0963e-01, -1.1068e-01,\n",
      "        -1.0623e-01, -1.1827e-01, -7.4822e-02, -1.0487e-01, -8.5200e-02,\n",
      "        -9.6750e-02, -1.0440e-01, -9.3059e-02, -6.2881e-02, -1.4360e-01,\n",
      "        -8.8832e-02, -1.2943e-01, -4.5583e-02, -5.2863e-02, -8.0130e-02,\n",
      "        -8.4937e-02, -1.3122e-01, -1.0039e-01, -1.1992e-01, -6.3371e-02,\n",
      "        -1.4744e-01, -4.1795e-02, -1.1268e-01, -6.6164e-02, -2.5791e-02,\n",
      "        -7.1058e-02, -5.8823e-02], device='cuda:0', requires_grad=True))\n",
      "('encoder.rnn.rnns.0._module.bias_hh_l1_reverse', Parameter containing:\n",
      "tensor([-8.5194e-02,  5.0748e-02, -4.2345e-02, -8.9201e-03, -1.2594e-01,\n",
      "         1.7850e-02,  1.4268e-02, -8.2107e-02,  1.0559e-01,  3.1797e-02,\n",
      "        -3.8011e-02, -9.8749e-03, -6.3035e-02, -4.3352e-02, -6.0415e-02,\n",
      "        -1.7647e-02, -3.3878e-02,  8.7495e-02, -5.6274e-02, -2.0285e-02,\n",
      "        -7.3616e-02, -1.6452e-02, -6.3552e-02, -5.4365e-02,  3.4121e-02,\n",
      "        -5.7629e-02, -1.2424e-01, -4.4267e-02,  9.4388e-03,  2.8660e-02,\n",
      "        -1.1146e-01, -1.0919e-01, -5.5710e-02,  3.3862e-02,  2.7875e-03,\n",
      "        -5.7307e-02, -7.6660e-03, -3.0281e-02, -6.5681e-02, -3.3209e-02,\n",
      "        -8.8059e-02, -6.0699e-03, -3.0260e-02, -4.0956e-02, -1.3897e-02,\n",
      "        -7.7983e-02, -6.7759e-02, -3.6184e-02,  4.2259e-02,  2.0130e-02,\n",
      "        -9.0917e-03, -8.6644e-03, -2.6129e-02, -5.0649e-02, -5.4276e-02,\n",
      "        -1.6026e-02, -9.1224e-03, -4.8640e-02, -1.1548e-01, -7.2881e-02,\n",
      "        -8.4433e-03,  9.4591e-03, -7.3279e-02, -7.9007e-02,  3.5890e-02,\n",
      "        -4.1443e-02, -1.0185e-01, -7.0682e-03,  1.9676e-03, -5.3172e-02,\n",
      "        -1.1073e-01,  2.3811e-02, -4.5280e-02, -3.9587e-03, -6.8454e-02,\n",
      "        -7.2889e-02, -7.3230e-03, -4.8429e-02,  7.8387e-02, -3.4647e-02,\n",
      "        -6.0389e-02, -7.5968e-02, -9.8876e-02, -2.4399e-02, -7.7866e-02,\n",
      "        -8.3291e-02, -1.4140e-01, -1.0745e-01, -4.6692e-02, -4.5490e-03,\n",
      "        -5.1385e-02, -1.1350e-01, -1.2480e-02,  2.8452e-02, -6.1673e-02,\n",
      "        -2.4638e-02,  1.9051e-02, -1.6957e-02, -2.4066e-02, -4.9330e-02,\n",
      "        -5.8934e-02, -5.1905e-03, -1.2178e-01,  5.8719e-02, -1.1871e-01,\n",
      "        -5.5876e-02, -8.1458e-02, -2.0993e-02, -5.9440e-02, -1.6258e-01,\n",
      "        -7.4437e-02,  6.1088e-02, -4.4021e-02,  6.7462e-03, -5.1425e-02,\n",
      "        -5.1184e-03, -1.3289e-01, -5.5719e-02, -8.9919e-02,  5.7741e-03,\n",
      "         1.9624e-02, -3.4006e-02, -4.6006e-02, -9.0814e-02, -8.7417e-02,\n",
      "        -2.4229e-02, -5.3843e-02, -5.6102e-02,  1.0810e+00,  9.6548e-01,\n",
      "         1.0327e+00,  9.3848e-01,  8.7390e-01,  9.9234e-01,  9.3967e-01,\n",
      "         8.3122e-01,  1.0445e+00,  1.0282e+00,  9.5764e-01,  9.2140e-01,\n",
      "         1.0389e+00,  9.7235e-01,  9.6408e-01,  8.5322e-01,  8.2607e-01,\n",
      "         9.8306e-01,  9.2006e-01,  9.9426e-01,  9.4097e-01,  9.5562e-01,\n",
      "         9.4873e-01,  9.4476e-01,  1.0726e+00,  9.5022e-01,  9.3246e-01,\n",
      "         9.3657e-01,  8.8288e-01,  1.0615e+00,  9.1758e-01,  9.3764e-01,\n",
      "         9.5291e-01,  1.0102e+00,  9.6668e-01,  9.7533e-01,  1.0386e+00,\n",
      "         9.9499e-01,  9.6557e-01,  8.5403e-01,  9.1489e-01,  1.0159e+00,\n",
      "         8.4977e-01,  9.6358e-01,  9.2550e-01,  9.7680e-01,  9.3521e-01,\n",
      "         9.2931e-01,  8.9567e-01,  1.0772e+00,  1.0024e+00,  8.6580e-01,\n",
      "         9.3738e-01,  1.0129e+00,  9.5295e-01,  9.6479e-01,  9.1282e-01,\n",
      "         8.7494e-01,  9.1950e-01,  1.0174e+00,  7.9923e-01,  7.9274e-01,\n",
      "         8.7110e-01,  9.7718e-01,  1.0171e+00,  9.4445e-01,  1.0443e+00,\n",
      "         1.0188e+00,  1.0532e+00,  8.5124e-01,  9.3951e-01,  1.0234e+00,\n",
      "         9.5625e-01,  9.1405e-01,  9.7411e-01,  9.0941e-01,  9.2810e-01,\n",
      "         9.6760e-01,  1.0467e+00,  9.5590e-01,  1.0540e+00,  8.9165e-01,\n",
      "         9.4323e-01,  9.8542e-01,  9.1518e-01,  9.2224e-01,  8.8912e-01,\n",
      "         9.1553e-01,  1.0429e+00,  8.7050e-01,  9.3991e-01,  9.9075e-01,\n",
      "         1.0153e+00,  8.6923e-01,  8.6238e-01,  9.8481e-01,  1.0011e+00,\n",
      "         8.1820e-01,  1.0136e+00,  9.5089e-01,  9.1416e-01,  9.6352e-01,\n",
      "         9.1127e-01,  1.1243e+00,  9.2045e-01,  1.1010e+00,  9.3505e-01,\n",
      "         9.5784e-01,  1.0304e+00,  1.0024e+00,  1.0193e+00,  1.0027e+00,\n",
      "         9.4980e-01,  8.5995e-01,  9.5968e-01,  8.6697e-01,  9.9105e-01,\n",
      "         9.3728e-01,  9.5841e-01,  1.0495e+00,  1.0790e+00,  9.4038e-01,\n",
      "         1.0016e+00,  9.0738e-01,  9.0829e-01,  9.7873e-01,  1.0011e+00,\n",
      "         8.7539e-01, -2.6814e-02,  1.7090e-03,  6.5089e-03, -2.5157e-02,\n",
      "        -3.8511e-02,  1.7454e-02,  4.6543e-03, -4.5388e-02, -1.7894e-02,\n",
      "         6.4569e-03, -3.0575e-02,  2.0534e-02,  2.0942e-02,  5.1053e-04,\n",
      "        -1.2404e-02, -2.5091e-02, -3.5209e-03,  6.6831e-02, -2.5032e-02,\n",
      "        -1.0898e-02,  2.2232e-02,  2.1761e-02,  7.0685e-03,  4.2037e-03,\n",
      "        -4.1356e-02,  3.9474e-03,  2.8118e-03, -1.1670e-02, -3.8580e-02,\n",
      "        -9.1252e-03, -2.2567e-02,  2.5050e-02,  1.4644e-03,  3.5647e-02,\n",
      "        -6.4453e-02, -2.7878e-02,  5.2735e-03,  3.1311e-02,  6.6675e-03,\n",
      "        -2.2180e-02, -4.5190e-04, -3.8080e-02,  2.5343e-02,  2.4318e-02,\n",
      "         1.0065e-02, -2.8401e-02, -2.2435e-02, -3.0955e-03, -8.6043e-02,\n",
      "        -2.6397e-02, -2.7660e-02, -4.6037e-02, -2.9842e-02,  1.1239e-04,\n",
      "         2.0865e-02, -2.4601e-02, -2.1017e-02, -2.5588e-02,  2.2877e-02,\n",
      "        -3.6845e-02,  2.6317e-02, -1.4098e-03, -2.7017e-02,  3.1445e-02,\n",
      "         9.8702e-03, -6.2134e-03,  5.5325e-02, -3.6057e-03, -2.2163e-02,\n",
      "         2.0476e-02,  2.7060e-03,  1.5106e-02,  4.7512e-03,  1.3311e-02,\n",
      "        -2.1150e-02, -3.6651e-02,  3.8230e-02,  1.7490e-03, -7.6503e-02,\n",
      "         2.3328e-02,  2.9566e-03, -4.1404e-02,  1.5676e-02,  1.0020e-02,\n",
      "        -9.6596e-03,  1.2744e-02, -5.1453e-02, -5.3052e-02,  2.5795e-03,\n",
      "        -6.4977e-03,  3.2390e-02, -1.5457e-02, -1.6475e-02, -1.2376e-02,\n",
      "        -1.1771e-02,  2.1142e-04,  1.8905e-02, -1.5262e-04,  2.0617e-02,\n",
      "         2.7840e-03, -2.4044e-02, -6.8746e-03,  2.0943e-04, -7.0919e-02,\n",
      "         2.3181e-02, -6.2171e-02, -5.4543e-04, -2.1738e-02,  2.0512e-02,\n",
      "        -8.2106e-03, -3.0736e-02,  3.8836e-02,  5.8306e-02, -4.1853e-03,\n",
      "         7.8189e-03, -1.3104e-02,  1.9061e-02,  2.8891e-02, -4.4002e-02,\n",
      "         5.8864e-03, -4.5037e-02, -2.0881e-02, -1.0878e-02, -2.5542e-02,\n",
      "         1.9232e-02, -8.0950e-02,  1.3532e-02, -1.2474e-02, -1.5060e-01,\n",
      "        -2.0253e-02, -4.6761e-02, -2.2804e-02, -1.1165e-01, -8.7994e-02,\n",
      "        -7.2182e-02, -9.8677e-02, -1.1599e-01, -6.8070e-02, -1.3728e-03,\n",
      "        -3.5718e-02, -8.3189e-02, -7.7364e-02, -7.9933e-02, -6.7202e-02,\n",
      "        -4.0220e-02, -1.4792e-02, -3.6608e-02, -5.3133e-02, -1.2673e-01,\n",
      "        -4.6986e-02, -6.3220e-02, -4.1274e-02, -7.3179e-02, -7.9192e-02,\n",
      "        -1.5744e-01, -9.5800e-02, -5.7112e-02, -8.0274e-02, -8.4491e-02,\n",
      "        -9.3302e-02, -4.4109e-02, -1.1122e-01, -6.5557e-02, -3.0388e-02,\n",
      "        -5.1858e-02, -1.3953e-01, -7.1740e-02, -6.1417e-02, -1.3685e-01,\n",
      "        -8.5980e-02, -5.1758e-02, -7.8130e-02, -3.3013e-02, -1.2844e-01,\n",
      "        -6.0793e-02, -8.8730e-02, -1.0569e-01,  9.8135e-03, -6.0705e-02,\n",
      "        -8.3302e-03, -8.4765e-02, -7.0114e-02, -6.5947e-02, -9.0714e-02,\n",
      "        -2.9939e-02, -7.2307e-02, -1.0836e-01, -1.4012e-01, -3.2584e-02,\n",
      "        -5.4277e-02, -4.4188e-02, -1.2672e-01, -8.8080e-02, -5.8405e-02,\n",
      "        -1.6399e-01, -1.1573e-01, -8.0764e-02, -1.2052e-01, -5.9525e-02,\n",
      "        -5.4104e-02, -1.0982e-01, -4.0574e-02, -6.0802e-02, -4.3820e-02,\n",
      "        -8.0018e-02, -1.1223e-01, -6.1934e-02, -6.4585e-02, -1.2785e-01,\n",
      "        -4.3723e-02, -1.0090e-01, -2.0742e-02, -8.6903e-02, -7.4653e-02,\n",
      "        -1.4329e-01, -1.1244e-01, -1.0008e-01, -6.1255e-02, -6.3625e-02,\n",
      "        -1.1185e-01, -4.1158e-02, -7.7259e-03, -7.6541e-02, -5.9322e-02,\n",
      "        -8.0740e-02, -5.1919e-02, -5.9182e-02, -1.0963e-01, -1.1068e-01,\n",
      "        -1.0623e-01, -1.1827e-01, -7.4822e-02, -1.0487e-01, -8.5200e-02,\n",
      "        -9.6750e-02, -1.0440e-01, -9.3059e-02, -6.2881e-02, -1.4360e-01,\n",
      "        -8.8832e-02, -1.2943e-01, -4.5583e-02, -5.2863e-02, -8.0130e-02,\n",
      "        -8.4937e-02, -1.3122e-01, -1.0039e-01, -1.1992e-01, -6.3371e-02,\n",
      "        -1.4744e-01, -4.1795e-02, -1.1268e-01, -6.6164e-02, -2.5791e-02,\n",
      "        -7.1058e-02, -5.8823e-02], device='cuda:0', requires_grad=True))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('encoder.pool.attn.vw', Parameter containing:\n",
      "tensor([[-1.5705e-03],\n",
      "        [-9.0919e-02],\n",
      "        [-2.4137e-03],\n",
      "        [-2.3563e-02],\n",
      "        [-1.0563e-02],\n",
      "        [ 5.3419e-02],\n",
      "        [ 2.4791e-02],\n",
      "        [-4.9531e-02],\n",
      "        [-1.0970e-01],\n",
      "        [ 1.4758e-02],\n",
      "        [-8.1911e-02],\n",
      "        [ 1.1493e-02],\n",
      "        [ 5.1062e-02],\n",
      "        [-8.7327e-02],\n",
      "        [ 1.9454e-03],\n",
      "        [-4.3543e-03],\n",
      "        [ 2.6138e-02],\n",
      "        [ 4.8943e-02],\n",
      "        [-5.9947e-02],\n",
      "        [ 2.1135e-02],\n",
      "        [-1.2889e-02],\n",
      "        [-7.3169e-02],\n",
      "        [-4.9371e-02],\n",
      "        [-1.7647e-02],\n",
      "        [ 2.3251e-02],\n",
      "        [ 6.7248e-03],\n",
      "        [ 1.1074e-01],\n",
      "        [-2.1450e-02],\n",
      "        [ 4.5926e-02],\n",
      "        [ 1.0774e-01],\n",
      "        [ 1.3423e-02],\n",
      "        [ 6.0045e-02],\n",
      "        [ 3.6146e-03],\n",
      "        [ 5.9694e-03],\n",
      "        [-3.6592e-02],\n",
      "        [ 7.9801e-02],\n",
      "        [ 4.9001e-02],\n",
      "        [ 2.3912e-02],\n",
      "        [ 8.1200e-02],\n",
      "        [-8.8567e-03],\n",
      "        [-9.9723e-03],\n",
      "        [-4.1508e-02],\n",
      "        [-8.2715e-02],\n",
      "        [ 5.1060e-02],\n",
      "        [ 3.1047e-03],\n",
      "        [ 8.0434e-03],\n",
      "        [ 8.3813e-02],\n",
      "        [ 6.8946e-03],\n",
      "        [-2.3419e-02],\n",
      "        [-1.5775e-02],\n",
      "        [-7.4450e-02],\n",
      "        [-3.2249e-02],\n",
      "        [-3.5411e-02],\n",
      "        [-7.7657e-02],\n",
      "        [ 3.4945e-02],\n",
      "        [-4.2971e-03],\n",
      "        [-2.0334e-02],\n",
      "        [-8.0403e-04],\n",
      "        [-5.2578e-02],\n",
      "        [-3.9754e-03],\n",
      "        [-3.3765e-02],\n",
      "        [-1.9515e-02],\n",
      "        [ 1.0628e-01],\n",
      "        [ 5.4897e-02],\n",
      "        [-9.1650e-02],\n",
      "        [ 4.6275e-03],\n",
      "        [-8.1457e-03],\n",
      "        [-1.0255e-01],\n",
      "        [-7.8508e-05],\n",
      "        [-1.3435e-02],\n",
      "        [-1.3421e-02],\n",
      "        [ 7.3506e-02],\n",
      "        [-5.0834e-03],\n",
      "        [-3.0206e-02],\n",
      "        [ 2.0189e-02],\n",
      "        [-5.9560e-04],\n",
      "        [ 2.3120e-02],\n",
      "        [-3.4793e-03],\n",
      "        [-9.0367e-03],\n",
      "        [-5.3428e-02],\n",
      "        [ 7.3719e-03],\n",
      "        [-3.7856e-02],\n",
      "        [-5.5119e-02],\n",
      "        [-2.5938e-02],\n",
      "        [ 5.9479e-02],\n",
      "        [ 7.4963e-02],\n",
      "        [ 7.5426e-02],\n",
      "        [-5.5122e-03],\n",
      "        [ 2.2610e-04],\n",
      "        [ 1.9852e-02],\n",
      "        [ 6.2503e-03],\n",
      "        [ 3.1011e-02],\n",
      "        [-8.3800e-03],\n",
      "        [-1.3543e-02],\n",
      "        [ 8.1630e-02],\n",
      "        [-4.7268e-03],\n",
      "        [ 2.4472e-03],\n",
      "        [ 5.3470e-02],\n",
      "        [ 1.0663e-01],\n",
      "        [-1.1953e-02],\n",
      "        [-8.5393e-03],\n",
      "        [ 3.2967e-02],\n",
      "        [ 2.1523e-03],\n",
      "        [ 3.0239e-02],\n",
      "        [ 4.5610e-02],\n",
      "        [-4.5336e-02],\n",
      "        [-3.1265e-02],\n",
      "        [-5.4020e-02],\n",
      "        [-4.2882e-02],\n",
      "        [-1.8740e-02],\n",
      "        [ 2.3237e-02],\n",
      "        [-1.0851e-03],\n",
      "        [ 6.9504e-03],\n",
      "        [ 7.2826e-02],\n",
      "        [ 4.4532e-03],\n",
      "        [-9.5517e-02],\n",
      "        [ 4.2908e-02],\n",
      "        [-1.7300e-02],\n",
      "        [ 3.9347e-02],\n",
      "        [ 2.9410e-02],\n",
      "        [-2.0798e-03],\n",
      "        [-7.8544e-02],\n",
      "        [-5.3376e-02],\n",
      "        [ 2.6051e-02],\n",
      "        [-2.5147e-02],\n",
      "        [ 1.3331e-02],\n",
      "        [-2.4707e-03],\n",
      "        [ 5.3773e-02],\n",
      "        [-1.0645e-03],\n",
      "        [-3.9864e-02],\n",
      "        [ 1.0073e-02],\n",
      "        [-6.0773e-02],\n",
      "        [-1.1341e-02],\n",
      "        [ 6.4707e-04],\n",
      "        [ 4.9156e-02],\n",
      "        [-9.0119e-03],\n",
      "        [-8.7734e-02],\n",
      "        [-5.7578e-02],\n",
      "        [ 2.4080e-03],\n",
      "        [-6.4463e-02],\n",
      "        [-3.2277e-03],\n",
      "        [-4.8739e-02],\n",
      "        [ 2.4279e-02],\n",
      "        [ 7.1940e-02],\n",
      "        [ 4.3319e-03],\n",
      "        [-1.8684e-02],\n",
      "        [ 4.8666e-02],\n",
      "        [ 5.4535e-03],\n",
      "        [ 1.2664e-02],\n",
      "        [ 2.5232e-03],\n",
      "        [ 1.4753e-02],\n",
      "        [ 4.7635e-02],\n",
      "        [-1.0625e-04],\n",
      "        [-2.8275e-02],\n",
      "        [-7.1571e-05],\n",
      "        [ 6.9090e-03],\n",
      "        [-7.3994e-03],\n",
      "        [-7.1727e-02],\n",
      "        [ 4.6259e-02],\n",
      "        [ 9.7642e-02],\n",
      "        [-4.1344e-02],\n",
      "        [-4.1254e-03],\n",
      "        [-2.9529e-02],\n",
      "        [ 1.8627e-02],\n",
      "        [-4.0920e-02],\n",
      "        [-7.3217e-02],\n",
      "        [-6.9974e-03],\n",
      "        [ 2.7702e-02],\n",
      "        [-2.0859e-02],\n",
      "        [ 5.8143e-02],\n",
      "        [-5.3258e-03],\n",
      "        [ 1.2398e-02],\n",
      "        [ 1.9714e-02],\n",
      "        [-7.1779e-02],\n",
      "        [ 1.0851e-02],\n",
      "        [ 4.3566e-02],\n",
      "        [ 8.2204e-02],\n",
      "        [-8.4011e-03],\n",
      "        [-1.5855e-02],\n",
      "        [-2.9151e-02],\n",
      "        [ 4.0737e-02],\n",
      "        [ 1.1613e-01],\n",
      "        [-4.2457e-03],\n",
      "        [ 6.3979e-03],\n",
      "        [ 7.4825e-02],\n",
      "        [ 5.3531e-02],\n",
      "        [-1.7370e-02],\n",
      "        [-1.0323e-03],\n",
      "        [ 4.1919e-02],\n",
      "        [-3.6270e-02],\n",
      "        [-8.7649e-03],\n",
      "        [-7.4129e-03],\n",
      "        [ 3.9756e-02],\n",
      "        [-6.0588e-03],\n",
      "        [-3.2946e-02],\n",
      "        [-2.0152e-02],\n",
      "        [ 6.6702e-02],\n",
      "        [-1.0282e-01],\n",
      "        [ 6.1759e-05],\n",
      "        [-6.3041e-02],\n",
      "        [-2.5532e-03],\n",
      "        [-3.5343e-02],\n",
      "        [ 2.2197e-02],\n",
      "        [ 3.5743e-02],\n",
      "        [-2.6514e-02],\n",
      "        [ 1.4490e-02],\n",
      "        [-4.7528e-03],\n",
      "        [ 5.7864e-02],\n",
      "        [ 1.1614e-02],\n",
      "        [ 1.1730e-02],\n",
      "        [ 9.0685e-03],\n",
      "        [ 2.5616e-03],\n",
      "        [ 7.8467e-03],\n",
      "        [-4.8304e-02],\n",
      "        [-1.1198e-01],\n",
      "        [-5.3921e-02],\n",
      "        [ 1.2371e-03],\n",
      "        [ 1.9606e-02],\n",
      "        [ 8.1570e-02],\n",
      "        [-6.5475e-02],\n",
      "        [-2.6078e-02],\n",
      "        [ 1.9729e-03],\n",
      "        [-5.0471e-02],\n",
      "        [-3.9045e-02],\n",
      "        [ 3.5198e-02],\n",
      "        [ 6.5807e-02],\n",
      "        [ 1.9612e-02],\n",
      "        [ 2.1793e-02],\n",
      "        [-4.6228e-02],\n",
      "        [-2.2731e-03],\n",
      "        [ 9.1200e-03],\n",
      "        [-6.2390e-03],\n",
      "        [ 2.3780e-03],\n",
      "        [ 4.1900e-02],\n",
      "        [-5.1261e-02],\n",
      "        [ 3.4556e-03],\n",
      "        [-1.1065e-01],\n",
      "        [ 1.7686e-02],\n",
      "        [ 1.3180e-02],\n",
      "        [ 7.0910e-02],\n",
      "        [-1.0834e-02],\n",
      "        [-4.9479e-02],\n",
      "        [-3.9271e-02],\n",
      "        [ 4.4710e-03],\n",
      "        [ 4.7074e-02],\n",
      "        [ 1.4896e-02],\n",
      "        [-3.1838e-02],\n",
      "        [ 3.4482e-02],\n",
      "        [-2.7056e-03],\n",
      "        [-3.5601e-02],\n",
      "        [ 1.8955e-02],\n",
      "        [-4.1952e-03],\n",
      "        [-6.1788e-02],\n",
      "        [-1.3239e-02],\n",
      "        [ 1.0524e-02],\n",
      "        [ 4.5240e-02],\n",
      "        [-1.4051e-02],\n",
      "        [-5.1884e-02],\n",
      "        [ 6.6361e-03],\n",
      "        [ 7.5130e-02],\n",
      "        [-7.9705e-03],\n",
      "        [-1.3945e-02],\n",
      "        [ 6.7192e-03],\n",
      "        [-5.7716e-03],\n",
      "        [-6.4622e-02],\n",
      "        [-4.2691e-02],\n",
      "        [-2.6728e-02],\n",
      "        [ 1.0308e-02],\n",
      "        [ 5.4436e-03],\n",
      "        [ 5.9330e-03],\n",
      "        [-1.5007e-03],\n",
      "        [-6.6599e-02],\n",
      "        [-2.5065e-03],\n",
      "        [-1.6892e-02],\n",
      "        [ 9.2141e-02],\n",
      "        [ 7.3309e-02],\n",
      "        [-7.3442e-02],\n",
      "        [-3.5307e-02],\n",
      "        [-5.4851e-02],\n",
      "        [ 2.7535e-02],\n",
      "        [ 3.5666e-02],\n",
      "        [ 6.2886e-04],\n",
      "        [ 4.1063e-03],\n",
      "        [ 9.2040e-02],\n",
      "        [ 4.2868e-03],\n",
      "        [-3.7834e-03],\n",
      "        [ 2.3875e-02],\n",
      "        [ 8.9186e-02],\n",
      "        [ 3.9832e-02],\n",
      "        [ 2.0554e-02],\n",
      "        [ 1.2266e-03],\n",
      "        [ 3.3709e-02],\n",
      "        [-2.6951e-03],\n",
      "        [-2.6941e-02],\n",
      "        [ 3.7431e-02],\n",
      "        [ 2.1346e-03],\n",
      "        [ 4.8993e-02],\n",
      "        [ 8.3865e-02],\n",
      "        [ 1.1196e-03],\n",
      "        [ 6.7090e-02],\n",
      "        [-1.1406e-02],\n",
      "        [ 5.1362e-02],\n",
      "        [ 6.1089e-03],\n",
      "        [ 7.5805e-02],\n",
      "        [ 4.8137e-02],\n",
      "        [ 1.3595e-02],\n",
      "        [ 1.0997e-02],\n",
      "        [ 1.1318e-02],\n",
      "        [-2.6586e-02],\n",
      "        [-1.3512e-02],\n",
      "        [-7.5622e-02],\n",
      "        [-4.4594e-03],\n",
      "        [-6.8611e-02],\n",
      "        [-7.7632e-03],\n",
      "        [ 6.6047e-02],\n",
      "        [-8.7434e-02],\n",
      "        [-2.0994e-03],\n",
      "        [-3.8563e-02],\n",
      "        [ 7.0600e-02],\n",
      "        [-5.4970e-02],\n",
      "        [ 2.6614e-03],\n",
      "        [ 6.7388e-02],\n",
      "        [ 4.1596e-02],\n",
      "        [-4.1106e-02],\n",
      "        [-2.4480e-02],\n",
      "        [-6.2980e-02],\n",
      "        [-9.7967e-03],\n",
      "        [ 2.9505e-02],\n",
      "        [-1.0353e-02],\n",
      "        [ 6.3885e-02],\n",
      "        [ 3.6200e-02],\n",
      "        [-7.7786e-02],\n",
      "        [ 1.1761e-01],\n",
      "        [-8.5038e-02],\n",
      "        [-1.1639e-02],\n",
      "        [-2.5409e-02],\n",
      "        [ 2.5944e-02],\n",
      "        [ 2.4685e-02],\n",
      "        [-6.0841e-02],\n",
      "        [-1.7019e-02],\n",
      "        [-4.2133e-03],\n",
      "        [-4.7207e-03],\n",
      "        [ 1.0430e-02],\n",
      "        [-6.3127e-02],\n",
      "        [ 5.0129e-02],\n",
      "        [ 2.4546e-02],\n",
      "        [ 3.9044e-05],\n",
      "        [-5.9255e-02],\n",
      "        [-5.6208e-03],\n",
      "        [-2.6148e-02],\n",
      "        [ 1.2024e-02],\n",
      "        [-1.9228e-02],\n",
      "        [ 1.0641e-03],\n",
      "        [-4.3640e-06],\n",
      "        [-5.0581e-03],\n",
      "        [-7.0865e-02],\n",
      "        [-3.9870e-02],\n",
      "        [ 5.4035e-02],\n",
      "        [-4.5150e-02],\n",
      "        [ 3.9835e-02],\n",
      "        [-2.1663e-02],\n",
      "        [ 8.4757e-03],\n",
      "        [ 7.5515e-03],\n",
      "        [ 6.9311e-02],\n",
      "        [-2.7412e-02],\n",
      "        [-1.6288e-03],\n",
      "        [ 3.6415e-02],\n",
      "        [ 2.5818e-03],\n",
      "        [ 9.3959e-02],\n",
      "        [ 2.6129e-02],\n",
      "        [ 1.6294e-02],\n",
      "        [-3.1673e-02],\n",
      "        [-1.9156e-02],\n",
      "        [-1.1564e-01],\n",
      "        [-4.5526e-02],\n",
      "        [ 1.5175e-02],\n",
      "        [-5.2668e-02],\n",
      "        [-1.8849e-02],\n",
      "        [-7.2639e-02],\n",
      "        [-6.1222e-02],\n",
      "        [-9.7055e-02],\n",
      "        [ 3.7399e-02],\n",
      "        [ 1.9239e-02],\n",
      "        [ 1.9953e-02],\n",
      "        [ 2.7475e-03],\n",
      "        [ 3.9633e-03],\n",
      "        [ 5.8617e-02],\n",
      "        [ 6.5266e-02],\n",
      "        [-2.0782e-02],\n",
      "        [ 1.4296e-02],\n",
      "        [-1.1623e-03],\n",
      "        [ 6.0014e-02],\n",
      "        [-2.3015e-03],\n",
      "        [ 4.6031e-02],\n",
      "        [-6.9701e-03],\n",
      "        [-1.6834e-02],\n",
      "        [-3.0323e-02],\n",
      "        [ 5.4027e-02],\n",
      "        [-3.0678e-02],\n",
      "        [ 5.2174e-02],\n",
      "        [ 7.2016e-02],\n",
      "        [-7.5176e-02],\n",
      "        [-9.3987e-02],\n",
      "        [-4.6656e-02],\n",
      "        [-6.7856e-03],\n",
      "        [-4.7964e-02],\n",
      "        [ 1.2376e-01],\n",
      "        [-6.0612e-02],\n",
      "        [-1.0667e-01],\n",
      "        [ 5.4819e-02],\n",
      "        [-3.6490e-03],\n",
      "        [ 3.7973e-02],\n",
      "        [-6.8842e-02],\n",
      "        [-4.0478e-03],\n",
      "        [-1.7034e-03],\n",
      "        [-2.3094e-02],\n",
      "        [ 2.0723e-02],\n",
      "        [ 1.8263e-02],\n",
      "        [-1.9698e-02],\n",
      "        [ 2.5693e-03],\n",
      "        [ 9.1228e-02],\n",
      "        [ 5.2967e-04],\n",
      "        [ 1.4124e-03],\n",
      "        [-9.6455e-03],\n",
      "        [ 5.6157e-03],\n",
      "        [-1.4414e-02],\n",
      "        [ 1.5957e-02],\n",
      "        [-1.5134e-02],\n",
      "        [-4.2906e-02],\n",
      "        [ 2.0656e-02],\n",
      "        [ 3.7154e-02],\n",
      "        [-5.9943e-02],\n",
      "        [ 6.7881e-02],\n",
      "        [ 5.8544e-02],\n",
      "        [ 1.2243e-03],\n",
      "        [ 6.4097e-02],\n",
      "        [ 7.4770e-02],\n",
      "        [ 5.9471e-02],\n",
      "        [ 3.1628e-03],\n",
      "        [-3.0363e-02],\n",
      "        [-1.6704e-02],\n",
      "        [ 2.4151e-02],\n",
      "        [ 8.4286e-02],\n",
      "        [-2.6420e-02],\n",
      "        [ 6.0440e-02],\n",
      "        [-2.4890e-03],\n",
      "        [-3.0967e-02],\n",
      "        [-2.9467e-03],\n",
      "        [ 1.0179e-01],\n",
      "        [ 1.0863e-01],\n",
      "        [-1.2211e-01],\n",
      "        [ 7.2876e-03],\n",
      "        [ 9.9244e-02],\n",
      "        [ 8.3743e-02],\n",
      "        [ 4.2702e-02],\n",
      "        [-6.2092e-02],\n",
      "        [ 9.2112e-02],\n",
      "        [ 5.9908e-04],\n",
      "        [-4.0114e-02],\n",
      "        [ 4.9044e-02],\n",
      "        [ 8.2427e-02],\n",
      "        [-1.1174e-03],\n",
      "        [-5.8259e-02],\n",
      "        [-4.8418e-04],\n",
      "        [ 1.6732e-02],\n",
      "        [-3.8704e-02],\n",
      "        [ 4.0686e-02],\n",
      "        [ 1.1367e-01],\n",
      "        [-1.2050e-02],\n",
      "        [ 3.6787e-02],\n",
      "        [-5.9485e-02],\n",
      "        [ 2.3528e-02],\n",
      "        [ 9.4229e-02],\n",
      "        [ 1.3217e-02],\n",
      "        [ 2.8966e-02],\n",
      "        [-6.3138e-04],\n",
      "        [-2.6258e-02],\n",
      "        [-1.1089e-02],\n",
      "        [-9.5741e-03],\n",
      "        [ 1.9428e-02],\n",
      "        [-1.2021e-02],\n",
      "        [-8.1460e-02],\n",
      "        [-1.6946e-03],\n",
      "        [-3.0884e-02],\n",
      "        [ 2.1495e-02],\n",
      "        [-5.1080e-02],\n",
      "        [-6.6356e-02],\n",
      "        [ 2.3613e-02],\n",
      "        [-5.9974e-02],\n",
      "        [ 6.4961e-02],\n",
      "        [ 1.0335e-02],\n",
      "        [ 3.5354e-03],\n",
      "        [-1.7620e-02],\n",
      "        [ 3.8452e-02],\n",
      "        [ 2.4597e-02],\n",
      "        [ 4.9889e-02],\n",
      "        [-7.5806e-02],\n",
      "        [ 4.4237e-02],\n",
      "        [-3.6348e-02],\n",
      "        [ 1.0135e-01],\n",
      "        [ 2.6566e-02],\n",
      "        [-2.7502e-03],\n",
      "        [ 7.5711e-02],\n",
      "        [ 6.3477e-02],\n",
      "        [-4.1070e-04],\n",
      "        [-3.6349e-02],\n",
      "        [ 8.4681e-04],\n",
      "        [-4.5653e-02],\n",
      "        [-9.9129e-02],\n",
      "        [-6.4424e-03],\n",
      "        [ 5.0825e-03],\n",
      "        [-7.4745e-02]], device='cuda:0', requires_grad=True))\n",
      "('encoder.pool.attn.b', Parameter containing:\n",
      "tensor([0.1053], device='cuda:0', requires_grad=True))\n",
      "('encoder.pool.attn.l1.weight', Parameter containing:\n",
      "tensor([[ 0.0734, -0.0904,  0.0297,  ...,  0.0021,  0.0681,  0.0426],\n",
      "        [-0.0709, -0.0280,  0.0101,  ...,  0.0822,  0.0211, -0.0157],\n",
      "        [-0.0303, -0.0661, -0.0590,  ..., -0.0443, -0.0231, -0.0181],\n",
      "        ...,\n",
      "        [-0.1028, -0.0474,  0.0658,  ...,  0.0049,  0.0832,  0.0199],\n",
      "        [ 0.0028,  0.0319,  0.0665,  ..., -0.0901,  0.0350, -0.0465],\n",
      "        [ 0.0087, -0.0520, -0.0502,  ...,  0.1010, -0.0192, -0.0610]],\n",
      "       device='cuda:0', requires_grad=True))\n",
      "('encoder.pool.attn.l1.bias', Parameter containing:\n",
      "tensor([ 0.0255, -0.1278,  0.1133, -0.1489, -0.0597,  0.0195,  0.0769, -0.0926,\n",
      "        -0.1482,  0.0900, -0.1620,  0.0905,  0.0722, -0.0525,  0.0450, -0.0262,\n",
      "         0.0656,  0.0823, -0.0420,  0.0673, -0.0446, -0.1703, -0.0555, -0.0562,\n",
      "         0.1251,  0.1065,  0.0531, -0.1128,  0.0869,  0.0933, -0.0880,  0.0448,\n",
      "         0.0716,  0.0961, -0.1012,  0.1122,  0.0714,  0.1619,  0.0644,  0.1113,\n",
      "        -0.1614, -0.1117, -0.0625,  0.1008,  0.1220,  0.0986,  0.0688,  0.0812,\n",
      "        -0.0789, -0.0743, -0.0488, -0.0380, -0.0480, -0.0782,  0.0251,  0.1090,\n",
      "        -0.1185,  0.0244, -0.0518, -0.0806, -0.0866, -0.1219,  0.1035,  0.1216,\n",
      "        -0.0908,  0.1073, -0.0319, -0.0581,  0.0891, -0.1043,  0.0115,  0.0913,\n",
      "         0.1149, -0.1018,  0.0915, -0.0971,  0.1061,  0.0604, -0.0568, -0.1159,\n",
      "         0.1235, -0.0410, -0.1336, -0.0766,  0.0386,  0.0522,  0.0928,  0.0743,\n",
      "        -0.0284,  0.1287, -0.0393,  0.0632, -0.0794, -0.0528,  0.0753, -0.0678,\n",
      "        -0.1004,  0.1543,  0.1133,  0.0608, -0.0547,  0.1029, -0.0435,  0.1676,\n",
      "         0.0488, -0.0610, -0.1168, -0.0452, -0.1576, -0.0540,  0.0716, -0.0587,\n",
      "         0.0876,  0.0716,  0.0919, -0.1388,  0.0611, -0.0981,  0.0548,  0.0932,\n",
      "         0.0726, -0.0679, -0.0743,  0.1078, -0.0651,  0.0801,  0.1544,  0.0808,\n",
      "        -0.0280, -0.0870,  0.1379, -0.0600, -0.0700, -0.0708,  0.0984, -0.0401,\n",
      "        -0.1642, -0.1130,  0.1017, -0.1181,  0.1654, -0.0952,  0.0844,  0.1647,\n",
      "        -0.1070, -0.0717,  0.1194,  0.0881, -0.0356,  0.0497,  0.0892,  0.1055,\n",
      "         0.0891, -0.0917, -0.0840, -0.0801,  0.1094, -0.1538,  0.0518,  0.1088,\n",
      "        -0.1124,  0.0816, -0.0470,  0.0877, -0.0792, -0.1426,  0.0376,  0.1017,\n",
      "        -0.0689,  0.0977, -0.0871,  0.0945,  0.0288, -0.1345,  0.1300,  0.0629,\n",
      "         0.1008,  0.0590, -0.0469, -0.0761,  0.0379,  0.0768,  0.0204,  0.0747,\n",
      "         0.0287,  0.1269, -0.1547,  0.0533,  0.0580, -0.1316, -0.0113, -0.0700,\n",
      "         0.1300, -0.1204, -0.0855, -0.0793,  0.1188, -0.0402,  0.0461, -0.0940,\n",
      "         0.0304, -0.1961,  0.0919,  0.1164, -0.0661, -0.0854,  0.0263,  0.1064,\n",
      "        -0.1263, -0.1979, -0.0144,  0.2219, -0.0033, -0.1530, -0.1077, -0.0610,\n",
      "         0.0717,  0.0956,  0.0894, -0.0553, -0.0560,  0.0504, -0.0940, -0.1289,\n",
      "         0.0842,  0.1448,  0.0525,  0.0828, -0.0930, -0.1270,  0.1098, -0.0620,\n",
      "        -0.0624,  0.0889, -0.0728,  0.0770, -0.1416, -0.0987, -0.0296,  0.0921,\n",
      "        -0.0645, -0.0758, -0.0927, -0.1124,  0.1408,  0.0936, -0.0645,  0.1051,\n",
      "        -0.0813, -0.0454,  0.0618,  0.0078, -0.0867, -0.0518,  0.1155,  0.0830,\n",
      "        -0.1025, -0.0766,  0.0781,  0.1123,  0.0951, -0.0474, -0.2650,  0.0508,\n",
      "        -0.1146, -0.0684, -0.0882,  0.0609,  0.0288, -0.0045,  0.0551, -0.0680,\n",
      "         0.0869, -0.0872,  0.1138,  0.0953, -0.1044, -0.0746, -0.1075,  0.1049,\n",
      "         0.1262, -0.0653,  0.0898,  0.0857,  0.0514, -0.1285,  0.0742,  0.0652,\n",
      "         0.0524,  0.0411,  0.0124,  0.0658,  0.0495, -0.0761,  0.1137, -0.0655,\n",
      "         0.0527,  0.0853,  0.0343,  0.0819, -0.0643,  0.0612,  0.0735,  0.1039,\n",
      "         0.0403,  0.0757,  0.0416,  0.0776, -0.0841, -0.0537, -0.0540,  0.1002,\n",
      "        -0.0971, -0.0776,  0.1110, -0.1297,  0.0442, -0.1559,  0.0834, -0.1765,\n",
      "         0.0282,  0.0723,  0.0834, -0.0641, -0.1743, -0.0963,  0.0991,  0.0782,\n",
      "        -0.0786,  0.1912,  0.0554, -0.0998,  0.1199, -0.0762,  0.0640, -0.1098,\n",
      "         0.0374,  0.1148, -0.1773,  0.1136, -0.0387, -0.0812,  0.0014, -0.1108,\n",
      "         0.1295,  0.1126, -0.0043, -0.1682,  0.1497, -0.0723,  0.0875, -0.1214,\n",
      "        -0.0156, -0.1308, -0.0305, -0.0817, -0.1276,  0.0648, -0.0690,  0.0985,\n",
      "        -0.0735,  0.0834,  0.0804,  0.0159, -0.1185,  0.0527,  0.0805, -0.0197,\n",
      "         0.0401,  0.0455,  0.1428, -0.0384, -0.0594, -0.0672, -0.0938,  0.1092,\n",
      "        -0.0784, -0.0673, -0.1069, -0.1551, -0.0695,  0.0598,  0.0600,  0.0921,\n",
      "         0.1017,  0.0853,  0.1333,  0.1357, -0.0644,  0.0242, -0.0382,  0.1047,\n",
      "        -0.0355,  0.0945, -0.0398, -0.0919, -0.1468,  0.1518, -0.0570,  0.0362,\n",
      "         0.1177, -0.1082, -0.0749, -0.0421, -0.0878, -0.0355,  0.0692, -0.0817,\n",
      "        -0.0573,  0.0510, -0.0656,  0.1052, -0.0303,  0.0762, -0.0785, -0.0776,\n",
      "         0.0363,  0.0022, -0.0560, -0.1478,  0.1155, -0.0011,  0.1450,  0.0178,\n",
      "         0.0826, -0.0440,  0.0805, -0.0810, -0.0969,  0.0943,  0.1340, -0.0524,\n",
      "         0.0740,  0.1712, -0.0521,  0.0223,  0.1069,  0.0929, -0.0869, -0.1046,\n",
      "        -0.2762,  0.0712,  0.0843, -0.0718,  0.1520,  0.0274, -0.0870, -0.1038,\n",
      "         0.0885,  0.0265, -0.0573,  0.0280,  0.0934,  0.0635,  0.1252, -0.1407,\n",
      "         0.0821,  0.1503, -0.1192,  0.0636,  0.0918, -0.0246, -0.0702, -0.0894,\n",
      "         0.0893, -0.0841,  0.1173,  0.0422, -0.0914,  0.0607, -0.0657,  0.2598,\n",
      "         0.0446,  0.0966,  0.0705, -0.0463, -0.0552,  0.0721, -0.0756,  0.1133,\n",
      "        -0.0974, -0.0771,  0.0648, -0.0178,  0.0948, -0.1447, -0.0807,  0.1277,\n",
      "        -0.0328,  0.0962,  0.1478, -0.0369, -0.1127,  0.0909,  0.0183,  0.1330,\n",
      "        -0.0669,  0.1663, -0.0812,  0.0717,  0.0208, -0.0639,  0.0540,  0.1727,\n",
      "         0.1249, -0.1014, -0.1669, -0.1188, -0.1236,  0.0389,  0.0524, -0.0937],\n",
      "       device='cuda:0', requires_grad=True))\n",
      "('projection.0.weight', Parameter containing:\n",
      "tensor([[-0.1078,  0.0099,  0.0508,  ..., -0.0356,  0.0178, -0.0415],\n",
      "        [ 0.0731,  0.0121,  0.0452,  ..., -0.0726,  0.0605, -0.0737],\n",
      "        [ 0.0185,  0.0305,  0.0742,  ..., -0.1156, -0.0099, -0.0143],\n",
      "        ...,\n",
      "        [ 0.0024,  0.0390,  0.0402,  ..., -0.0354,  0.0083, -0.0462],\n",
      "        [ 0.0702,  0.0117, -0.0010,  ...,  0.0027, -0.0122,  0.0545],\n",
      "        [-0.0928,  0.0640, -0.0907,  ..., -0.0589, -0.0273, -0.0404]],\n",
      "       device='cuda:0', requires_grad=True))\n",
      "('projection.0.bias', Parameter containing:\n",
      "tensor([ 0.0285,  0.1594,  0.0631,  0.0639,  0.0593,  0.0369, -0.0404, -0.0956,\n",
      "        -0.0197,  0.0538, -0.0336,  0.1254, -0.0028, -0.1346, -0.0587,  0.1188,\n",
      "        -0.0586, -0.0183, -0.0464,  0.0472,  0.0637,  0.0488, -0.0085, -0.1190,\n",
      "         0.0595,  0.0054,  0.0656,  0.0560, -0.0742,  0.1273,  0.0475,  0.0523,\n",
      "         0.1160,  0.1052,  0.0388,  0.0100,  0.1332, -0.0337, -0.0829, -0.0742,\n",
      "        -0.0295, -0.0185,  0.0780,  0.0237,  0.0061, -0.0078, -0.0776,  0.0521,\n",
      "        -0.0520, -0.0686], device='cuda:0', requires_grad=True))\n",
      "('projection.3.weight', Parameter containing:\n",
      "tensor([[-1.5527e-01, -3.0693e-03, -1.6444e-02, -8.8326e-02, -1.5415e-01,\n",
      "         -1.6205e-02,  1.5700e-01, -6.5422e-02, -1.8453e-01, -1.0750e-01,\n",
      "         -1.1457e-01, -2.9048e-02, -1.6533e-01,  1.3043e-03,  1.6172e-01,\n",
      "          2.3363e-02,  1.0317e-01,  1.0007e-01,  1.6012e-01, -3.6003e-02,\n",
      "         -1.6320e-01, -8.4787e-02,  7.9475e-02,  2.8901e-04, -6.2134e-02,\n",
      "         -1.5702e-01, -1.6402e-01, -1.2584e-01,  1.6687e-02,  3.2441e-02,\n",
      "         -1.8745e-01, -1.6089e-01, -1.0702e-02, -5.5713e-02, -1.2495e-01,\n",
      "          5.3239e-02, -1.4021e-03,  1.2986e-01,  7.4293e-02,  1.8635e-01,\n",
      "          1.6900e-01,  8.5075e-02, -1.4005e-01,  6.6541e-02, -4.6714e-03,\n",
      "          1.7590e-01,  1.4537e-02, -4.2900e-03,  1.7928e-01,  8.9238e-03],\n",
      "        [ 4.6931e-02, -1.8656e-01,  9.1631e-03, -1.2320e-01, -1.3366e-01,\n",
      "         -1.8440e-02,  8.9829e-03, -9.8602e-05,  1.0620e-01, -1.0014e-01,\n",
      "          1.4908e-01, -1.1835e-02,  7.1426e-02,  2.4459e-02,  9.4279e-02,\n",
      "         -2.8042e-01,  4.0457e-02, -1.1983e-02,  1.7298e-02, -5.8806e-02,\n",
      "          5.4175e-02, -1.3808e-01, -4.8869e-04,  5.3031e-02,  4.0420e-02,\n",
      "         -3.3021e-02, -1.0029e-01, -9.7691e-03,  6.1990e-02, -9.1992e-02,\n",
      "         -2.7485e-02,  4.4740e-02, -2.8439e-01, -1.6101e-01, -8.0261e-02,\n",
      "          2.5298e-02, -1.8387e-01,  9.3730e-03,  4.1847e-02,  8.5596e-02,\n",
      "          6.3546e-02,  8.8678e-02, -5.2384e-03,  5.6621e-02, -1.0514e-02,\n",
      "          5.5428e-02,  1.0555e-01,  2.0084e-02,  4.0276e-02,  9.0149e-02],\n",
      "        [-1.3354e-01, -1.2520e-01, -1.7547e-01, -4.1447e-02, -2.0196e-01,\n",
      "         -1.3070e-01,  7.0515e-02,  7.2030e-02, -7.9795e-02, -1.0401e-01,\n",
      "          7.0541e-02, -1.1037e-01, -1.1259e-01,  9.7566e-02,  1.0383e-01,\n",
      "          3.6486e-03,  1.0099e-01, -4.3781e-02,  9.2079e-02, -1.8415e-02,\n",
      "         -1.0635e-01, -1.0168e-01, -1.3546e-01,  1.6027e-01, -1.4495e-01,\n",
      "         -9.8239e-02,  1.0102e-02, -5.8099e-04,  4.0177e-02, -2.2725e-01,\n",
      "         -7.4722e-02, -1.2472e-01, -7.6233e-02, -1.2207e-01, -1.2098e-01,\n",
      "         -2.2340e-02, -6.4486e-02,  5.6190e-02,  1.0534e-01,  8.0342e-02,\n",
      "          4.9847e-02,  1.3072e-01, -6.7201e-02, -3.4731e-02, -2.0522e-02,\n",
      "          1.7257e-02,  1.9810e-01, -8.2526e-02,  9.5493e-02,  1.4796e-01],\n",
      "        [-1.8400e-01,  5.8254e-03,  9.2990e-03, -1.8573e-01, -8.4947e-02,\n",
      "          5.5945e-03,  4.0287e-02,  7.1199e-02, -3.1555e-03, -1.7775e-01,\n",
      "         -3.5844e-02,  9.3350e-02, -1.4063e-01, -1.2323e-01,  1.0009e-02,\n",
      "         -2.6793e-01,  7.4702e-03, -3.7315e-02,  4.0440e-02, -1.0362e-01,\n",
      "         -8.7631e-02, -1.3855e-01,  4.1105e-02, -1.4760e-02, -1.0874e-01,\n",
      "         -5.1898e-02, -1.2536e-01, -1.2856e-01,  7.2762e-02,  1.2114e-01,\n",
      "          4.6777e-02, -1.1156e-02, -1.5010e-01, -1.8210e-01, -7.0962e-02,\n",
      "          9.3953e-02, -2.1956e-02,  5.7962e-02, -8.0698e-02,  3.4799e-02,\n",
      "          9.2911e-04, -2.6200e-02,  1.0443e-02,  1.4149e-01, -1.1156e-01,\n",
      "          5.2097e-02, -6.2720e-02, -5.2002e-02, -2.3288e-02,  9.2714e-02],\n",
      "        [-5.8760e-02, -9.2889e-02, -1.2244e-01, -1.2784e-01, -1.3916e-01,\n",
      "          1.6264e-02,  6.9619e-02,  8.0899e-02, -2.0779e-02, -5.9034e-02,\n",
      "          2.6530e-02, -1.6575e-01, -6.4687e-02,  2.2550e-02,  1.0183e-01,\n",
      "         -4.2642e-02,  3.7005e-02,  1.5238e-01,  6.1014e-02, -9.0978e-02,\n",
      "         -1.1627e-01, -1.9494e-01,  1.5210e-01,  2.8412e-02, -4.4315e-02,\n",
      "         -5.5034e-02, -1.3289e-01, -1.4722e-01,  7.1681e-02, -7.5503e-02,\n",
      "         -6.4471e-02, -9.2852e-02, -9.1604e-02, -1.4967e-01, -2.8835e-02,\n",
      "          2.0295e-02, -3.0848e-02,  6.1687e-02,  6.9389e-02,  8.1903e-02,\n",
      "          1.7826e-02, -4.6003e-02, -1.2139e-01,  1.8869e-02, -3.1296e-02,\n",
      "          8.0230e-02, -9.5456e-02, -2.8776e-02,  3.3992e-02, -8.3218e-03],\n",
      "        [ 1.4353e-02, -5.5240e-02,  5.6188e-02, -1.6906e-01, -9.7442e-02,\n",
      "          7.9726e-02,  2.2193e-02,  1.3186e-01,  1.6980e-02, -1.5055e-01,\n",
      "         -5.1397e-02, -4.8725e-02, -1.0077e-01,  1.4979e-03,  4.0195e-02,\n",
      "         -3.0060e-01, -1.9514e-02, -1.3512e-01,  4.0140e-03, -2.8587e-02,\n",
      "         -1.4421e-01, -9.2639e-02,  7.0437e-02,  5.6250e-02, -9.3187e-02,\n",
      "         -1.0293e-01, -4.6507e-02,  2.3372e-02,  4.7065e-02,  1.2598e-01,\n",
      "         -1.5679e-01,  1.6324e-02, -1.6742e-01, -1.8255e-02, -1.0446e-01,\n",
      "          5.5096e-02, -1.1745e-01,  1.1742e-02,  1.0837e-02,  9.8549e-02,\n",
      "          3.2406e-02, -1.1630e-01, -1.1975e-01,  1.3119e-01,  2.2004e-01,\n",
      "         -1.9186e-02, -5.7187e-02,  1.4999e-01, -8.8980e-03,  1.4141e-01]],\n",
      "       device='cuda:0', requires_grad=True))\n",
      "('projection.3.bias', Parameter containing:\n",
      "tensor([-2.1506, -4.7089, -2.9253, -5.7310, -2.9731, -4.7180], device='cuda:0',\n",
      "       requires_grad=True))\n"
     ]
    }
   ],
   "source": [
    "for p in model.named_parameters():\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import expit\n",
    "from collections import defaultdict\n",
    "from allennlp.common.tqdm import Tqdm\n",
    "\n",
    "def dict_append(d: Dict[str, List], upd: Dict[str, Any]) -> Dict[str, List]:\n",
    "    for k, v in upd.items(): d[k].append(v)\n",
    "\n",
    "def tonp(tsr): return tsr.detach().cpu().numpy()\n",
    "        \n",
    "class Predictor:\n",
    "    def __init__(self, model: Model, iterator: DataIterator,\n",
    "                 cuda_device: int=-1) -> None:\n",
    "        self.model = model\n",
    "        self.iterator = iterator\n",
    "        self.cuda_device = cuda_device\n",
    "        \n",
    "    def _extract_data(self, batch) -> Dict[str, np.ndarray]:\n",
    "        out_dict = self.model(**batch)\n",
    "        lens = tonp(get_text_field_mask(batch[\"tokens\"]).sum(1))\n",
    "        \n",
    "        if self.model.loss._get_name()==\"MBernLossWithLogits\":\n",
    "            preds = tonp(out_dict[\"marginal_probs\"])\n",
    "        else:\n",
    "            preds = expit(tonp(out_dict[\"class_logits\"]))\n",
    "        \n",
    "        return {\n",
    "                \"preds\": preds,\n",
    "                \"oov_ratio\": tonp((batch[\"tokens\"][\"tokens\"] == 1).sum(1)) / lens,\n",
    "                \"lens\": lens,\n",
    "               }\n",
    "        \n",
    "    def _postprocess(self, predictions: Dict[str, np.ndarray]) -> np.ndarray:\n",
    "        return {k: np.concatenate(v, axis=0) for k, v in predictions.items()}\n",
    "    \n",
    "    @gpu_mem_restore\n",
    "    def predict(self, ds: Iterable[Instance]) -> np.ndarray:\n",
    "        pred_generator = self.iterator(ds, num_epochs=1, shuffle=False)\n",
    "        self.model.eval()\n",
    "        pred_generator_tqdm = Tqdm.tqdm(pred_generator,\n",
    "                                        total=self.iterator.get_num_batches(ds))\n",
    "        preds = defaultdict(list)\n",
    "        with torch.no_grad():\n",
    "            for batch in pred_generator_tqdm:\n",
    "                batch = nn_util.move_to_device(batch, self.cuda_device)\n",
    "                dict_append(preds, self._extract_data(batch))\n",
    "        return self._postprocess(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.data.iterators import BasicIterator\n",
    "seq_iterator = BasicIterator(batch_size=64)\n",
    "seq_iterator.index_with(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Horrible solution to the shuffling problem with BasicIterator\n",
    "# TODO: Solve more elegantly?\n",
    "if not config.bucket:\n",
    "    del train_ds; import gc; gc.collect()\n",
    "    if config.val_ratio > 0.0:\n",
    "        train_ds = reader.read(DATA_ROOT / \"train_wo_val.csv\")\n",
    "    else:\n",
    "        train_ds = reader.read(DATA_ROOT / \"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2494/2494 [04:19<00:00, 10.58it/s]\n",
      "100%|██████████| 1000/1000 [01:55<00:00,  6.76it/s]\n"
     ]
    }
   ],
   "source": [
    "predictor = Predictor(model, seq_iterator, cuda_device=0 if USE_GPU else -1)\n",
    "train_meta = predictor.predict(train_ds) \n",
    "train_preds = train_meta.pop(\"preds\")\n",
    "test_meta = predictor.predict(test_ds)\n",
    "test_preds = test_meta.pop(\"preds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst_df = pd.read_csv(DATA_ROOT / \"test_proced.csv\")\n",
    "test_labels = tst_df[label_cols].values\n",
    "test_texts = tst_df[JIGSAW_TEXT_NAME].values\n",
    "if config.testing:\n",
    "    test_labels = test_labels[:len(test_ds), :]\n",
    "    test_texts = test_texts[:len(test_ds)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, f1_score, accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluator:\n",
    "    def __init__(self, thres=0.5):\n",
    "        if isinstance(thres, float):\n",
    "            self.thres = np.ones(len(label_cols)) * thres\n",
    "        else:\n",
    "            self.thres = thres\n",
    "    \n",
    "    def _to_metric_dict(self, t: np.ndarray, y: np.ndarray, thres: float) -> Dict:\n",
    "        tn, fp, fn, tp = confusion_matrix(t, y >= thres).ravel()\n",
    "        return {\"auc\": roc_auc_score(t, y),\n",
    "                \"f1\": f1_score(t, y >= thres),\n",
    "                \"acc\": accuracy_score(t, y >= thres),\n",
    "                \"tnr\": tn / len(t), \"fpr\": fp / len(t),\n",
    "                \"fnr\": fn / len(t), \"tpr\": tp / len(t),\n",
    "                \"precision\": tp / (tp + fp), \"recall\": tp / (tp + fn),\n",
    "        }\n",
    "\n",
    "    def _stats_per_quadrant(self, tgt, preds, \n",
    "                            metadata: Dict[str, np.ndarray],\n",
    "                            texts: np.ndarray=None):\n",
    "        out_data = {}\n",
    "        for i, lbl in enumerate(label_cols):\n",
    "            # get indicies of each quadrant`\n",
    "            preds_bin = preds[:, i] >= self.thres[i]\n",
    "            quads = {\n",
    "                \"tp\": np.where((tgt[:, i] == 1) & preds_bin)[0],\n",
    "                \"fp\": np.where((tgt[:, i] == 0) & preds_bin)[0],\n",
    "                \"tn\": np.where((tgt[:, i] == 0) & ~preds_bin)[0],\n",
    "                \"fn\": np.where((tgt[:, i] == 1) & ~preds_bin)[0],\n",
    "            }\n",
    "            \n",
    "            # get stats for metadata\n",
    "            out_data[lbl] = {}\n",
    "            for quad, qidxs in quads.items():\n",
    "                quad_data = {}\n",
    "                for k, full_data in metadata.items():\n",
    "                    data = full_data[qidxs]\n",
    "                    for metric in [\"mean\", \"std\", \"min\", \"max\"]:\n",
    "                        if len(data) > 0:\n",
    "                            quad_data[f\"{k}_{metric}\"] = getattr(data, metric)()\n",
    "                        else:\n",
    "                            quad_data[f\"{k}_{metric}\"] = np.nan\n",
    "\n",
    "                out_data[lbl][quad] = quad_data\n",
    "            \n",
    "            # do error analysis\n",
    "            if texts is not None:\n",
    "                for quad, qidxs in quads.items():\n",
    "                    quad_preds = preds[qidxs, i]\n",
    "                    if len(quad_preds) == 0: continue\n",
    "                    if quad in [\"tp\", \"fp\"]:\n",
    "                        out_data[lbl][quad][\"most_confident\"] = texts[qidxs[quad_preds.argmax()]]\n",
    "                        out_data[lbl][quad][\"most_confident_prob\"] = quad_preds.max()\n",
    "                        out_data[lbl][quad][\"least_confident\"] = texts[qidxs[quad_preds.argmin()]]\n",
    "                        out_data[lbl][quad][\"least_confident_prob\"] = quad_preds.min()\n",
    "                    else:\n",
    "                        out_data[lbl][quad][\"most_confident\"] = texts[qidxs[quad_preds.argmin()]]\n",
    "                        out_data[lbl][quad][\"most_confident_prob\"] = quad_preds.min()\n",
    "                        out_data[lbl][quad][\"least_confident\"] = texts[qidxs[quad_preds.argmax()]]\n",
    "                        out_data[lbl][quad][\"least_confident_prob\"] = quad_preds.max()\n",
    "        return out_data        \n",
    "    \n",
    "    @gpu_mem_restore\n",
    "    def evaluate(self, tgt: np.ndarray, preds: np.ndarray,\n",
    "                 trn_tgt: np.ndarray, trn_preds: np.ndarray,\n",
    "                 metadata: Dict[str, np.ndarray]={}, \n",
    "                 texts: np.ndarray=None) -> Dict:\n",
    "        \"\"\"\n",
    "        Metadata: Data about the inputs (e.g. length, OOV ratio)\n",
    "        \"\"\"\n",
    "        train_label_metrics = {}\n",
    "        label_metrics = {}\n",
    "                \n",
    "        # get per-label stats\n",
    "        for i, lbl in enumerate(label_cols):\n",
    "            train_label_metrics[lbl] = self._to_metric_dict(trn_tgt[:, i],\n",
    "                                                            trn_preds[:, i],\n",
    "                                                            self.thres[i])\n",
    "            label_metrics[lbl] = self._to_metric_dict(tgt[:, i], preds[:, i],\n",
    "                                                      self.thres[i])\n",
    "            print(f\"========{lbl}=========\")\n",
    "            print(label_metrics[lbl])\n",
    "        \n",
    "        # get global stats\n",
    "        for mtrc in label_metrics[\"toxic\"].keys():\n",
    "            label_metrics[f\"global_{mtrc}\"] = \\\n",
    "                np.mean([label_metrics[col][mtrc] for col in label_cols])\n",
    "            \n",
    "        # get per-label-quadrant stats\n",
    "        quad_stats = self._stats_per_quadrant(tgt, preds, metadata=metadata, texts=texts)\n",
    "        if len(quad_stats) > 0:\n",
    "            for c in label_cols:\n",
    "                label_metrics[c][\"quad_stats\"] = quad_stats[c]\n",
    "\n",
    "        label_metrics[\"train\"] = train_label_metrics,\n",
    "        return label_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anna/neuralnlp/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# Compute best threshold based on training data\n",
    "if config.compute_thres_on_test:\n",
    "    lbls, pds = test_labels, test_preds\n",
    "else:\n",
    "    lbls, pds = train_labels, train_preds\n",
    "    \n",
    "thres = np.zeros(len(label_cols))\n",
    "best_scores = np.zeros(len(label_cols))\n",
    "for i, col in enumerate(label_cols):\n",
    "    best_score = -1\n",
    "    best_thres = -1\n",
    "    for x in np.linspace(0, 1.0, num=999):\n",
    "        scr = f1_score(lbls[:, i], pds[:, i] >= x)\n",
    "        if scr > best_score:\n",
    "            best_thres = x\n",
    "            best_score = scr\n",
    "    thres[i] = best_thres\n",
    "    best_scores[i] = best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.37775551, 0.22144289, 0.35470942, 0.05911824, 0.36673347,\n",
       "       0.23046092])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========toxic=========\n",
      "{'auc': 0.9692859494377227, 'f1': 0.6755386565272496, 'acc': 0.9199724905436244, 'tnr': 0.8366626027697021, 'fpr': 0.06814842602144487, 'fnr': 0.011879083434930757, 'tpr': 0.08330988777392229, 'precision': 0.5500515995872033, 'recall': 0.8752052545155994}\n",
      "========severe_toxic=========\n",
      "{'auc': 0.9887240382267269, 'f1': 0.4213938411669368, 'acc': 0.9888399137203414, 'tnr': 0.9847760167557598, 'fpr': 0.009487636375003907, 'fnr': 0.0016724499046547252, 'tpr': 0.004063896964581575, 'precision': 0.29988465974625145, 'recall': 0.7084468664850136}\n",
      "========obscene=========\n",
      "{'auc': 0.9781865949193076, 'f1': 0.6757250513815939, 'acc': 0.9556097408484167, 'tnr': 0.9093594673168902, 'fpr': 0.03294882615899215, 'fnr': 0.011441432992591203, 'tpr': 0.04625027353152646, 'precision': 0.5839747385040458, 'recall': 0.8016797615822271}\n",
      "========threat=========\n",
      "{'auc': 0.9647828881167418, 'f1': 0.1789473684210526, 'acc': 0.9926849854637532, 'tnr': 0.9918878364437775, 'fpr': 0.004814154865735097, 'fnr': 0.0025008596705117384, 'tpr': 0.0007971490199756166, 'precision': 0.14206128133704735, 'recall': 0.24170616113744076}\n",
      "========insult=========\n",
      "{'auc': 0.974361699798606, 'f1': 0.6611243072050672, 'acc': 0.9598612022882866, 'tnr': 0.9207071180718372, 'fpr': 0.02572759386038951, 'fnr': 0.014411203851323892, 'tpr': 0.039154084216449406, 'precision': 0.6034690436039508, 'recall': 0.7309600233440326}\n",
      "========identity_hate=========\n",
      "{'auc': 0.9673781726663628, 'f1': 0.4447552447552448, 'acc': 0.9875894838850855, 'tnr': 0.9826190252899434, 'fpr': 0.006252149176279346, 'fnr': 0.006158366938635156, 'tpr': 0.00497045859514208, 'precision': 0.4428969359331476, 'recall': 0.44662921348314605}\n"
     ]
    }
   ],
   "source": [
    "evaluator = Evaluator(thres=thres)\n",
    "label_metrics = evaluator.evaluate(\n",
    "    test_labels, test_preds,\n",
    "    train_labels, train_preds,\n",
    "    metadata=test_meta, texts=test_texts,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9737865571942447"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_metrics[\"global_auc\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'toxic': {'auc': 0.9692859494377227,\n",
       "  'f1': 0.6755386565272496,\n",
       "  'acc': 0.9199724905436244,\n",
       "  'tnr': 0.8366626027697021,\n",
       "  'fpr': 0.06814842602144487,\n",
       "  'fnr': 0.011879083434930757,\n",
       "  'tpr': 0.08330988777392229,\n",
       "  'precision': 0.5500515995872033,\n",
       "  'recall': 0.8752052545155994,\n",
       "  'quad_stats': {'tp': {'oov_ratio_mean': 0.0,\n",
       "    'oov_ratio_std': 0.0,\n",
       "    'oov_ratio_min': 0.0,\n",
       "    'oov_ratio_max': 0.0,\n",
       "    'lens_mean': 52.33939962476548,\n",
       "    'lens_std': 141.16227075273093,\n",
       "    'lens_min': 1,\n",
       "    'lens_max': 4950,\n",
       "    'most_confident': 'FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU FUCK YOU',\n",
       "    'most_confident_prob': 0.99998593,\n",
       "    'least_confident': '*Please do not refer to my comment as moronic and retarded , that is a personal attack',\n",
       "    'least_confident_prob': 0.37833354},\n",
       "   'fp': {'oov_ratio_mean': 0.0,\n",
       "    'oov_ratio_std': 0.0,\n",
       "    'oov_ratio_min': 0.0,\n",
       "    'oov_ratio_max': 0.0,\n",
       "    'lens_mean': 73.323623853211,\n",
       "    'lens_std': 155.48249729422713,\n",
       "    'lens_min': 1,\n",
       "    'lens_max': 3307,\n",
       "    'most_confident': 'fuck u ill do whatever I fucking want boy',\n",
       "    'most_confident_prob': 0.99910885,\n",
       "    'least_confident': '\" \\n\\n ::Yeah, I agree with the block as well. That was really stupid of me. But I did not think that that deserved a whole week. Well, 48 hours it is!  \"',\n",
       "    'least_confident_prob': 0.3779649},\n",
       "   'tn': {'oov_ratio_mean': 0.0,\n",
       "    'oov_ratio_std': 0.0,\n",
       "    'oov_ratio_min': 0.0,\n",
       "    'oov_ratio_max': 0.0,\n",
       "    'lens_mean': 84.82452174562846,\n",
       "    'lens_std': 120.21965016831952,\n",
       "    'lens_min': 1,\n",
       "    'lens_max': 1640,\n",
       "    'most_confident': \"::* I've made some additional copy edits and fixed a broken link in the External Links section. Looks good to me.    \\n\\n ===GAR Criteria Checklist=== \\n :GA review (see here for what the criteria are, and here for what they are not) \\n\\n #It is reasonably well written. \\n #:a (prose, no copyvios, spelling and grammar):  b (MoS for lead, layout, word choice, fiction, and lists):  \\n #::  \\n #It is factually accurate and verifiable. \\n #:a (reference section):  b (citations to reliable sources):  c (OR):  \\n #::  \\n #It is broad in its coverage. \\n #:a (major aspects):  b (focused):  \\n #::  \\n #It follows the neutral point of view policy. \\n #:Fair representation without bias:  \\n #::  \\n #It is stable. \\n #:No edit wars, etc.:  \\n #::  \\n #It is illustrated by images and other media, where possible and appropriate. \\n #:a (images are tagged and non-free content have fair use rationales):  b (appropriate use with suitable captions):  \\n #:: no images available for use \\n #Overall:  \\n #:Pass/Fail:  \\n #:: Meets all criteria for good article.\",\n",
       "    'most_confident_prob': 6.7485926e-06,\n",
       "    'least_confident': '*You were here only to put pictures in articles that tickle your sexual fancy. Please play elsewhere.',\n",
       "    'least_confident_prob': 0.37768856},\n",
       "   'fn': {'oov_ratio_mean': 0.0,\n",
       "    'oov_ratio_std': 0.0,\n",
       "    'oov_ratio_min': 0.0,\n",
       "    'oov_ratio_max': 0.0,\n",
       "    'lens_mean': 56.57368421052632,\n",
       "    'lens_std': 91.1145308308826,\n",
       "    'lens_min': 3,\n",
       "    'lens_max': 1505,\n",
       "    'most_confident': '\"::::The controversy should be covered in the article. Googling frozen gay, there are numerous articles covering it. Wikipedia should summarize the controversy. I think it should be covered in a section apart from \"\"Critical reception\"\", and we should come up with a neutral section heading to define the matter. WP:FRINGE does not apply here because even if the groups were fringe, the controversy has clearly become discussed in the mainstream. Wikipedia should not ignore such discussion. \\xa0\\xa0|\\xa0 (ping me)  \\n\\n \"',\n",
       "    'most_confident_prob': 0.0012717206,\n",
       "    'least_confident': \"::::::::::I have so many better things to do than some stupid vandalism like that...you can check who did it with the edit history you know, but it's likely just an anon.—\",\n",
       "    'least_confident_prob': 0.3776804}}},\n",
       " 'severe_toxic': {'auc': 0.9887240382267269,\n",
       "  'f1': 0.4213938411669368,\n",
       "  'acc': 0.9888399137203414,\n",
       "  'tnr': 0.9847760167557598,\n",
       "  'fpr': 0.009487636375003907,\n",
       "  'fnr': 0.0016724499046547252,\n",
       "  'tpr': 0.004063896964581575,\n",
       "  'precision': 0.29988465974625145,\n",
       "  'recall': 0.7084468664850136,\n",
       "  'quad_stats': {'tp': {'oov_ratio_mean': 0.0,\n",
       "    'oov_ratio_std': 0.0,\n",
       "    'oov_ratio_min': 0.0,\n",
       "    'oov_ratio_max': 0.0,\n",
       "    'lens_mean': 162.27307692307693,\n",
       "    'lens_std': 290.9498873817545,\n",
       "    'lens_min': 3,\n",
       "    'lens_max': 1345,\n",
       "    'most_confident': 'FUCK MY ASS HOLE! YOU SON OF A BITCH! FUCK MY ASS HOLE! YOU SON OF A BITCH! FUCK MY ASS HOLE! YOU SON OF A BITCH! FUCK MY ASS HOLE! YOU SON OF A BITCH! FUCK MY ASS HOLE! YOU SON OF A BITCH! FUCK MY ASS HOLE! YOU SON OF A BITCH! FUCK MY ASS HOLE! YOU SON OF A BITCH! FUCK MY ASS HOLE! YOU SON OF A BITCH! FUCK MY ASS HOLE! YOU SON OF A BITCH! FUCK MY ASS HOLE! YOU SON OF A BITCH! FUCK MY ASS HOLE! YOU SON OF A BITCH! FUCK MY ASS HOLE! YOU SON OF A BITCH! FUCK MY ASS HOLE! YOU SON OF A BITCH! FUCK MY ASS HOLE! YOU SON OF A BITCH! FUCK MY ASS HOLE! YOU SON OF A BITCH! FUCK MY ASS HOLE! YOU SON OF A BITCH! FUCK MY ASS HOLE! YOU SON OF A BITCH! FUCK MY ASS HOLE! YOU SON OF A BITCH! FUCK MY ASS HOLE! YOU SON OF A BITCH! FUCK MY ASS HOLE! YOU SON OF A BITCH! FUCK MY ASS HOLE! YOU SON OF A BITCH! FUCK MY ASS HOLE! YOU SON OF A BITCH! FUCK MY ASS HOLE! YOU SON OF A BITCH! FUCK MY ASS HOLE! YOU SON OF A BITCH! FUCK MY ASS HOLE! YOU SON OF A BITCH! FUCK MY ASS HOLE! YOU SON OF A BITCH! FUCK MY ASS HOLE! YOU SON OF A BITCH! FUCK MY ASS HOLE! YOU SON OF A BITCH! FUCK MY ASS HOLE! YOU SON OF A BITCH! FUCK MY ASS HOLE! YOU SON OF A BITCH! FUCK MY ASS HOLE! YOU SON OF A BITCH! FUCK MY ASS HOLE! YOU SON OF A BITCH! FUCK MY ASS HOLE! YOU SON OF A BITCH! FUCK MY ASS HOLE! YOU SON OF A BITCH! FUCK MY ASS HOLE! YOU SON OF A BITCH! FUCK MY ASS HOLE! YOU SON OF A BITCH! FUCK MY ASS HOLE! YOU SON OF A BITCH! FUCK MY ASS HOLE! YOU SON OF A BITCH! FUCK MY ASS HOLE! YOU SON OF A BITCH! FUCK MY ASS HOLE! YOU SON OF A BITCH! FUCK MY ASS HOLE! YOU SON OF A BITCH! FUCK MY ASS HOLE! YOU SON OF A BITCH! FUCK MY ASS HOLE! YOU SON OF A BITCH! FUCK MY ASS HOLE! YOU SON OF A BITCH! FUCK MY ASS HOLE! YOU SON OF A BITCH! FUCK MY ASS HOLE! YOU SON OF A BITCH! FUCK MY ASS HOLE! YOU SON OF A BITCH! FUCK MY ASS HOLE! YOU SON OF A BITCH! FUCK MY ASS HOLE! YOU SON OF A BITCH! FUCK MY ASS HOLE! YOU SON OF A BITCH! FUCK MY ASS HOLE! YOU SON OF A BITCH! FUCK MY ASS HOLE! YOU SON OF A BITCH! FUCK MY ASS HOLE! YOU SON OF A BITCH! FUCK MY ASS HOLE! YOU SON OF A BITCH! FUCK MY ASS HOLE! YOU SON OF A BITCH! FUCK MY ASS HOLE! YOU SON OF A BITCH! FUCK MY ASS HOLE! YOU SON OF A BITCH! FUCK MY ASS HOLE! YOU SON OF A BITCH! FUCK MY ASS HOLE! YOU SON OF A BITCH! FUCK MY ASS HOLE! YOU SON OF A BITCH! FUCK MY ASS HOLE! YOU SON OF A BITCH! FUCK MY ASS HOLE! YOU SON OF A BITCH! FUCK MY ASS HOLE! YOU SON OF A BITCH! FUCK MY ASS HOLE! YOU SON OF A BITCH! FUCK MY ASS HOLE! YOU SON OF A BITCH! FUCK MY ASS HOLE! YOU SON OF A BITCH! FUCK MY ASS HOLE! YOU SON OF A BITCH! FUCK MY ASS HOLE! YOU SON OF A BITCH! FUCK MY ASS HOLE! YOU SON OF A BITCH! FUCK MY ASS HOLE! YOU SON OF A BITCH! FUCK MY ASS HOLE! YOU SON OF A BITCH! FUCK MY ASS HOLE! YOU SON OF A BITCH! FUCK MY ASS HOLE! YOU SON OF A BITCH! FUCK MY ASS HOLE! YOU SON OF A BITCH! FUCK MY ASS HOLE! YOU SON OF A BITCH! FUCK MY ASS HOLE! YOU SON OF A BITCH! FUCK MY ASS HOLE! YOU SON OF A BITCH! FUCK MY ASS HOLE! YOU SON OF A BITCH! FUCK MY ASS HOLE! YOU SON OF A BITCH! FUCK MY ASS HOLE! YOU SON OF A BITCH! FUCK MY ASS HOLE! YOU SON OF A BITCH! FUCK MY ASS HOLE! YOU SON OF A BITCH! FUCK MY ASS HOLE! YOU SON OF A BITCH! FUCK MY ASS HOLE! YOU SON OF A BITCH! FUCK MY ASS HOLE! YOU SON OF A BITCH! FUCK MY ASS HOLE! YOU SON OF A BITCH! FUCK MY ASS HOLE! YOU SON OF A BITCH! FUCK MY ASS HOLE! YOU SON OF A BITCH! FUCK MY ASS HOLE! YOU SON OF A BITCH! FUCK MY ASS HOLE! YOU SON OF A BITCH! FUCK MY ASS HOLE! YOU SON OF A BITCH! FUCK MY ASS HOLE! YOU SON OF A BITCH! FUCK MY ASS HOLE! YOU SON OF A BITCH! FUCK MY ASS HOLE! YOU SON OF A BITCH! FUCK MY ASS HOLE! YOU SON OF A BITCH! FUCK MY ASS HOLE! YOU SON OF A BITCH! FUCK MY ASS HOLE! YOU SON OF A BITCH! FUCK MY ASS HOLE! YOU SON OF A BITCH! FUCK MY ASS HOLE! YOU SON OF A BITCH! FUCK MY ASS HOLE! YOU SON OF A BITCH! FUCK MY ASS HOLE! YOU SON OF A BITCH! FUCK MY ASS HOLE! YOU SON OF A BITCH! FUCK MY ASS HOLE! YOU SON OF A BITCH! FUCK MY ASS HOLE! YOU SON OF A BITCH! FUCK MY ASS HOLE! YOU SON OF A BITCH! FUCK MY ASS HOLE! YOU SON OF A BITCH! FUCK MY ASS HOLE! YOU SON OF A BITCH! FUCK MY ASS HOLE! YOU SON OF A BITCH! FUCK MY ASS HOLE! YOU SON OF A BITCH! FUCK MY ASS HOLE! YOU SON OF A BITCH! FUCK MY ASS HOLE! YOU SON OF A BITCH! FUCK MY ASS HOLE! YOU SON OF A BITCH! FUCK MY ASS HOLE! YOU SON OF A BITCH! FUCK MY ASS HOLE! YOU SON OF A BITCH!  \\n\\n SHIT SHIT SHIT \\n SHIT SHIT SHIT \\n SHIT SHIT SHIT \\n SHIT SHIT SHIT \\n SHIT SHIT SHIT \\n SHIT SHIT SHIT \\n SHIT SHIT SHIT \\n SHIT SHIT SHIT \\n SHIT SHIT SHIT \\n SHIT SHIT SHIT \\n SHIT SHIT SHIT \\n SHIT SHIT SHIT \\n SHIT SHIT SHIT \\n SHIT SHIT SHIT \\n SHIT SHIT SHIT \\n SHIT SHIT SHIT \\n SHIT SHIT SHIT \\n SHIT SHIT SHIT \\n SHIT SHIT SHIT \\n SHIT SHIT SHIT \\n SHIT SHIT SHIT \\n SHIT SHIT SHIT \\n SHIT SHIT SHIT \\n SHIT SHIT SHIT \\n SHIT SHIT SHIT \\n SHIT SHIT SHIT \\n SHIT SHIT SHIT \\n SHIT SHIT SHIT \\n SHIT SHIT SHIT \\n SHIT SHIT SHIT \\n SHI',\n",
       "    'most_confident_prob': 0.8405538,\n",
       "    'least_confident': 'I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.I am going to rip off your tiny balls and decapitate you.[[I am going to rip off your tiny balls and decapitate you.',\n",
       "    'least_confident_prob': 0.2222016},\n",
       "   'fp': {'oov_ratio_mean': 0.0,\n",
       "    'oov_ratio_std': 0.0,\n",
       "    'oov_ratio_min': 0.0,\n",
       "    'oov_ratio_max': 0.0,\n",
       "    'lens_mean': 84.58484349258649,\n",
       "    'lens_std': 222.49669631059035,\n",
       "    'lens_min': 1,\n",
       "    'lens_max': 2321,\n",
       "    'most_confident': 'FUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUiiiiiiiiiiiiiiii \\n #;lkjh \\n 90876590876 \\n FUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUiiiiiiiiiiiiiiii \\n #;lkjh \\n 90876590876 \\n FUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUiiiiiiiiiiiiiiii \\n #;lkjh \\n 90876590876 \\n FUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUiiiiiiiiiiiiiiii \\n #;lkjh \\n 90876590876 \\n FUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUiiiiiiiiiiiiiiii \\n #;lkjh \\n 90876590876 \\n FUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUiiiiiiiiiiiiiiii \\n #;lkjh \\n 90876590876 \\n FUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUiiiiiiiiiiiiiiii \\n #;lkjh \\n 90876590876 \\n FUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUiiiiiiiiiiiiiiii \\n #;lkjh \\n 90876590876 \\n FUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUiiiiiiiiiiiiiiii \\n #;lkjh \\n 90876590876 \\n FUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUiiiiiiiiiiiiiiii \\n #;lkjh \\n 90876590876 \\n FUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUiiiiiiiiiiiiiiii \\n #;lkjh \\n 90876590876 \\n FUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUiiiiiiiiiiiiiiii \\n #;lkjh \\n 90876590876 \\n FUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUF',\n",
       "    'most_confident_prob': 0.8888985,\n",
       "    'least_confident': 'Does that give him a pass on being a RACIST ASS bitch? Making RACIST and lewd comments aimed at a 16 yr OLD GIRL(I bet that bastard would be screaming if a white guy made any comments half as bad as this racist ass did aimed at his own daughter) \\n I guess he is Oppressed that this lil white girl is making far more than his racist ass will before she turned 18 than he will the rest of his racist ass life',\n",
       "    'least_confident_prob': 0.22189234},\n",
       "   'tn': {'oov_ratio_mean': 0.0,\n",
       "    'oov_ratio_std': 0.0,\n",
       "    'oov_ratio_min': 0.0,\n",
       "    'oov_ratio_max': 0.0,\n",
       "    'lens_mean': 80.56831312297632,\n",
       "    'lens_std': 121.99806535471753,\n",
       "    'lens_min': 1,\n",
       "    'lens_max': 4950,\n",
       "    'most_confident': \"::* I've made some additional copy edits and fixed a broken link in the External Links section. Looks good to me.    \\n\\n ===GAR Criteria Checklist=== \\n :GA review (see here for what the criteria are, and here for what they are not) \\n\\n #It is reasonably well written. \\n #:a (prose, no copyvios, spelling and grammar):  b (MoS for lead, layout, word choice, fiction, and lists):  \\n #::  \\n #It is factually accurate and verifiable. \\n #:a (reference section):  b (citations to reliable sources):  c (OR):  \\n #::  \\n #It is broad in its coverage. \\n #:a (major aspects):  b (focused):  \\n #::  \\n #It follows the neutral point of view policy. \\n #:Fair representation without bias:  \\n #::  \\n #It is stable. \\n #:No edit wars, etc.:  \\n #::  \\n #It is illustrated by images and other media, where possible and appropriate. \\n #:a (images are tagged and non-free content have fair use rationales):  b (appropriate use with suitable captions):  \\n #:: no images available for use \\n #Overall:  \\n #:Pass/Fail:  \\n #:: Meets all criteria for good article.\",\n",
       "    'most_confident_prob': 2.621446e-07,\n",
       "    'least_confident': 'Hi. Go fuck yourself.    \\n :',\n",
       "    'least_confident_prob': 0.22095972},\n",
       "   'fn': {'oov_ratio_mean': 0.0,\n",
       "    'oov_ratio_std': 0.0,\n",
       "    'oov_ratio_min': 0.0,\n",
       "    'oov_ratio_max': 0.0,\n",
       "    'lens_mean': 116.66355140186916,\n",
       "    'lens_std': 232.15298468395343,\n",
       "    'lens_min': 4,\n",
       "    'lens_max': 1032,\n",
       "    'most_confident': \"== Why all muslims should be burned alive == \\n\\n Because they deserve it.  \\n\\n We hope to finish off some 800 million muslims by the year 2010.  \\n That's a lot of killing to do. But we will sure enjoy it.  \\n In the name of the United States of America in the name of freedom this must be accomplished.  \\n\\n Post your comments on how best to achieve this.\",\n",
       "    'most_confident_prob': 0.00013943264,\n",
       "    'least_confident': '\" \\n\\n ==Hi== \\n\\n Dbachmann, you are a motherfuckin\\' sonofabitch. Your ancestor Hitler had much more respect for Indians. He balked about Aryans.... We are Aryans. And your fuckin\\' racism won\\'t work here. You BASTARD !!!!! Indians Are Real Great People. It\\'s only Europeans who corrupted them !!!! Fuckin\\' sonofabitch, where are you from ? Zurich ? That is you are a SLAVE,  a SLAVE of AMERICA !!!! Rascal !!!! Madarchod !!!! Behenchod !!!!! Bhadva !!!!!!! RANDVA !!!!! Chutmarike !!!!! YOU FUCKIN\\' GAY !!!! SUCKER HOMO !!!!! ROT IN HELL, YOU SISTERFUCKER !!!! HECK, THEY WON\\'T TAKE YOU EVEN THERE !!! \\n\\n  \\n\\n \\' \\n\\n  \\n \\' \\n\\n  \\n \\' \\n\\n  \\n \\' \\n\\n  \\n \\' \\n\\n  \\n \\'\\'\\'\\'\\'\\' \\n\\n  \\n \\' \\n\\n  \\n \\' \\n\\n  \\n \\' \\n\\n  \\n \\' \\n\\n  \\n \\' \\n\\n  \\n \\'\\'\\'\\'\\'\\' \\n\\n  \\n \\'\\'\\'\\'\\'\\' \\n\\n  \\n \\'\\'\\'\\'\\'\\' \\n\\n  \\n \\'\\'\\'\\'\\'\\'',\n",
       "    'least_confident_prob': 0.22117363}}},\n",
       " 'obscene': {'auc': 0.9781865949193076,\n",
       "  'f1': 0.6757250513815939,\n",
       "  'acc': 0.9556097408484167,\n",
       "  'tnr': 0.9093594673168902,\n",
       "  'fpr': 0.03294882615899215,\n",
       "  'fnr': 0.011441432992591203,\n",
       "  'tpr': 0.04625027353152646,\n",
       "  'precision': 0.5839747385040458,\n",
       "  'recall': 0.8016797615822271,\n",
       "  'quad_stats': {'tp': {'oov_ratio_mean': 0.0,\n",
       "    'oov_ratio_std': 0.0,\n",
       "    'oov_ratio_min': 0.0,\n",
       "    'oov_ratio_max': 0.0,\n",
       "    'lens_mean': 61.38864481243663,\n",
       "    'lens_std': 174.309769068137,\n",
       "    'lens_min': 1,\n",
       "    'lens_max': 4950,\n",
       "    'most_confident': 'FUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUiiiiiiiiiiiiiiii \\n #;lkjh \\n 90876590876 \\n FUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUiiiiiiiiiiiiiiii \\n #;lkjh \\n 90876590876 \\n FUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUiiiiiiiiiiiiiiii \\n #;lkjh \\n 90876590876 \\n FUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUiiiiiiiiiiiiiiii \\n #;lkjh \\n 90876590876 \\n FUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUiiiiiiiiiiiiiiii \\n #;lkjh \\n 90876590876 \\n FUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUiiiiiiiiiiiiiiii \\n #;lkjh \\n 90876590876 \\n FUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUiiiiiiiiiiiiiiii \\n #;lkjh \\n 90876590876 \\n FUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUiiiiiiiiiiiiiiii \\n #;lkjh \\n 90876590876 \\n FUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUiiiiiiiiiiiiiiii \\n #;lkjh \\n 90876590876 \\n FUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUiiiiiiiiiiiiiiii \\n #;lkjh \\n 90876590876 \\n FUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUiiiiiiiiiiiiiiii \\n #;lkjh \\n 90876590876 \\n FUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUiiiiiiiiiiiiiiii \\n #;lkjh \\n 90876590876 \\n FUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUFUCK YOUF',\n",
       "    'most_confident_prob': 0.99969196,\n",
       "    'least_confident': \"::::::::: haha ok. There will an exclusive Beyonce vid from me at the end of June on my channel. it's 7:40 here, I need to get ready to hit the clubsss, going out at 9 lol. Yes we will, I love your page btw, makes this page look a bit shit lol. Byee\",\n",
       "    'least_confident_prob': 0.35579255},\n",
       "   'fp': {'oov_ratio_mean': 0.0,\n",
       "    'oov_ratio_std': 0.0,\n",
       "    'oov_ratio_min': 0.0,\n",
       "    'oov_ratio_max': 0.0,\n",
       "    'lens_mean': 81.31356736242884,\n",
       "    'lens_std': 155.72497367164598,\n",
       "    'lens_min': 1,\n",
       "    'lens_max': 1990,\n",
       "    'most_confident': 'fuck u ill do whatever I fucking want boy',\n",
       "    'most_confident_prob': 0.99378973,\n",
       "    'least_confident': 'you are fat thats right',\n",
       "    'least_confident_prob': 0.35490617},\n",
       "   'tn': {'oov_ratio_mean': 0.0,\n",
       "    'oov_ratio_std': 0.0,\n",
       "    'oov_ratio_min': 0.0,\n",
       "    'oov_ratio_max': 0.0,\n",
       "    'lens_mean': 82.45251723130339,\n",
       "    'lens_std': 120.85383991712281,\n",
       "    'lens_min': 1,\n",
       "    'lens_max': 3307,\n",
       "    'most_confident': \"::* I've made some additional copy edits and fixed a broken link in the External Links section. Looks good to me.    \\n\\n ===GAR Criteria Checklist=== \\n :GA review (see here for what the criteria are, and here for what they are not) \\n\\n #It is reasonably well written. \\n #:a (prose, no copyvios, spelling and grammar):  b (MoS for lead, layout, word choice, fiction, and lists):  \\n #::  \\n #It is factually accurate and verifiable. \\n #:a (reference section):  b (citations to reliable sources):  c (OR):  \\n #::  \\n #It is broad in its coverage. \\n #:a (major aspects):  b (focused):  \\n #::  \\n #It follows the neutral point of view policy. \\n #:Fair representation without bias:  \\n #::  \\n #It is stable. \\n #:No edit wars, etc.:  \\n #::  \\n #It is illustrated by images and other media, where possible and appropriate. \\n #:a (images are tagged and non-free content have fair use rationales):  b (appropriate use with suitable captions):  \\n #:: no images available for use \\n #Overall:  \\n #:Pass/Fail:  \\n #:: Meets all criteria for good article.\",\n",
       "    'most_confident_prob': 8.975755e-06,\n",
       "    'least_confident': \"I wish, this guys a dumbass. And besides Mighty B sucks so much, that if i tried to say 'Bessie' in the meaning of this show it comes out as a puking noise. So this just makes me mad, which isnt helping. The games already out and it STILL gets vandalised 28 1/2 times a day. They may aswell just delete the article.\",\n",
       "    'least_confident_prob': 0.35467157},\n",
       "   'fn': {'oov_ratio_mean': 0.0,\n",
       "    'oov_ratio_std': 0.0,\n",
       "    'oov_ratio_min': 0.0,\n",
       "    'oov_ratio_max': 0.0,\n",
       "    'lens_mean': 43.82513661202186,\n",
       "    'lens_std': 77.35548501651196,\n",
       "    'lens_min': 3,\n",
       "    'lens_max': 946,\n",
       "    'most_confident': \"Dammit this thing's addictive - Italic textAn unnamed, senior Blaupunkt executive (retd)\",\n",
       "    'most_confident_prob': 0.0010377696,\n",
       "    'least_confident': \"It's not Plagarism, just stop being a jerk-off about it.\",\n",
       "    'least_confident_prob': 0.35345015}}},\n",
       " 'threat': {'auc': 0.9647828881167418,\n",
       "  'f1': 0.1789473684210526,\n",
       "  'acc': 0.9926849854637532,\n",
       "  'tnr': 0.9918878364437775,\n",
       "  'fpr': 0.004814154865735097,\n",
       "  'fnr': 0.0025008596705117384,\n",
       "  'tpr': 0.0007971490199756166,\n",
       "  'precision': 0.14206128133704735,\n",
       "  'recall': 0.24170616113744076,\n",
       "  'quad_stats': {'tp': {'oov_ratio_mean': 0.0,\n",
       "    'oov_ratio_std': 0.0,\n",
       "    'oov_ratio_min': 0.0,\n",
       "    'oov_ratio_max': 0.0,\n",
       "    'lens_mean': 169.15686274509804,\n",
       "    'lens_std': 272.271661667204,\n",
       "    'lens_min': 7,\n",
       "    'lens_max': 1169,\n",
       "    'most_confident': 'KILL ALL THE NIGGGERS KILL ALL THE NIGGGERS KILL ALL THE NIGGGERS KILL ALL THE NIGGGERS KILL ALL THE NIGGGERS KILL ALL THE NIGGGERS KILL ALL THE NIGGGERS KILL ALL THE NIGGGERS KILL ALL THE NIGGGERS KILL ALL THE NIGGGERS KILL ALL THE NIGGGERS KILL ALL THE NIGGGERS KILL ALL THE NIGGGERS KILL ALL THE NIGGGERS KILL ALL THE NIGGGERS KILL ALL THE NIGGGERS KILL ALL THE NIGGGERS KILL ALL THE NIGGGERS KILL ALL THE NIGGGERS KILL ALL THE NIGGGERS KILL ALL THE NIGGGERS KILL ALL THE NIGGGERS KILL ALL THE NIGGGERS KILL ALL THE NIGGGERS KILL ALL THE NIGGGERS KILL ALL THE NIGGGERS KILL ALL THE NIGGGERS KILL ALL THE NIGGGERS KILL ALL THE NIGGGERS KILL ALL THE NIGGGERS KILL ALL THE NIGGGERS KILL ALL THE NIGGGERS KILL ALL THE NIGGGERS KILL ALL THE NIGGGERS KILL ALL THE NIGGGERS KILL ALL THE NIGGGERS KILL ALL THE NIGGGERS KILL ALL THE NIGGGERS KILL ALL THE NIGGGERS KILL ALL THE NIGGGERS KILL ALL THE NIGGGERS KILL ALL THE NIGGGERS KILL ALL THE NIGGGERS KILL ALL THE NIGGGERS KILL ALL THE NIGGGERS KILL ALL THE NIGGGERS KILL ALL THE NIGGGERS KILL ALL THE NIGGGERS KILL ALL THE NIGGGERS KILL ALL THE NIGGGERS KILL ALL THE NIGGGERS KILL ALL THE NIGGGERS KILL ALL THE NIGGGERS KILL ALL THE NIGGGERS KILL ALL THE NIGGGERS KILL ALL THE NIGGGERS KILL ALL THE NIGGGERS KILL ALL THE NIGGGERS KILL ALL THE NIGGGERS KILL ALL THE NIGGGERS KILL ALL THE NIGGGERS KILL ALL THE NIGGGERS KILL ALL THE NIGGGERS KILL ALL THE NIGGGERS KILL ALL THE NIGGGERS KILL ALL THE NIGGGERS KILL ALL THE NIGGGERS KILL ALL THE NIGGGERS KILL ALL THE NIGGGERS KILL ALL THE NIGGGERS KILL ALL THE NIGGGERS KILL ALL THE NIGGGERS KILL ALL THE NIGGGERS KILL ALL THE NIGGGERS KILL ALL THE NIGGGERS KILL ALL THE NIGGGERS KILL ALL THE NIGGGERS KILL ALL THE NIGGGERS KILL ALL THE NIGGGERS KILL ALL THE NIGGGERS KILL ALL THE NIGGGERS KILL ALL THE NIGGGERS KILL ALL THE NIGGGERS KILL ALL THE NIGGGERS KILL ALL THE NIGGGERS KILL ALL THE NIGGGERS KILL ALL THE NIGGGERS KILL ALL THE NIGGGERS KILL ALL THE NIGGGERS KILL ALL THE NIGGGERS KILL ALL THE NIGGGERS KILL ALL THE NIGGGERS KILL ALL THE NIGGGERS KILL ALL THE NIGGGERS KILL ALL THE NIGGGERS KILL ALL THE NIGGGERS KILL ALL THE NIGGGERS KILL ALL THE NIGGGERS KILL ALL THE NIGGGERS KILL ALL THE NIGGGERS KILL ALL THE NIGGGERS KILL ALL THE NIGGGERS KILL ALL THE NIGGGERS KILL ALL THE NIGGGERS KILL ALL THE NIGGGERS KILL ALL THE NIGGGERS KILL ALL THE NIGGGERS KILL ALL THE NIGGGERS KILL ALL THE NIGGGERS KILL ALL THE NIGGGERS KILL ALL THE NIGGGERS KILL ALL THE NIGGGERS KILL ALL THE NIGGGERS KILL ALL THE NIGGGERS KILL ALL THE NIGGGERS KILL ALL THE NIGGGERS KILL ALL THE NIGGGERS KILL ALL THE NIGGGERS KILL ALL THE NIGGGERS KILL ALL THE NIGGGERS KILL ALL THE NIGGGERS KILL ALL THE NIGGGERS KILL ALL THE NIGGGERS',\n",
       "    'most_confident_prob': 0.16638446,\n",
       "    'least_confident': '\"I hate your stupid guts, you shit-head! \"\"Praise be to Allah that I converted to Islam. It is only through him that we wake up and see the reality.\"\" What reality? Perhaps an alternate one! Now you listen here, you Benedict Arnold; if you can\\'t get your awful NPOV comments off of here, I will personally take a gun and blast you through your own damn computer screen! HA HA HA HA HA HAH AHAHAHAHHAHAHA! GET YOUR FRICKEN\\' ASS OFF OF WIKIPEDIA OR I WILL STICK YOUR HEAD IN A MICROWAVE! I KNOW WHO YOU ARE, \"\"ANONOYMOUS EDITOR\"\"! YOUR NAME IS OSAMA BIN LADEN, AND YOU ARE REMAINING ANONOMYOUS SO THAT YOU CAN ESCAPE THE JUST PUNISHMENT YOU DESERVE! AAAAAAAAAAARRRRRRRRRGGGGGGG!   \\n :\"',\n",
       "    'least_confident_prob': 0.05921409},\n",
       "   'fp': {'oov_ratio_mean': 0.0,\n",
       "    'oov_ratio_std': 0.0,\n",
       "    'oov_ratio_min': 0.0,\n",
       "    'oov_ratio_max': 0.0,\n",
       "    'lens_mean': 213.37987012987014,\n",
       "    'lens_std': 340.1133527394528,\n",
       "    'lens_min': 4,\n",
       "    'lens_max': 1990,\n",
       "    'most_confident': 'Nigger alert! I fucked a nigger last night! ;D \\n Nigger alert! I fucked a nigger last night! ;D \\n Nigger alert! I fucked a nigger last night! ;D \\n Nigger alert! I fucked a nigger last night! ;D \\n Nigger alert! I fucked a nigger last night! ;D \\n Nigger alert! I fucked a nigger last night! ;D \\n Nigger alert! I fucked a nigger last night! ;D \\n Nigger alert! I fucked a nigger last night! ;D \\n Nigger alert! I fucked a nigger last night! ;D \\n Nigger alert! I fucked a nigger last night! ;D \\n Nigger alert! I fucked a nigger last night! ;D \\n Nigger alert! I fucked a nigger last night! ;D \\n Nigger alert! I fucked a nigger last night! ;D \\n Nigger alert! I fucked a nigger last night! ;D \\n Nigger alert! I fucked a nigger last night! ;D \\n Nigger alert! I fucked a nigger last night! ;D \\n Nigger alert! I fucked a nigger last night! ;D \\n Nigger alert! I fucked a nigger last night! ;D \\n Nigger alert! I fucked a nigger last night! ;D \\n Nigger alert! I fucked a nigger last night! ;D \\n Nigger alert! I fucked a nigger last night! ;D \\n Nigger alert! I fucked a nigger last night! ;D \\n Nigger alert! I fucked a nigger last night! ;D \\n Nigger alert! I fucked a nigger last night! ;D \\n Nigger alert! I fucked a nigger last night! ;D \\n Nigger alert! I fucked a nigger last night! ;D \\n Nigger alert! I fucked a nigger last night! ;D \\n Nigger alert! I fucked a nigger last night! ;D \\n Nigger alert! I fucked a nigger last night! ;D \\n Nigger alert! I fucked a nigger last night! ;D \\n Nigger alert! I fucked a nigger last night! ;D \\n Nigger alert! I fucked a nigger last night! ;D \\n Nigger alert! I fucked a nigger last night! ;D \\n Nigger alert! I fucked a nigger last night! ;D \\n Nigger alert! I fucked a nigger last night! ;D \\n Nigger alert! I fucked a nigger last night! ;D \\n Nigger alert! I fucked a nigger last night! ;D \\n Nigger alert! I fucked a nigger last night! ;D \\n Nigger alert! I fucked a nigger last night! ;D \\n Nigger alert! I fucked a nigger last night! ;D \\n Nigger alert! I fucked a nigger last night! ;D \\n Nigger alert! I fucked a nigger last night! ;D \\n Nigger alert! I fucked a nigger last night! ;D \\n Nigger alert! I fucked a nigger last night! ;D \\n Nigger alert! I fucked a nigger last night! ;D \\n Nigger alert! I fucked a nigger last night! ;D \\n Nigger alert! I fucked a nigger last night! ;D \\n Nigger alert! I fucked a nigger last night! ;D \\n Nigger alert! I fucked a nigger last night! ;D \\n Nigger alert! I fucked a nigger last night! ;D \\n Nigger alert! I fucked a nigger last night! ;D \\n Nigger alert! I fucked a nigger last night! ;D \\n Nigger alert! I fucked a nigger last night! ;D \\n Nigger alert! I fucked a nigger last night! ;D \\n Nigger alert! I fucked a nigger last night! ;D \\n Nigger alert! I fucked a nigger last night! ;D \\n Nigger alert! I fucked a nigger last night! ;D \\n Nigger alert! I fucked a nigger last night! ;D \\n Nigger alert! I fucked a nigger last night! ;D \\n Nigger alert! I fucked a nigger last night! ;D \\n Nigger alert! I fucked a nigger last night! ;D \\n Nigger alert! I fucked a nigger last night! ;D \\n Nigger alert! I fucked a nigger last night! ;D \\n Nigger alert! I fucked a nigger last night! ;D \\n Nigger alert! I fucked a nigger last night! ;D \\n Nigger alert! I fucked a nigger last night! ;D \\n Nigger alert! I fucked a nigger last night! ;D \\n Nigger alert! I fucked a nigger last night! ;D \\n Nigger alert! I fucked a nigger last night! ;D \\n Nigger alert! I fucked a nigger last night! ;D \\n Nigger alert! I fucked a nigger last night! ;D \\n Nigger alert! I fucked a nigger last night! ;D \\n Nigger alert! I fucked a nigger last night! ;D \\n Nigger alert! I fucked a nigger last night! ;D \\n Nigger alert! I fucked a nigger last night! ;D \\n Nigger alert! I fucked a nigger last night! ;D \\n Nigger alert! I fucked a nigger last night! ;D \\n Nigger alert! I fucked a nigger last night! ;D \\n Nigger alert! I fucked a nigger last night! ;D \\n Nigger alert! I fucked a nigger last night! ;D \\n Nigger alert! I fucked a nigger last night! ;D \\n Nigger alert! I fucked a nigger last night! ;D \\n Nigger alert! I fucked a nigger last night! ;D \\n Nigger alert! I fucked a nigger last night! ;D \\n Nigger alert! I fucked a nigger last night! ;D \\n Nigger alert! I fucked a nigger last night! ;D \\n Nigger alert! I fucked a nigger last night! ;D \\n Nigger alert! I fucked a nigger last night! ;D \\n Nigger alert! I fucked a nigger last night! ;D \\n Nigger alert! I fucked a nigger last night! ;D \\n Nigger alert! I fucked a nigger last night! ;D \\n Nigger alert! I fucked a nigger last night! ;D \\n Nigger alert! I fucked a nigger last night! ;D \\n Nigger alert! I fucked a nigger last night! ;D \\n Nigger alert! I fu',\n",
       "    'most_confident_prob': 0.19735593,\n",
       "    'least_confident': \"fuck hidtory all u bitch's it is for geeks aand fags me id rather be out fucking a ruond than be inside studing being a fucking teacher pet wich every one fucking hates so if u are a teacher fuck u and if u are a kid fuck u if u dont like what i gotto say met up with me\",\n",
       "    'least_confident_prob': 0.059131704},\n",
       "   'tn': {'oov_ratio_mean': 0.0,\n",
       "    'oov_ratio_std': 0.0,\n",
       "    'oov_ratio_min': 0.0,\n",
       "    'oov_ratio_max': 0.0,\n",
       "    'lens_mean': 80.37394223041649,\n",
       "    'lens_std': 122.35188517588628,\n",
       "    'lens_min': 1,\n",
       "    'lens_max': 4950,\n",
       "    'most_confident': \"::* I've made some additional copy edits and fixed a broken link in the External Links section. Looks good to me.    \\n\\n ===GAR Criteria Checklist=== \\n :GA review (see here for what the criteria are, and here for what they are not) \\n\\n #It is reasonably well written. \\n #:a (prose, no copyvios, spelling and grammar):  b (MoS for lead, layout, word choice, fiction, and lists):  \\n #::  \\n #It is factually accurate and verifiable. \\n #:a (reference section):  b (citations to reliable sources):  c (OR):  \\n #::  \\n #It is broad in its coverage. \\n #:a (major aspects):  b (focused):  \\n #::  \\n #It follows the neutral point of view policy. \\n #:Fair representation without bias:  \\n #::  \\n #It is stable. \\n #:No edit wars, etc.:  \\n #::  \\n #It is illustrated by images and other media, where possible and appropriate. \\n #:a (images are tagged and non-free content have fair use rationales):  b (appropriate use with suitable captions):  \\n #:: no images available for use \\n #Overall:  \\n #:Pass/Fail:  \\n #:: Meets all criteria for good article.\",\n",
       "    'most_confident_prob': 5.7046915e-08,\n",
       "    'least_confident': '== YOU ALL GAY!!!!!!!!!!!!!!!!!!!!!!!111 == \\n\\n IM THE COOLE ST YUO AL L SUCK GO TO HELL!!!!!!!!!!!!!11',\n",
       "    'least_confident_prob': 0.059104566},\n",
       "   'fn': {'oov_ratio_mean': 0.0,\n",
       "    'oov_ratio_std': 0.0,\n",
       "    'oov_ratio_min': 0.0,\n",
       "    'oov_ratio_max': 0.0,\n",
       "    'lens_mean': 45.90625,\n",
       "    'lens_std': 112.22693509553532,\n",
       "    'lens_min': 2,\n",
       "    'lens_max': 946,\n",
       "    'most_confident': 'Bảng mạch chính là một bản mạch đóng vai trò là trung gian giao tiếp giữa các thiết bị với nhau. Một cách tổng quát, nó là mạch điện chính của một hệ thống hay thiết bị điện tử.',\n",
       "    'most_confident_prob': 0.000267617,\n",
       "    'least_confident': ', you stupid retard, or I will kill you!!!   ]]',\n",
       "    'least_confident_prob': 0.058396347}}},\n",
       " 'insult': {'auc': 0.974361699798606,\n",
       "  'f1': 0.6611243072050672,\n",
       "  'acc': 0.9598612022882866,\n",
       "  'tnr': 0.9207071180718372,\n",
       "  'fpr': 0.02572759386038951,\n",
       "  'fnr': 0.014411203851323892,\n",
       "  'tpr': 0.039154084216449406,\n",
       "  'precision': 0.6034690436039508,\n",
       "  'recall': 0.7309600233440326,\n",
       "  'quad_stats': {'tp': {'oov_ratio_mean': 0.0,\n",
       "    'oov_ratio_std': 0.0,\n",
       "    'oov_ratio_min': 0.0,\n",
       "    'oov_ratio_max': 0.0,\n",
       "    'lens_mean': 59.494211576846304,\n",
       "    'lens_std': 152.32910081962052,\n",
       "    'lens_min': 1,\n",
       "    'lens_max': 2321,\n",
       "    'most_confident': 'YOU SUCK YOU LYING SACK OF SHIT!!!!!!! DIE YOU BASTARD YOU ARE BRIANWASHING SON-OF-A BITCH!!!! \\n YOU SUCK YOU LYING SACK OF SHIT!!!!!!! DIE YOU BASTARD YOU ARE BRIANWASHING SON-OF-A BITCH!!!! \\n YOU SUCK YOU LYING SACK OF SHIT!!!!!!! DIE YOU BASTARD YOU ARE BRIANWASHING SON-OF-A BITCH!!!!YOU SUCK YOU LYING SACK OF SHIT!!!!!!! DIE YOU BASTARD YOU ARE BRIANWASHING SON-OF-A BITCH!!!! \\n YOU SUCK YOU LYING SACK OF SHIT!!!!!!! DIE YOU BASTARD YOU ARE BRIANWASHING SON-OF-A BITCH!!!! \\n YOU SUCK YOU LYING SACK OF SHIT!!!!!!! DIE YOU BASTARD YOU ARE BRIANWASHING SON-OF-A BITCH!!!!YOU SUCK YOU LYING SACK OF SHIT!!!!!!! DIE YOU BASTARD YOU ARE BRIANWASHING SON-OF-A BITCH!!!! \\n YOU SUCK YOU LYING SACK OF SHIT!!!!!!! DIE YOU BASTARD YOU ARE BRIANWASHING SON-OF-A BITCH!!!! \\n YOU SUCK YOU LYING SACK OF SHIT!!!!!!! DIE YOU BASTARD YOU ARE BRIANWASHING SON-OF-A BITCH!!!!YOU SUCK YOU LYING SACK OF SHIT!!!!!!! DIE YOU BASTARD YOU ARE BRIANWASHING SON-OF-A BITCH!!!! \\n YOU SUCK YOU LYING SACK OF SHIT!!!!!!! DIE YOU BASTARD YOU ARE BRIANWASHING SON-OF-A BITCH!!!! \\n YOU SUCK YOU LYING SACK OF SHIT!!!!!!! DIE YOU BASTARD YOU ARE BRIANWASHING SON-OF-A BITCH!!!!YOU SUCK YOU LYING SACK OF SHIT!!!!!!! DIE YOU BASTARD YOU ARE BRIANWASHING SON-OF-A BITCH!!!! \\n YOU SUCK YOU LYING SACK OF SHIT!!!!!!! DIE YOU BASTARD YOU ARE BRIANWASHING SON-OF-A BITCH!!!! \\n YOU SUCK YOU LYING SACK OF SHIT!!!!!!! DIE YOU BASTARD YOU ARE BRIANWASHING SON-OF-A BITCH!!!!YOU SUCK YOU LYING SACK OF SHIT!!!!!!! DIE YOU BASTARD YOU ARE BRIANWASHING SON-OF-A BITCH!!!! \\n YOU SUCK YOU LYING SACK OF SHIT!!!!!!! DIE YOU BASTARD YOU ARE BRIANWASHING SON-OF-A BITCH!!!! \\n YOU SUCK YOU LYING SACK OF SHIT!!!!!!! DIE YOU BASTARD YOU ARE BRIANWASHING SON-OF-A BITCH!!!!YOU SUCK YOU LYING SACK OF SHIT!!!!!!! DIE YOU BASTARD YOU ARE BRIANWASHING SON-OF-A BITCH!!!! \\n YOU SUCK YOU LYING SACK OF SHIT!!!!!!! DIE YOU BASTARD YOU ARE BRIANWASHING SON-OF-A BITCH!!!! \\n YOU SUCK YOU LYING SACK OF SHIT!!!!!!! DIE YOU BASTARD YOU ARE BRIANWASHING SON-OF-A BITCH!!!!YOU SUCK YOU LYING SACK OF SHIT!!!!!!! DIE YOU BASTARD YOU ARE BRIANWASHING SON-OF-A BITCH!!!! \\n YOU SUCK YOU LYING SACK OF SHIT!!!!!!! DIE YOU BASTARD YOU ARE BRIANWASHING SON-OF-A BITCH!!!! \\n YOU SUCK YOU LYING SACK OF SHIT!!!!!!! DIE YOU BASTARD YOU ARE BRIANWASHING SON-OF-A BITCH!!!!',\n",
       "    'most_confident_prob': 0.9686933,\n",
       "    'least_confident': 'someone should try and talk to these people who are trying to sell essays what a bunch of idiots',\n",
       "    'least_confident_prob': 0.36673814},\n",
       "   'fp': {'oov_ratio_mean': 0.0,\n",
       "    'oov_ratio_std': 0.0,\n",
       "    'oov_ratio_min': 0.0,\n",
       "    'oov_ratio_max': 0.0,\n",
       "    'lens_mean': 70.2411907654921,\n",
       "    'lens_std': 200.67026696373466,\n",
       "    'lens_min': 1,\n",
       "    'lens_max': 4950,\n",
       "    'most_confident': 'FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT FAGGOT',\n",
       "    'most_confident_prob': 0.95091456,\n",
       "    'least_confident': \":::: Oh wow!  I had no idea that's what you meant! Dam my mental retardation!!!  Seriously, don't be a troll.  I am here honestly presenting a big part of the story and you are calling me an idiot.  Wikipedia is ruined by people like you who are only here to protect their opinions by censoring anything they don't like.  If you seriously have any honest intentions, civilly discuss matters; don't resort to an ad hominem.\",\n",
       "    'least_confident_prob': 0.36674196},\n",
       "   'tn': {'oov_ratio_mean': 0.0,\n",
       "    'oov_ratio_std': 0.0,\n",
       "    'oov_ratio_min': 0.0,\n",
       "    'oov_ratio_max': 0.0,\n",
       "    'lens_mean': 82.78119005177828,\n",
       "    'lens_std': 121.28600704788879,\n",
       "    'lens_min': 1,\n",
       "    'lens_max': 3307,\n",
       "    'most_confident': \"::* I've made some additional copy edits and fixed a broken link in the External Links section. Looks good to me.    \\n\\n ===GAR Criteria Checklist=== \\n :GA review (see here for what the criteria are, and here for what they are not) \\n\\n #It is reasonably well written. \\n #:a (prose, no copyvios, spelling and grammar):  b (MoS for lead, layout, word choice, fiction, and lists):  \\n #::  \\n #It is factually accurate and verifiable. \\n #:a (reference section):  b (citations to reliable sources):  c (OR):  \\n #::  \\n #It is broad in its coverage. \\n #:a (major aspects):  b (focused):  \\n #::  \\n #It follows the neutral point of view policy. \\n #:Fair representation without bias:  \\n #::  \\n #It is stable. \\n #:No edit wars, etc.:  \\n #::  \\n #It is illustrated by images and other media, where possible and appropriate. \\n #:a (images are tagged and non-free content have fair use rationales):  b (appropriate use with suitable captions):  \\n #:: no images available for use \\n #Overall:  \\n #:Pass/Fail:  \\n #:: Meets all criteria for good article.\",\n",
       "    'most_confident_prob': 1.7292347e-06,\n",
       "    'least_confident': \"Munster ruggers think they are great, and they are. They have a piss-stink stadium, are forelock tuggers to the English and just monkey-shite to the French, and they still can't win the Heineken, no matter how much it's offered them. Come back to the sliothar, ye gowls.\",\n",
       "    'least_confident_prob': 0.3666117},\n",
       "   'fn': {'oov_ratio_mean': 0.0,\n",
       "    'oov_ratio_std': 0.0,\n",
       "    'oov_ratio_min': 0.0,\n",
       "    'oov_ratio_max': 0.0,\n",
       "    'lens_mean': 44.75813449023861,\n",
       "    'lens_std': 63.81791022851023,\n",
       "    'lens_min': 3,\n",
       "    'lens_max': 758,\n",
       "    'most_confident': ':Niggaz b niggaz b niggaz; why be difficult about this?',\n",
       "    'most_confident_prob': 0.0011373482,\n",
       "    'least_confident': 'NATHAN IS A GAY CRACKER',\n",
       "    'least_confident_prob': 0.36631328}}},\n",
       " 'identity_hate': {'auc': 0.9673781726663628,\n",
       "  'f1': 0.4447552447552448,\n",
       "  'acc': 0.9875894838850855,\n",
       "  'tnr': 0.9826190252899434,\n",
       "  'fpr': 0.006252149176279346,\n",
       "  'fnr': 0.006158366938635156,\n",
       "  'tpr': 0.00497045859514208,\n",
       "  'precision': 0.4428969359331476,\n",
       "  'recall': 0.44662921348314605,\n",
       "  'quad_stats': {'tp': {'oov_ratio_mean': 0.0,\n",
       "    'oov_ratio_std': 0.0,\n",
       "    'oov_ratio_min': 0.0,\n",
       "    'oov_ratio_max': 0.0,\n",
       "    'lens_mean': 92.4748427672956,\n",
       "    'lens_std': 201.6558911207838,\n",
       "    'lens_min': 3,\n",
       "    'lens_max': 1215,\n",
       "    'most_confident': 'GAY FAGS SPREAD AIDS AND FUCK LITTLE KIDS, STONE AND GAS THE FAGS. GAY FAGS FUCK THE WORLD AND GOD HATES FAGS. GAY FAGS SPREAD AIDS AND FUCK LITTLE KIDS, STONE AND GAS THE FAGS. GAY FAGS FUCK THE WORLD AND GOD HATES FAGS. GAY FAGS SPREAD AIDS AND FUCK LITTLE KIDS, STONE AND GAS THE FAGS. GAY FAGS FUCK THE WORLD AND GOD HATES FAGS. GAY FAGS SPREAD AIDS AND FUCK LITTLE KIDS, STONE AND GAS THE FAGS. GAY FAGS FUCK THE WORLD AND GOD HATES FAGS. GAY FAGS SPREAD AIDS AND FUCK LITTLE KIDS, STONE AND GAS THE FAGS. GAY FAGS FUCK THE WORLD AND GOD HATES FAGS. GAY FAGS SPREAD AIDS AND FUCK LITTLE KIDS, STONE AND GAS THE FAGS. GAY FAGS FUCK THE WORLD AND GOD HATES FAGS. GAY FAGS SPREAD AIDS AND FUCK LITTLE KIDS, STONE AND GAS THE FAGS. GAY FAGS FUCK THE WORLD AND GOD HATES FAGS. GAY FAGS SPREAD AIDS AND FUCK LITTLE KIDS, STONE AND GAS THE FAGS. GAY FAGS FUCK THE WORLD AND GOD HATES FAGS. GAY FAGS SPREAD AIDS AND FUCK LITTLE KIDS, STONE AND GAS THE FAGS. GAY FAGS FUCK THE WORLD AND GOD HATES FAGS. GAY FAGS SPREAD AIDS AND FUCK LITTLE KIDS, STONE AND GAS THE FAGS. GAY FAGS FUCK THE WORLD AND GOD HATES FAGS. GAY FAGS SPREAD AIDS AND FUCK LITTLE KIDS, STONE AND GAS THE FAGS. GAY FAGS FUCK THE WORLD AND GOD HATES FAGS. GAY FAGS SPREAD AIDS AND FUCK LITTLE KIDS, STONE AND GAS THE FAGS. GAY FAGS FUCK THE WORLD AND GOD HATES FAGS. GAY FAGS SPREAD AIDS AND FUCK LITTLE KIDS, STONE AND GAS THE FAGS. GAY FAGS FUCK THE WORLD AND GOD HATES FAGS. GAY FAGS SPREAD AIDS AND FUCK LITTLE KIDS, STONE AND GAS THE FAGS. GAY FAGS FUCK THE WORLD AND GOD HATES FAGS. GAY FAGS SPREAD AIDS AND FUCK LITTLE KIDS, STONE AND GAS THE FAGS. GAY FAGS FUCK THE WORLD AND GOD HATES FAGS. GAY FAGS SPREAD AIDS AND FUCK LITTLE KIDS, STONE AND GAS THE FAGS. GAY FAGS FUCK THE WORLD AND GOD HATES FAGS. GAY FAGS SPREAD AIDS AND FUCK LITTLE KIDS, STONE AND GAS THE FAGS. GAY FAGS FUCK THE WORLD AND GOD HATES FAGS. GAY FAGS SPREAD AIDS AND FUCK LITTLE KIDS, STONE AND GAS THE FAGS. GAY FAGS FUCK THE WORLD AND GOD HATES FAGS. GAY FAGS SPREAD AIDS AND FUCK LITTLE KIDS, STONE AND GAS THE FAGS. GAY FAGS FUCK THE WORLD AND GOD HATES FAGS. GAY FAGS SPREAD AIDS AND FUCK LITTLE KIDS, STONE AND GAS THE FAGS. GAY FAGS FUCK THE WORLD AND GOD HATES FAGS. GAY FAGS SPREAD AIDS AND FUCK LITTLE KIDS, STONE AND GAS THE FAGS. GAY FAGS FUCK THE WORLD AND GOD HATES FAGS. GAY FAGS SPREAD AIDS AND FUCK LITTLE KIDS, STONE AND GAS THE FAGS. GAY FAGS FUCK THE WORLD AND GOD HATES FAGS. GAY FAGS SPREAD AIDS AND FUCK LITTLE KIDS, STONE AND GAS THE FAGS. GAY FAGS FUCK THE WORLD AND GOD HATES FAGS. GAY FAGS SPREAD AIDS AND FUCK LITTLE KIDS, STONE AND GAS THE FAGS. GAY FAGS FUCK THE WORLD AND GOD HATES FAGS. GAY FAGS SPREAD AIDS AND FUCK LITTLE KIDS, STONE AND GAS THE FAGS. GAY FAGS FUCK THE WORLD AND GOD HATES FAGS. GAY FAGS SPREAD AIDS AND FUCK LITTLE KIDS, STONE AND GAS THE FAGS. GAY FAGS FUCK THE WORLD AND GOD HATES FAGS. GAY FAGS SPREAD AIDS AND FUCK LITTLE KIDS, STONE AND GAS THE FAGS. GAY FAGS FUCK THE WORLD AND GOD HATES FAGS. GAY FAGS SPREAD AIDS AND FUCK LITTLE KIDS, STONE AND GAS THE FAGS. GAY FAGS FUCK THE WORLD AND GOD HATES FAGS. GAY FAGS SPREAD AIDS AND FUCK LITTLE KIDS, STONE AND GAS THE FAGS. GAY FAGS FUCK THE WORLD AND GOD HATES FAGS. GAY FAGS SPREAD AIDS AND FUCK LITTLE KIDS, STONE AND GAS THE FAGS. GAY FAGS FUCK THE WORLD AND GOD HATES FAGS. GAY FAGS SPREAD AIDS AND FUCK LITTLE KIDS, STONE AND GAS THE FAGS. GAY FAGS FUCK THE WORLD AND GOD HATES FAGS. GAY FAGS SPREAD AIDS AND FUCK LITTLE KIDS, STONE AND GAS THE FAGS. GAY FAGS FUCK THE WORLD AND GOD HATES FAGS. GAY FAGS SPREAD AIDS AND FUCK LITTLE KIDS, STONE AND GAS THE FAGS. GAY FAGS FUCK THE WORLD AND GOD HATES FAGS. GAY FAGS SPREAD AIDS AND FUCK LITTLE KIDS, STONE AND GAS THE FAGS. GAY FAGS FUCK THE WORLD AND GOD HATES FAGS. GAY FAGS SPREAD AIDS AND FUCK LITTLE KIDS, STONE AND GAS THE FAGS. GAY FAGS FUCK THE WORLD AND GOD HATES FAGS. GAY FAGS SPREAD AIDS AND FUCK LITTLE KIDS, STONE AND GAS THE FAGS. GAY FAGS FUCK THE WORLD AND GOD HATES FAGS. GAY FAGS SPREAD AIDS AND FUCK LITTLE KIDS, STONE AND GAS THE FAGS. GAY FAGS FUCK THE WORLD AND GOD HATES FAGS. GAY FAGS SPREAD AIDS AND FUCK LITTLE KIDS, STONE AND GAS THE FAGS. GAY FAGS FUCK THE WORLD AND GOD HATES FAGS. GAY FAGS SPREAD AIDS AND FUCK LITTLE KIDS, STONE AND GAS THE FAGS. GAY FAGS FUCK THE WORLD AND GOD HATES FAGS. GAY FAGS SPREAD AIDS AND FUCK LITTLE KIDS, STONE AND GAS THE FAGS. GAY FAGS FUCK THE WORLD AND GOD HATES FAGS. GAY FAGS SPREAD AIDS AND FUCK LITTLE KIDS, STONE AND GAS THE FAGS. GAY FAGS FUCK THE WORLD AND GOD HATES FAGS. GAY FAGS SPREAD AIDS AND FUCK LITTLE KIDS, STONE AND GAS THE FAGS. GAY FAGS FUCK THE WORLD AND GOD HATES FAGS. GAY FAGS SPREAD AIDS AND FUCK LITTLE KIDS, STONE AND GAS THE FAGS. GAY FAGS FUCK THE WORLD AND GOD HATES FAGS. GAY FAGS SPREAD AIDS AND FUCK LITTLE KIDS, STONE AND GAS THE FAGS. GAY FAGS FUCK THE WORLD AND GOD HATES FAGS. GAY FAGS SPREAD AIDS AND FUCK LITTLE KIDS, STONE AND GAS THE FAGS. GAY FAGS FUCK THE WORLD AND GOD HATES FAGS',\n",
       "    'most_confident_prob': 0.9131504,\n",
       "    'least_confident': ':: Suck my dick cocksucker . This is a bullshit page and should be removed. death to all muslims.',\n",
       "    'least_confident_prob': 0.23433197},\n",
       "   'fp': {'oov_ratio_mean': 0.0,\n",
       "    'oov_ratio_std': 0.0,\n",
       "    'oov_ratio_min': 0.0,\n",
       "    'oov_ratio_max': 0.0,\n",
       "    'lens_mean': 109.3125,\n",
       "    'lens_std': 255.68954777962668,\n",
       "    'lens_min': 3,\n",
       "    'lens_max': 1990,\n",
       "    'most_confident': 'GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY GAY',\n",
       "    'most_confident_prob': 0.8568181,\n",
       "    'least_confident': \"Fuck off you stupid cunt.  Wikipedia is shit for retards who still live with their moms. \\n\\n ==Wikipedia is fucking shit== \\n Wikipedia, or 'fucking shit' as it is in English, is a fucking retarded website for retards. \\n\\n It started off as a shit website and then developed into a shit website.  Today it is a shit website full of shit and read by retards like  (a retard who is shit, lives with his mom and still wets the bed).\",\n",
       "    'least_confident_prob': 0.23099674},\n",
       "   'tn': {'oov_ratio_mean': 0.0,\n",
       "    'oov_ratio_std': 0.0,\n",
       "    'oov_ratio_min': 0.0,\n",
       "    'oov_ratio_max': 0.0,\n",
       "    'lens_mean': 81.0227945153183,\n",
       "    'lens_std': 123.30588833482861,\n",
       "    'lens_min': 1,\n",
       "    'lens_max': 4950,\n",
       "    'most_confident': \"::* I've made some additional copy edits and fixed a broken link in the External Links section. Looks good to me.    \\n\\n ===GAR Criteria Checklist=== \\n :GA review (see here for what the criteria are, and here for what they are not) \\n\\n #It is reasonably well written. \\n #:a (prose, no copyvios, spelling and grammar):  b (MoS for lead, layout, word choice, fiction, and lists):  \\n #::  \\n #It is factually accurate and verifiable. \\n #:a (reference section):  b (citations to reliable sources):  c (OR):  \\n #::  \\n #It is broad in its coverage. \\n #:a (major aspects):  b (focused):  \\n #::  \\n #It follows the neutral point of view policy. \\n #:Fair representation without bias:  \\n #::  \\n #It is stable. \\n #:No edit wars, etc.:  \\n #::  \\n #It is illustrated by images and other media, where possible and appropriate. \\n #:a (images are tagged and non-free content have fair use rationales):  b (appropriate use with suitable captions):  \\n #:: no images available for use \\n #Overall:  \\n #:Pass/Fail:  \\n #:: Meets all criteria for good article.\",\n",
       "    'most_confident_prob': 3.0789892e-07,\n",
       "    'least_confident': 'your my sex slave stupid',\n",
       "    'least_confident_prob': 0.23041262},\n",
       "   'fn': {'oov_ratio_mean': 0.0,\n",
       "    'oov_ratio_std': 0.0,\n",
       "    'oov_ratio_min': 0.0,\n",
       "    'oov_ratio_max': 0.0,\n",
       "    'lens_mean': 39.16751269035533,\n",
       "    'lens_std': 60.66256307466665,\n",
       "    'lens_min': 2,\n",
       "    'lens_max': 627,\n",
       "    'most_confident': '::mostly true (some blacks voted in every state)but that happened around 1900 and had no connection with CSA.',\n",
       "    'most_confident_prob': 0.00017796035,\n",
       "    'least_confident': 'To me, VKM and DX are equally funny. VKM does get the edge though when it comes to talking about how Vince is gay.',\n",
       "    'least_confident_prob': 0.22971746}}},\n",
       " 'global_auc': 0.9737865571942447,\n",
       " 'global_f1': 0.5095807449095242,\n",
       " 'global_acc': 0.9674263027915847,\n",
       " 'global_tnr': 0.9376686777746516,\n",
       " 'global_fpr': 0.02456313107630748,\n",
       " 'global_fnr': 0.008010566132107912,\n",
       " 'global_tpr': 0.029757625016932906,\n",
       " 'global_precision': 0.43705637645194106,\n",
       " 'global_recall': 0.63410454675791,\n",
       " 'train': ({'toxic': {'auc': 0.9870045369886017,\n",
       "    'f1': 0.8328309761187108,\n",
       "    'acc': 0.968371445939425,\n",
       "    'tnr': 0.8895852003183535,\n",
       "    'fpr': 0.014570316661548777,\n",
       "    'fnr': 0.01705823739902614,\n",
       "    'tpr': 0.0787862456210715,\n",
       "    'precision': 0.8439283077129623,\n",
       "    'recall': 0.8220217078592912},\n",
       "   'severe_toxic': {'auc': 0.9911468715616181,\n",
       "    'f1': 0.5352331606217616,\n",
       "    'acc': 0.988757355659863,\n",
       "    'tnr': 0.9822837483001298,\n",
       "    'fpr': 0.007720701129904557,\n",
       "    'fnr': 0.0035219432102324357,\n",
       "    'tpr': 0.006473607359733285,\n",
       "    'precision': 0.45607064017660043,\n",
       "    'recall': 0.6476489028213166},\n",
       "   'obscene': {'auc': 0.994414186508736,\n",
       "    'f1': 0.8463189650171143,\n",
       "    'acc': 0.9833992392101322,\n",
       "    'tnr': 0.9376891791114927,\n",
       "    'fpr': 0.00936260348058231,\n",
       "    'fnr': 0.007238157309285522,\n",
       "    'tpr': 0.045710060098639475,\n",
       "    'precision': 0.8299954483386436,\n",
       "    'recall': 0.8632974316487159},\n",
       "   'threat': {'auc': 0.9700072668740083,\n",
       "    'f1': 0.17150395778364116,\n",
       "    'acc': 0.9921288956013311,\n",
       "    'tnr': 0.9913142112288573,\n",
       "    'fpr': 0.0056902570015855015,\n",
       "    'fnr': 0.00218084739708343,\n",
       "    'tpr': 0.0008146843724736951,\n",
       "    'precision': 0.1252408477842004,\n",
       "    'recall': 0.2719665271966527},\n",
       "   'insult': {'auc': 0.988820758120063,\n",
       "    'f1': 0.7593582887700534,\n",
       "    'acc': 0.9751834606538782,\n",
       "    'tnr': 0.9360284763522194,\n",
       "    'fpr': 0.01460791747873987,\n",
       "    'fnr': 0.010208621867381917,\n",
       "    'tpr': 0.03915498430165882,\n",
       "    'precision': 0.7282900104907332,\n",
       "    'recall': 0.7931953789513775},\n",
       "   'identity_hate': {'auc': 0.9878920601595071,\n",
       "    'f1': 0.5425017277125087,\n",
       "    'acc': 0.9917027530064987,\n",
       "    'tnr': 0.9867833127573306,\n",
       "    'fpr': 0.004411829217088318,\n",
       "    'fnr': 0.003885417776413007,\n",
       "    'tpr': 0.004919440249168082,\n",
       "    'precision': 0.5271994627266622,\n",
       "    'recall': 0.5587188612099644}},)}"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'toxic': {'auc': 0.9870045369886017,\n",
       "   'f1': 0.8328309761187108,\n",
       "   'acc': 0.968371445939425,\n",
       "   'tnr': 0.8895852003183535,\n",
       "   'fpr': 0.014570316661548777,\n",
       "   'fnr': 0.01705823739902614,\n",
       "   'tpr': 0.0787862456210715,\n",
       "   'precision': 0.8439283077129623,\n",
       "   'recall': 0.8220217078592912},\n",
       "  'severe_toxic': {'auc': 0.9911468715616181,\n",
       "   'f1': 0.5352331606217616,\n",
       "   'acc': 0.988757355659863,\n",
       "   'tnr': 0.9822837483001298,\n",
       "   'fpr': 0.007720701129904557,\n",
       "   'fnr': 0.0035219432102324357,\n",
       "   'tpr': 0.006473607359733285,\n",
       "   'precision': 0.45607064017660043,\n",
       "   'recall': 0.6476489028213166},\n",
       "  'obscene': {'auc': 0.994414186508736,\n",
       "   'f1': 0.8463189650171143,\n",
       "   'acc': 0.9833992392101322,\n",
       "   'tnr': 0.9376891791114927,\n",
       "   'fpr': 0.00936260348058231,\n",
       "   'fnr': 0.007238157309285522,\n",
       "   'tpr': 0.045710060098639475,\n",
       "   'precision': 0.8299954483386436,\n",
       "   'recall': 0.8632974316487159},\n",
       "  'threat': {'auc': 0.9700072668740083,\n",
       "   'f1': 0.17150395778364116,\n",
       "   'acc': 0.9921288956013311,\n",
       "   'tnr': 0.9913142112288573,\n",
       "   'fpr': 0.0056902570015855015,\n",
       "   'fnr': 0.00218084739708343,\n",
       "   'tpr': 0.0008146843724736951,\n",
       "   'precision': 0.1252408477842004,\n",
       "   'recall': 0.2719665271966527},\n",
       "  'insult': {'auc': 0.988820758120063,\n",
       "   'f1': 0.7593582887700534,\n",
       "   'acc': 0.9751834606538782,\n",
       "   'tnr': 0.9360284763522194,\n",
       "   'fpr': 0.01460791747873987,\n",
       "   'fnr': 0.010208621867381917,\n",
       "   'tpr': 0.03915498430165882,\n",
       "   'precision': 0.7282900104907332,\n",
       "   'recall': 0.7931953789513775},\n",
       "  'identity_hate': {'auc': 0.9878920601595071,\n",
       "   'f1': 0.5425017277125087,\n",
       "   'acc': 0.9917027530064987,\n",
       "   'tnr': 0.9867833127573306,\n",
       "   'fpr': 0.004411829217088318,\n",
       "   'fnr': 0.003885417776413007,\n",
       "   'tpr': 0.004919440249168082,\n",
       "   'precision': 0.5271994627266622,\n",
       "   'recall': 0.5587188612099644}},)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_metrics[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Record results and save weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.environ[\"IS_COLAB\"] != \"True\":\n",
    "    import sys\n",
    "    sys.path.append(\"../lib\")\n",
    "    from record_experiments import record, find\n",
    "else:\n",
    "    PASSWORD = \"mongo11747\" # FILL IN IF COLAB\n",
    "\n",
    "    from typing import *\n",
    "    import pymongo\n",
    "    from bson.objectid import ObjectId\n",
    "    import os\n",
    "    import logging\n",
    "\n",
    "    # Logger\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logger.setLevel(logging.INFO)\n",
    "    ch = logging.StreamHandler()\n",
    "    ch.setLevel(logging.DEBUG)\n",
    "    formatter = logging.Formatter('[%(levelname)s] %(asctime)s - %(name)s %(message)s')\n",
    "    ch.setFormatter(formatter)\n",
    "    logger.addHandler(ch)\n",
    "\n",
    "    conn_str = f\"mongodb+srv://root:{PASSWORD}@cluster0-ptgoc.mongodb.net/test?retryWrites=true\"\n",
    "\n",
    "    client = pymongo.MongoClient(conn_str)\n",
    "    db = client.experiments\n",
    "    collection = db.logs\n",
    "\n",
    "    def _cln(v: Any) -> Any:\n",
    "        \"\"\"Ensure variables are serializable\"\"\"\n",
    "        if isinstance(v, (np.float, np.float16, np.float32, np.float64, np.float128)):\n",
    "            return float(v)\n",
    "        elif isinstance(v, (np.int, np.int0, np.int8, np.int16, np.int32, np.int64)):\n",
    "            return int(v)\n",
    "        elif isinstance(v, dict):\n",
    "            return {k: _cln(v_) for k, v_ in v.items()}\n",
    "        else:\n",
    "            return v\n",
    "\n",
    "    def record(log: dict):\n",
    "        res = collection.insert_one({str(k): _cln(v) for k, v in log.items()})\n",
    "        logger.info(f\"Inserted results at id {res.inserted_id}\")\n",
    "        return res\n",
    "\n",
    "    def find(id_: Optional[str]=None, query: Optional[dict]=None):\n",
    "        if query is None: query = {\"_id\": ObjectId(id_)}\n",
    "        res = collection.find_one(query)\n",
    "        return res\n",
    "\n",
    "    def delete(id_: Optional[str]=None, query: Optional[dict]=None):\n",
    "        if query is None: query = {\"_id\": ObjectId(id_)}\n",
    "        res = collection.delete_many(query)\n",
    "        logger.info(f\"Deleted {res.deleted_count} entries\")\n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Record summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from pytz import timezone\n",
    "\n",
    "if not config.testing:\n",
    "    experiment_log = dict(config)\n",
    "    tz = timezone('EST')\n",
    "    experiment_log[\"execution_date\"] = datetime.now(tz).strftime(\"%Y-%m-%d %H:%M %Z\")\n",
    "    experiment_log.update(metrics)\n",
    "    experiment_log.update(label_metrics)\n",
    "    res = record(experiment_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(DATA_ROOT / f\"test_basic_aug_preds_{res.inserted_id}.npy\", test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(DATA_ROOT / f\"thres_basic_aug_preds_{res.inserted_id}.npy\", thres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find('5cc0abfd145ce50c21fec6d2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
