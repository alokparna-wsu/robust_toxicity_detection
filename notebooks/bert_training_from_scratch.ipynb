{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings for seamlessly running on colab\n",
    "import os\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive')\n",
    "    os.environ[\"IS_COLAB\"] = \"True\"\n",
    "except ImportError:\n",
    "    os.environ[\"IS_COLAB\"] = \"False\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "if [ \"$IS_COLAB\" = \"True\" ]; then\n",
    "    pip install git+https://github.com/facebookresearch/fastText.git\n",
    "    pip install torch\n",
    "    pip install torchvision\n",
    "    pip install --upgrade git+https://github.com/keitakurita/allennlp@develop\n",
    "    pip install dnspython\n",
    "    pip install jupyter_slack\n",
    "    pip install git+https://github.com/keitakurita/Better_LSTM_PyTorch.git\n",
    "    if [ -d \"apex\" ]; then\n",
    "      git clone https://github.com/NVIDIA/apex.git\n",
    "    fi\n",
    "    cd apex && python setup.py install --cpp_ext --cuda_ext\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import *\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from overrides import overrides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from contextlib import contextmanager\n",
    "\n",
    "class Config(dict):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        for k, v in kwargs.items():\n",
    "            setattr(self, k, v)\n",
    "    \n",
    "    def set(self, key, val):\n",
    "        self[key] = val\n",
    "        setattr(self, key, val)\n",
    "\n",
    "@contextmanager\n",
    "def timer(name):\n",
    "    t0 = time.time()\n",
    "    yield\n",
    "    print(f'[{name}] done in {time.time() - t0:.0f} s')\n",
    "    \n",
    "import functools\n",
    "import traceback\n",
    "import sys\n",
    "\n",
    "def get_ref_free_exc_info():\n",
    "    \"Free traceback from references to locals/globals to avoid circular reference leading to gc.collect() unable to reclaim memory\"\n",
    "    type, val, tb = sys.exc_info()\n",
    "    traceback.clear_frames(tb)\n",
    "    return (type, val, tb)\n",
    "\n",
    "def gpu_mem_restore(func):\n",
    "    \"Reclaim GPU RAM if CUDA out of memory happened, or execution was interrupted\"\n",
    "    @functools.wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        try:\n",
    "            return func(*args, **kwargs)\n",
    "        except:\n",
    "            type, val, tb = get_ref_free_exc_info() # must!\n",
    "            raise type(val).with_traceback(tb) from None\n",
    "    return wrapper\n",
    "\n",
    "def ifnone(a: Any, alt: Any): return alt if a is None else a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for papermill\n",
    "testing = True\n",
    "debugging = False\n",
    "seed = 1\n",
    "char_encoder = \"subword\"\n",
    "computational_batch_size = 16\n",
    "batch_size = 64\n",
    "loss = \"is\"\n",
    "num_neg_samples = 1280\n",
    "lr = 1e-4\n",
    "lr_schedule = \"slanted_triangular\"\n",
    "epochs = 1 if not testing else 1\n",
    "hidden_sz = 128 if not testing else 32\n",
    "num_attention_heads = 4\n",
    "num_hidden_layers = 4\n",
    "dataset = \"jigsaw\"\n",
    "softmax = \"cnn_softmax\"\n",
    "n_classes = 6\n",
    "max_seq_len = 128\n",
    "download_data = False\n",
    "ft_model_path = \"../data/jigsaw/ft_model.txt\"\n",
    "max_vocab_size = 300000\n",
    "dropouti = 0.2\n",
    "dropoutw = 0.0\n",
    "dropoute = 0.2\n",
    "dropoutr = 0.3 # TODO: Implement\n",
    "val_ratio = 0.0\n",
    "use_augmented = False\n",
    "freeze_embeddings = True\n",
    "mixup_ratio = 0.0\n",
    "discrete_mixup_ratio = 0.0\n",
    "attention_bias = True\n",
    "weight_decay = 0.\n",
    "bias_init = True\n",
    "neg_splits = 1\n",
    "num_layers = 2\n",
    "rnn_type = \"lstm\"\n",
    "pooling_type = \"augmented_multipool\" # attention or multipool or augmented_multipool\n",
    "model_type = \"standard\"\n",
    "use_word_level_features = True\n",
    "use_sentence_level_features = True\n",
    "bucket = True\n",
    "compute_thres_on_test = True\n",
    "find_lr = False\n",
    "permute_sentences = False\n",
    "run_id = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Can we make this play better with papermill?\n",
    "config = Config(\n",
    "    testing=testing,\n",
    "    debugging=debugging,\n",
    "    seed=seed,\n",
    "    char_encoder=char_encoder,\n",
    "    computational_batch_size=computational_batch_size,\n",
    "    batch_size=batch_size,\n",
    "    loss=loss,\n",
    "    num_neg_samples=num_neg_samples,\n",
    "    lr=lr,\n",
    "    lr_schedule=lr_schedule,\n",
    "    epochs=epochs,\n",
    "    hidden_sz=hidden_sz,\n",
    "    num_attention_heads=num_attention_heads,\n",
    "    num_hidden_layers=num_hidden_layers,\n",
    "    dataset=dataset,\n",
    "    softmax=softmax,\n",
    "    n_classes=n_classes,\n",
    "    max_seq_len=max_seq_len, # necessary to limit memory usage\n",
    "    ft_model_path=ft_model_path,\n",
    "    max_vocab_size=max_vocab_size,\n",
    "    dropouti=dropouti,\n",
    "    dropoutw=dropoutw,\n",
    "    dropoute=dropoute,\n",
    "    dropoutr=dropoutr,\n",
    "    val_ratio=val_ratio,\n",
    "    use_augmented=use_augmented,\n",
    "    freeze_embeddings=freeze_embeddings,\n",
    "    attention_bias=attention_bias,\n",
    "    weight_decay=weight_decay,\n",
    "    bias_init=bias_init,\n",
    "    neg_splits=neg_splits,\n",
    "    num_layers=num_layers,\n",
    "    rnn_type=rnn_type,\n",
    "    pooling_type=pooling_type,\n",
    "    model_type=model_type,\n",
    "    use_word_level_features=use_word_level_features,\n",
    "    use_sentence_level_features=use_sentence_level_features,\n",
    "    mixup_ratio=mixup_ratio,\n",
    "    discrete_mixup_ratio=discrete_mixup_ratio,\n",
    "    bucket=bucket,\n",
    "    compute_thres_on_test=compute_thres_on_test,\n",
    "    permute_sentences=permute_sentences,\n",
    "    find_lr=find_lr,\n",
    "    run_id=run_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = TypeVar(\"T\")\n",
    "TensorDict = Dict[str, Union[torch.Tensor, Dict[str, torch.Tensor]]]  # pylint: disable=invalid-name\n",
    "\n",
    "from allennlp.data import Instance\n",
    "from allennlp.data.token_indexers import TokenIndexer\n",
    "from allennlp.data.tokenizers import Token\n",
    "from allennlp.nn import util as nn_util\n",
    "from allennlp.data.dataset_readers import DatasetReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.environ[\"IS_COLAB\"] != \"True\":\n",
    "    DATA_ROOT = Path(\"../data\") / config.dataset\n",
    "else:\n",
    "    DATA_ROOT = Path(\"./gdrive/My Drive/Colab_Workspace/Colab Notebooks/data\") / config.dataset\n",
    "    config.ft_model_path = str(DATA_ROOT / \"ft_model.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "if download_data:\n",
    "    if config.val_ratio > 0.0:\n",
    "        fnames = [\"train_wo_val.csv\", \"test_proced.csv\", \"val.csv\", \"ft_model.txt\"]\n",
    "    else:\n",
    "        fnames = [\"train.csv\", \"test_proced.csv\", \"ft_model.txt\"]\n",
    "    if config.use_augmented or config.discrete_mixup_ratio > 0.0: fnames.append(\"train_extra.csv\")\n",
    "    for fname in fnames:\n",
    "        if not (DATA_ROOT / fname).exists():\n",
    "            print(subprocess.Popen([f\"aws s3 cp s3://nnfornlp/raw_data/jigsaw/{fname} {str(DATA_ROOT)}\"],\n",
    "                                   shell=True, stdout=subprocess.PIPE).stdout.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.data.fields import (TextField, SequenceLabelField, LabelField, \n",
    "                                  MetadataField, ArrayField)\n",
    "\n",
    "class MemoryOptimizedTextField(TextField):\n",
    "    @overrides\n",
    "    def __init__(self, tokens: List[str], token_indexers: Dict[str, TokenIndexer]) -> None:\n",
    "        self.tokens = tokens\n",
    "        self._token_indexers = token_indexers\n",
    "        self._indexed_tokens: Optional[Dict[str, TokenList]] = None\n",
    "        self._indexer_name_to_indexed_token: Optional[Dict[str, List[str]]] = None\n",
    "        # skip checks for tokens\n",
    "    @overrides\n",
    "    def index(self, vocab):\n",
    "        super().index(vocab)\n",
    "        self.tokens = None # empty tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JigsawLMDatasetReader(DatasetReader):\n",
    "    def __init__(self, tokenizer: Callable[[str], List[str]]=lambda x: x.split(),\n",
    "                 token_indexers: Dict[str, TokenIndexer]=None, # TODO: Handle mapping from BERT\n",
    "                 output_token_indexers: Dict[str, TokenIndexer]=None,\n",
    "                 max_seq_len: Optional[int]=config.max_seq_len) -> None:\n",
    "        super().__init__(lazy=False)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.token_indexers = token_indexers\n",
    "        self.output_token_indexers = output_token_indexers or token_indexers\n",
    "        self.max_seq_len = max_seq_len\n",
    "        \n",
    "    def _clean(self, x: str) -> str:\n",
    "        \"\"\"\n",
    "        Maps a word to its desired output. Will leave as identity for now.\n",
    "        In the future, will change to denoising operation.\n",
    "        \"\"\"\n",
    "        return x\n",
    "\n",
    "    @overrides\n",
    "    def text_to_instance(self, tokens: List[str]) -> Instance:\n",
    "        sentence_field = MemoryOptimizedTextField(\n",
    "            [x for x in tokens],\n",
    "            self.token_indexers)\n",
    "        fields = {\"input\": sentence_field}\n",
    "        output_sentence_field = MemoryOptimizedTextField(\n",
    "            [self._clean(x) for x in tokens],\n",
    "            self.output_token_indexers)\n",
    "        fields[\"output\"] = output_sentence_field\n",
    "        return Instance(fields)\n",
    "    \n",
    "    @overrides\n",
    "    def _read(self, file_path: str) -> Iterator[Instance]:\n",
    "        df = pd.read_csv(file_path)\n",
    "        if config.testing: df = df.head(1000)\n",
    "        for i, row in df.iterrows():\n",
    "            yield self.text_to_instance(\n",
    "                self.tokenizer(row[\"comment_text\"]),\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.data.tokenizers.word_splitter import SpacyWordSplitter\n",
    "from allennlp.data.token_indexers.elmo_indexer import ELMoCharacterMapper, ELMoTokenCharactersIndexer\n",
    "from allennlp.data.token_indexers import SingleIdTokenIndexer\n",
    "\n",
    "if config.char_encoder == \"cnn\":\n",
    "    token_indexer = ELMoTokenCharactersIndexer()\n",
    "else:\n",
    "    token_indexer = SingleIdTokenIndexer(\n",
    "        lowercase_tokens=config.softmax != \"cnn_softmax\") # Temporary\n",
    "\n",
    "_spacy_tok = SpacyWordSplitter(language='en_core_web_sm', pos_tags=False).split_words\n",
    "\n",
    "def tokenizer(x: str):\n",
    "        return [\"[CLS]\"] + [w.text for w in\n",
    "                _spacy_tok(x)[:config.max_seq_len - 2]] + [\"[SEP]\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not isinstance(token_indexer, SingleIdTokenIndexer):\n",
    "    output_token_indexer = SingleIdTokenIndexer(lowercase_tokens=True) # lowercase for now, we will need to \n",
    "else:\n",
    "    output_token_indexer = token_indexer\n",
    "    \n",
    "reader = JigsawLMDatasetReader(\n",
    "    tokenizer=tokenizer,\n",
    "    token_indexers={\"tokens\": token_indexer},\n",
    "    output_token_indexers={\"words\": output_token_indexer}\n",
    ")\n",
    "train_ds, val_ds, test_ds = (reader.read(DATA_ROOT / fname) for fname in [\"train_wo_val.csv\",\n",
    "                                                                          \"val.csv\",\n",
    "                                                                          \"test_proced.csv\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars(train_ds[2].fields[\"input\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars(train_ds[2].fields[\"output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.data.vocabulary import Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_ds = train_ds + test_ds + val_ds\n",
    "vocab = Vocabulary.from_instances(full_ds, tokens_to_add={\"tokens\": [\"[MASK]\"]},\n",
    "                                  max_vocab_size=config.max_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.get_vocab_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.set(\"vocab_sz\", vocab.get_vocab_size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Implement fast sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.loss == \"is\":\n",
    "    freqs = np.zeros(vocab.get_vocab_size())\n",
    "    for w, c in vocab._retained_counter[\"tokens\"].items():\n",
    "        freqs[vocab.get_token_index(w)] = c\n",
    "    freqs /= freqs.sum()\n",
    "    freqs **= 2 / 3\n",
    "    # renormalize\n",
    "    freqs /= freqs.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.data.iterators import BucketIterator, DataIterator, BasicIterator, MemoryOptimizedIterator\n",
    "if config.bucket:\n",
    "    iterator = BucketIterator(\n",
    "            batch_size=config.computational_batch_size, \n",
    "            biggest_batch_first=config.testing,\n",
    "            sorting_keys=[(\"input\", \"num_tokens\")],\n",
    "            max_instances_in_memory=config.batch_size * 2,\n",
    "    )\n",
    "else:\n",
    "    iterator = MemoryOptimizedIterator(\n",
    "        batch_size=config.computational_batch_size,\n",
    "        max_instances_in_memory=config.batch_size * 2,\n",
    "    )\n",
    "iterator.index_with(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iterator(train_ds))[\"input\"][\"tokens\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iterator(train_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build word to indices mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fnv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Output this constructed dictionary to disk and load in the future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fnv_hash(w):\n",
    "    return fnv.hash(w.encode(\"utf-8\"), bits=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_char_ngrams(w, range_=range(2, 6)):\n",
    "    w = \"<\" + w + \">\"\n",
    "    for r in range_: # prioritize smaller n-grams\n",
    "        for i, c in enumerate(w):\n",
    "            if i + r <= len(w): yield w[i:i+r]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.char_encoder == \"cnn\":\n",
    "    from tqdm import tqdm\n",
    "    # TODO: Speed up\n",
    "    # TODO: Debug\n",
    "    # See allennlp/data/token_indexers/elmo_indexer.py\n",
    "    with timer(\"Building character indexes\"):\n",
    "        word_id_to_char_idxs = np.zeros((config.vocab_sz, 50))\n",
    "        for w, idx in tqdm(vocab.get_token_to_index_vocabulary().items()):\n",
    "            # TODO: Check for start/end of word symbols\n",
    "            if idx > 0: \n",
    "                word_id_to_char_idxs[idx, :] = 261\n",
    "                word_id_to_char_idxs[idx, 0] = 259\n",
    "                for i, c in enumerate(w.encode(\"utf-8\")):\n",
    "                    if i + 1 == 48: break\n",
    "                    word_id_to_char_idxs[idx, i+1] = int(c) + 1\n",
    "                word_id_to_char_idxs[idx, i+2] = 260\n",
    "        word_id_to_char_idxs = np.array(word_id_to_char_idxs)\n",
    "\n",
    "    word_id_to_char_idxs = torch.LongTensor(word_id_to_char_idxs)\n",
    "elif config.char_encoder == \"subword\":\n",
    "    config.set(\"num_buckets\", 50000) # TODO: add to configurable parameters\n",
    "    offset = vocab.get_vocab_size()\n",
    "    with timer(\"Building word to subword indices mapping\"):\n",
    "        subword_ids = [[] for _ in vocab.get_token_to_index_vocabulary()]\n",
    "        for word, idx in vocab.get_token_to_index_vocabulary().items():\n",
    "            if idx < 2 or word == \"[MASK]\":\n",
    "                subword_ids[idx] = [idx]\n",
    "            else:\n",
    "                subword_ids[idx] = [idx] + list([fnv_hash(x) % config.num_buckets + offset\n",
    "                                                 for x in generate_char_ngrams(word)])\n",
    "        maxlen = int(np.percentile([len(a) for a in subword_ids], 99)) # 99th-percentile\n",
    "        word_id_to_subword_ids = torch.zeros(len(subword_ids), maxlen, dtype=torch.long).to(device)\n",
    "        for i, idxs in enumerate(subword_ids):\n",
    "            for j, id_ in enumerate(idxs):\n",
    "                if j >= maxlen: break\n",
    "                word_id_to_subword_ids[i, j] = id_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.char_encoder == \"cnn\":\n",
    "    mask_char_ids = torch.ones(50, dtype=torch.int64).to(device) * 261\n",
    "    for i, c in enumerate(\"[MASK]\".encode(\"utf-8\")):\n",
    "        mask_char_ids[i+1] = int(c) + 1\n",
    "    mask_char_ids[i+2] = 260"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bert configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_pretrained_bert.modeling import (BertConfig, BertForMaskedLM, \n",
    "                                              BertEncoder, BertPooler, BertOnlyMLMHead)\n",
    "\n",
    "bert_config = BertConfig(\n",
    "        config.max_vocab_size, \n",
    "        hidden_size=config.hidden_sz, \n",
    "        num_attention_heads=config.num_attention_heads,\n",
    "        num_hidden_layers=config.num_hidden_layers, \n",
    "        intermediate_size=config.hidden_sz * config.num_attention_heads,\n",
    "        max_position_embeddings=config.max_seq_len,\n",
    ")\n",
    "\n",
    "bert_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building token embedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.char_encoder == \"cnn\":\n",
    "    from allennlp.modules.token_embedders.elmo_token_embedder import ElmoTokenEmbedder\n",
    "    from allennlp.modules.elmo import _ElmoCharacterEncoder\n",
    "\n",
    "    options_file = 'https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x1024_128_2048cnn_1xhighway/elmo_2x1024_128_2048cnn_1xhighway_options.json'\n",
    "    weight_file = 'https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x1024_128_2048cnn_1xhighway/elmo_2x1024_128_2048cnn_1xhighway_weights.hdf5'\n",
    "    _inner_char_encoder = _ElmoCharacterEncoder(\n",
    "        options_file=options_file, \n",
    "        weight_file=weight_file,\n",
    "        requires_grad=True\n",
    "    )\n",
    "    class ElmoEncoder(nn.Module):\n",
    "        def __init__(self, _inner):\n",
    "            super().__init__()\n",
    "            self._inner = _inner\n",
    "        def forward(self, *args):\n",
    "            # TODO: Stop Elmo encoder from adding SoS and EoS tokens\n",
    "            return self._inner(*args)[\"token_embedding\"][:, 1:-1, :]\n",
    "        def get_output_dim(self):\n",
    "            return self._inner.get_output_dim()\n",
    "    char_encoder = ElmoEncoder(_inner_char_encoder)\n",
    "\n",
    "# char_encoder\n",
    "\n",
    "# sample_idxs = next(iterator(train_ds))[\"tokens\"][\"tokens\"]\n",
    "\n",
    "# char_encoder(sample_idxs)\n",
    "    config.set(\"embedding_sz\", char_encoder.get_output_dim())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag of char-ngrams using fastText (too much memory consumption for now...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.modules.sparse import EmbeddingBag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.char_encoder == \"subword\":\n",
    "    from torch.nn.modules.sparse import EmbeddingBag\n",
    "\n",
    "    class SubwordEncoder(nn.Module):\n",
    "        def __init__(self, num_embeddings, embedding_sz):\n",
    "            super().__init__()\n",
    "            self._subword_encoding = word_id_to_subword_ids\n",
    "            self.bag = EmbeddingBag(num_embeddings, embedding_sz, mode=\"sum\")\n",
    "            self.n_subwords_per_word = self._subword_encoding.size(1)\n",
    "\n",
    "        def forward(self, \n",
    "                    tsr: torch.LongTensor, # (batch, seq) or # (batch)\n",
    "                   ) -> torch.FloatTensor: # (batch, seq, feat) or # (batch, feat)\n",
    "            # TODO: can I use offsets in a differentiable manner??\n",
    "            if len(tsr.shape) > 1:\n",
    "                bs, seq = tsr.size(0), tsr.size(1)\n",
    "                subword_ids = self._subword_encoding[tsr]\n",
    "                bag_of_embs = self.bag(subword_ids.view((-1, self.n_subwords_per_word)) # need to convert to 2D\n",
    "                                      ).view((bs, seq, -1)) # reshape to 3d\n",
    "            else:\n",
    "                subword_ids = self._subword_encoding[tsr]\n",
    "                bag_of_embs = self.bag(subword_ids)\n",
    "            norm_factor = (subword_ids > 0).float().sum(dim=-1, keepdim=True) + 0.1 # add a bit of smoothings\n",
    "            return bag_of_embs / norm_factor\n",
    "        \n",
    "    char_encoder = SubwordEncoder(vocab.get_vocab_size() + config.num_buckets, 300)\n",
    "    config.set(\"embedding_sz\", 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple word-level embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.char_encoder == \"fasttext\":\n",
    "    from allennlp.modules import Embedding\n",
    "\n",
    "    ft_matrix = np.random.randn(bert_config.vocab_size, 300) * 0.3\n",
    "    char_encoder = Embedding(bert_config.vocab_size, 300, weight=torch.FloatTensor(ft_matrix))\n",
    "    config.set(\"embedding_sz\", 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze(x):\n",
    "    x.requires_grad = False\n",
    "    if hasattr(x, \"parameters\"):\n",
    "        for p in x.parameters: freeze(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, hidden_size, eps=1e-12):\n",
    "        \"\"\"Construct a layernorm module in the TF style (epsilon inside the square root).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(hidden_size))\n",
    "        self.bias = nn.Parameter(torch.zeros(hidden_size))\n",
    "        self.variance_epsilon = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        u = x.mean(-1, keepdim=True)\n",
    "        s = (x - u).pow(2).mean(-1, keepdim=True)\n",
    "        x = (x - u) / torch.sqrt(s + self.variance_epsilon)\n",
    "        return self.weight * x + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingWPositionEmbs(nn.Module):\n",
    "    \"\"\"Embeds, then maps the embeddings to the bert_hidden_sz for processing\"\"\"\n",
    "    def __init__(self, word_emb: nn.Module, \n",
    "                 embedding_dim,\n",
    "                 bert_hidden_sz, \n",
    "                 freeze_embeddings=False,\n",
    "                 dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.word_emb = word_emb\n",
    "        if freeze_embeddings: freeze(self.word_emb)\n",
    "        self.position_embeddings = nn.Embedding(config.max_seq_len, \n",
    "                                                bert_hidden_sz)\n",
    "        self.linear = nn.Linear(embedding_dim, bert_hidden_sz,\n",
    "                                bias=False) # Transform dimensions\n",
    "        self.norm = LayerNorm(bert_hidden_sz)\n",
    "        self.do = nn.Dropout(0.1)\n",
    "    \n",
    "    def get_word_embs(self, input_ids):\n",
    "        return self.linear(self.word_emb(input_ids))\n",
    "    \n",
    "    def forward(self, input_ids):\n",
    "        # We won't be using token types since we won't be predicting the next sentence\n",
    "        bs, seq_length, *_ = input_ids.shape\n",
    "        position_ids = (torch.arange(seq_length, dtype=torch.long)\n",
    "                             .to(input_ids.device)\n",
    "                             .unsqueeze(0)\n",
    "                             .expand((bs, seq_length)))\n",
    "        word_embs = self.get_word_embs(input_ids)\n",
    "        position_embs = self.position_embeddings(position_ids)\n",
    "        return self.do(self.norm(word_embs + position_embs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_embs = EmbeddingWPositionEmbs(\n",
    "    char_encoder,\n",
    "    config.embedding_sz,\n",
    "    bert_config.hidden_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available(): sample_embs.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Masked Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_encoder = BertEncoder(bert_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomBert(nn.Module):\n",
    "    def __init__(self, embeddings, encoder):\n",
    "        super().__init__()\n",
    "        self.embeddings = embeddings\n",
    "        self.encoder = encoder\n",
    "        \n",
    "    def forward(self, input_ids, \n",
    "                token_type_ids=None, \n",
    "                attention_mask=None):\n",
    "        if attention_mask is None:\n",
    "            if len(input_ids.shape) > 2:\n",
    "                attention_mask = torch.ones_like(input_ids[:, :, 0])\n",
    "            else:\n",
    "                attention_mask = torch.ones_like(input_ids)\n",
    "        if token_type_ids is None:\n",
    "            if len(input_ids.shape) > 2:\n",
    "                token_type_ids = torch.ones_like(input_ids[:, :, 0])\n",
    "            else:\n",
    "                token_type_ids = torch.ones_like(input_ids)\n",
    "        \n",
    "        # We create a 3D attention mask from a 2D tensor mask.\n",
    "        # Sizes are [batch_size, 1, 1, to_seq_length]\n",
    "        # So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]\n",
    "        # this attention mask is more simple than the triangular masking of causal attention\n",
    "        # used in OpenAI GPT, we just need to prepare the broadcast dimension here.\n",
    "        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "        # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n",
    "        # masked positions, this operation will create a tensor which is 0.0 for\n",
    "        # positions we want to attend and -10000.0 for masked positions.\n",
    "        # Since we are adding it to the raw scores before the softmax, this is\n",
    "        # effectively the same as removing these entirely.\n",
    "        extended_attention_mask = extended_attention_mask.to(dtype=next(self.parameters()).dtype) # fp16 compatibility\n",
    "        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
    "\n",
    "        embedding_output = self.embeddings(input_ids)\n",
    "        encoded_layers = self.encoder(embedding_output,\n",
    "                                      extended_attention_mask,\n",
    "                                      output_all_encoded_layers=True)\n",
    "        return encoded_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertMLMPooler(nn.Module):\n",
    "    def forward(self, x: List[torch.FloatTensor]) -> torch.FloatTensor:\n",
    "        return x[-1] # return final layer only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model = CustomBert(sample_embs, bert_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def gelu(x):\n",
    "    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))\n",
    "\n",
    "class BertCustomLMPredictionHead(nn.Module):\n",
    "    def __init__(self, config, out_sz, vocab_sz, \n",
    "                 embedding, output_logits=True):\n",
    "        super().__init__()\n",
    "        # Projections\n",
    "        self.dense = nn.Linear(config.hidden_size, out_sz)\n",
    "        self.transform_act_fn = gelu\n",
    "        self.LayerNorm = LayerNorm(out_sz)\n",
    "        \n",
    "        # Predictions\n",
    "        self.output_logits = output_logits\n",
    "        if self.output_logits:\n",
    "            self.decoder = nn.Linear(out_sz, vocab_sz, \n",
    "                                     bias=False)\n",
    "            if embedding is not None:\n",
    "                self.decoder.weight = embedding\n",
    "            self.bias = nn.Parameter(torch.zeros(vocab_sz))\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        if self.output_logits:\n",
    "            hidden_states = self.dense(hidden_states)\n",
    "            hidden_states = self.transform_act_fn(hidden_states)\n",
    "            preds = self.LayerNorm(hidden_states)\n",
    "            preds = self.decoder(preds) + self.bias\n",
    "        else:\n",
    "            preds = hidden_states\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_logits = config.loss != \"is\"\n",
    "bert_mlm_head = BertCustomLMPredictionHead(config=bert_config, \n",
    "                                           out_sz=config.embedding_sz, \n",
    "                                           vocab_sz=config.vocab_sz,\n",
    "                                           embedding=(sample_embs.word_emb.weight \n",
    "                                                      if config.char_encoder == \"fasttext\" \n",
    "                                                      else None),\n",
    "                                           output_logits=output_logits,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_model = nn.Sequential(\n",
    "    bert_model,\n",
    "    BertMLMPooler(),\n",
    "    bert_mlm_head,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ElmoDecoder(nn.Module):\n",
    "    \"\"\"TODO: Add word correction\"\"\"\n",
    "    def __init__(self, enc: nn.Module, dec: nn.Linear, word_correction_dim: int=0):\n",
    "        super().__init__()\n",
    "        self._enc = enc\n",
    "        self._dec = dec\n",
    "        self.word_correction_dim = word_correction_dim\n",
    "        if word_correction_dim > 0:\n",
    "            self.word_correction = nn.Embedding(config.max_vocab_size,\n",
    "                                                word_correction_dim)\n",
    "            self.back_projection = nn.Linear(word_correction_dim, self._dec.out_features, \n",
    "                                             bias=False)\n",
    "        \n",
    "    def forward(self, idxs):\n",
    "        if len(idxs.shape) == 1: idxs = idxs.unsqueeze(0)\n",
    "        char_idxs = word_id_to_char_idxs[idxs]\n",
    "        output = self._dec(self._enc(char_idxs)).squeeze(0) \n",
    "        if self.word_correction_dim > 0:\n",
    "            corr = self.back_projection(self.word_correction(idxs.squeeze(0)))\n",
    "            return output + corr\n",
    "        else: return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.char_encoder == \"fasttext\":\n",
    "    output_embs = sample_embs.get_word_embs\n",
    "elif config.char_encoder == \"cnn\" and config.softmax == \"cnn_softmax\" and config.loss == \"is\":\n",
    "    _inner_char_decoder = _ElmoCharacterEncoder(\n",
    "            options_file=options_file, \n",
    "            weight_file=weight_file,\n",
    "            requires_grad=True\n",
    "    )\n",
    "    char_decoder = ElmoEncoder(_inner_char_decoder)\n",
    "    # share just the linear transformation with \n",
    "    output_embs = ElmoDecoder(char_decoder, sample_embs.linear)\n",
    "    if torch.cuda.is_available(): output_embs.cuda()\n",
    "elif config.char_encoder == \"subword\":\n",
    "    output_embs = sample_embs.get_word_embs # tie input and output weights\n",
    "else:\n",
    "    from allennlp.modules import Embedding\n",
    "    mtrx = None\n",
    "    output_embs = Embedding(config.vocab_sz, bert_config.hidden_size, weight=mtrx)\n",
    "    if torch.cuda.is_available(): output_embs.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordCorrection(nn.Module):\n",
    "    \"\"\"From the paper `Exploring the Limitations of Language Modeling`\"\"\"\n",
    "    def __init__(self, hidden_sz: int, bottleneck_sz: int=128):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Linear(hidden_sz, bottleneck_sz)\n",
    "        \n",
    "    def forward(self, h: torch.FloatTensor, \n",
    "                corr: torch.FloatTensor):\n",
    "        x = self.l1(h)\n",
    "        return x @ corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Masked Cross Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedCrossEntropyLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self._loss = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "\n",
    "    def forward(self, preds, tgts, mask=None) -> torch.tensor:\n",
    "        if mask is None:\n",
    "            return self._loss(preds, tgts).mean()\n",
    "        else:\n",
    "            # Is this reshaping really necessary? Seems like there would be a more elegant solution\n",
    "            loss = self._loss(preds.view((-1, preds.size(-1))),\n",
    "                              tgts.view((-1, )))\n",
    "            n_elements = mask.sum()\n",
    "            return (loss * mask.view((-1, )).float()).sum() / n_elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importance Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UniformSampler:\n",
    "    def __init__(self, min_, max_):\n",
    "        self.min_, self.max_ = min_, max_\n",
    "    \n",
    "    def sample(self, shape):\n",
    "        return torch.randint(low=self.min_,\n",
    "                             high=self.max_, size=shape)\n",
    "\n",
    "class UnigramSampler:\n",
    "    def __init__(self, probs):\n",
    "        self.probs = probs\n",
    "    @staticmethod\n",
    "    def _prod(x):\n",
    "        acc = 1\n",
    "        for a in x: acc *= a\n",
    "        return acc\n",
    "    def sample(self, shape):\n",
    "        return torch.multinomial(self.probs, self._prod(shape), replacement=True).view(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImportanceSamplingLoss(nn.Module):\n",
    "    def __init__(self, embedding_generator,\n",
    "                 probs: np.ndarray, k=config.num_neg_samples):\n",
    "        super().__init__()\n",
    "        self.embedding_generator = embedding_generator\n",
    "        # TODO: Should this be according to the unigram probability?\n",
    "        # Or should it be uniform?\n",
    "        self.sampler = UnigramSampler(probs=torch.FloatTensor(probs))\n",
    "        # TODO: Compute samples in advance\n",
    "        self._loss_func = MaskedCrossEntropyLoss()\n",
    "        self.k = k\n",
    "    \n",
    "    def get_negative_samples(self) -> torch.LongTensor:\n",
    "        neg = self.sampler.sample((self.k, )) # TODO: Speed up??\n",
    "        return neg\n",
    "    \n",
    "    def get_embeddings(self, idxs: torch.LongTensor) -> torch.FloatTensor:\n",
    "        \"\"\"Converts indexes into vectors\"\"\"\n",
    "        return self.embedding_generator(idxs) # TODO: Implement general case\n",
    "    \n",
    "    def forward(self, y: torch.LongTensor, tgt, mask=None):\n",
    "        \"\"\"\n",
    "        Expects input of shape\n",
    "        y: (batch * seq, feature_sz)\n",
    "        tgt: (batch * seq, )\n",
    "        \"\"\"\n",
    "        if len(y.shape) > 2:            \n",
    "            y = y.view((-1, y.size(-1))) # (batch * seq, emb_sz)\n",
    "            tgt = tgt.view((-1, )) # (batch * seq, s)\n",
    "        bs, emb_sz = y.size(0), y.size(1)\n",
    "        pos_embeddings = self.get_embeddings(tgt) # (bs, emb_sz)\n",
    "        # share negative samples across the batch\n",
    "        neg_samples = (self.get_negative_samples()\n",
    "                       .to(y.device)) # (k, )\n",
    "        neg_embeddings = self.get_embeddings(neg_samples) # (k, emb_sz)\n",
    "        embs = torch.cat([\n",
    "            pos_embeddings.unsqueeze(1), # (bs, 1, emb_sz)\n",
    "            neg_embeddings.unsqueeze(0).expand(bs, self.k, emb_sz) # (bs, k, emb_sz)\n",
    "        ], dim=1) # (bs, k+1, emb_sz)\n",
    "        dot_prods = torch.einsum(\"bkf,bf->bk\", embs, y)\n",
    "        return self._loss_func(dot_prods, torch.zeros(bs, dtype=torch.int64).to(y.device),\n",
    "                               mask=mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_sz = bert_config.hidden_size\n",
    "y = torch.randn((3, 7, emb_sz)).view((-1, emb_sz)).to(device)\n",
    "tgt = torch.randint(100, (3, 7)).view((-1, )).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_id_to_subword_ids[tgt].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = ImportanceSamplingLoss(\n",
    "    output_embs, k=10, probs=torch.rand(100),\n",
    ")\n",
    "loss(y, tgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Masker(nn.Module):\n",
    "    def __init__(self, vocab: Vocabulary, \n",
    "                 noise_rate: float=0.15,\n",
    "                 mask_rate=0.8,\n",
    "                 replace_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.vocab = vocab\n",
    "        self.vocab_sz = vocab.get_vocab_size()\n",
    "        if config.char_encoder == \"cnn\":\n",
    "            self.mask_id = mask_char_ids.unsqueeze(0).unsqueeze(1)\n",
    "        else:\n",
    "            self.mask_id = vocab.get_token_index(\"[MASK]\")\n",
    "        self.noise_rate = noise_rate\n",
    "        self.mask_rate = mask_rate\n",
    "        self.replace_rate = replace_rate\n",
    "        \n",
    "    def create_mask(self, shape, ones_ratio, dtype=torch.uint8):\n",
    "        return (torch.ones(shape, dtype=dtype, \n",
    "                           requires_grad=False)\n",
    "                     .bernoulli(ones_ratio))\n",
    "\n",
    "    def get_random_input_ids(self, shape):\n",
    "        \"\"\"Returns randomly sampled \"\"\"\n",
    "        if config.char_encoder == \"cnn\":\n",
    "            rint = torch.randint(self.vocab_sz, shape)\n",
    "            return word_id_to_char_idxs[rint]\n",
    "        elif config.char_encoder == \"fasttext\" or config.char_encoder == \"subword\":\n",
    "            return torch.randint(self.vocab_sz, shape)\n",
    "\n",
    "    def forward(self, x: torch.LongTensor) -> torch.LongTensor:\n",
    "        char_level = len(x.shape) > 2 # using character-level features, but mask at word-level\n",
    "        if self.noise_rate > 0:\n",
    "            with torch.no_grad(): # no grads required here\n",
    "                mask_shape = x.shape[:-1] if char_level else x.shape\n",
    "                mask = self.create_mask(mask_shape, \n",
    "                                        self.noise_rate * self.mask_rate).to(x.device)\n",
    "                if config.char_encoder == \"cnn\":\n",
    "                    x = torch.where(mask.unsqueeze(2), self.mask_id, x)                \n",
    "                else:\n",
    "                    x = x.masked_fill(mask, self.mask_id)\n",
    "                \n",
    "                if self.replace_rate > 0.:\n",
    "                    # this is techinically incorrect, since we might overwrite the mask tokens\n",
    "                    # but I guess it will do for now\n",
    "                    mask = self.create_mask(mask_shape,\n",
    "                                            self.noise_rate * self.replace_rate).to(x.device)\n",
    "                    x = torch.where(mask.unsqueeze(2) if config.char_encoder == \"cnn\" else mask, \n",
    "                                    self.get_random_input_ids(mask_shape).to(x.device),\n",
    "                                    x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.nn.util import get_text_field_mask\n",
    "from allennlp.training.metrics import CategoricalAccuracy\n",
    "\n",
    "class MaskedLM(Model):\n",
    "    def __init__(self, vocab: Vocabulary, model: nn.Module,\n",
    "                loss: nn.Module, noise_rate=0.8):\n",
    "        super().__init__(vocab)\n",
    "        self.masker = Masker(vocab, noise_rate=noise_rate)\n",
    "        self.accuracy = CategoricalAccuracy()\n",
    "        self.model = model\n",
    "        self.loss = loss\n",
    "    \n",
    "    @property\n",
    "    def outputs_logits(self) -> bool:\n",
    "        return self.model[-1].output_logits\n",
    "    \n",
    "    def forward(self, input: TensorDict, \n",
    "                output: TensorDict, **kwargs) -> TensorDict:\n",
    "        mask = get_text_field_mask(input)\n",
    "        x = self.masker(input[\"tokens\"])\n",
    "        tgt = output[\"words\"]\n",
    "        \n",
    "        logits = self.model(x)\n",
    "        out_dict = {\"loss\": self.loss(logits, tgt, mask=mask)}\n",
    "        out_dict[\"logits\"] = logits\n",
    "        if self.outputs_logits:\n",
    "            out_dict[\"accuracy\"] = self.accuracy(logits, tgt)\n",
    "        return out_dict\n",
    "\n",
    "    def get_metrics(self, reset: bool = False) -> Dict[str, float]:\n",
    "        if self.outputs_logits:\n",
    "            return {\"accuracy\": self.accuracy.get_metric(reset)}\n",
    "        else:\n",
    "            return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.loss == \"masked_crossentropy\":\n",
    "    loss = MaskedCrossEntropyLoss()\n",
    "elif config.loss == \"crossentropy\":\n",
    "    _loss = nn.CrossEntropyLoss()\n",
    "    def ce(y, t, mask=None): \n",
    "        return _loss(y.view((-1, y.size(-1))), t.view((-1, )))\n",
    "    loss = ce\n",
    "elif config.loss == \"is\":\n",
    "    # TODO: Implement masking\n",
    "    loss = ImportanceSamplingLoss(output_embs, freqs)\n",
    "else:\n",
    "    raise ValueError(\"AAAAAAAAAAAAA\")\n",
    "masked_lm = MaskedLM(vocab, custom_model, loss, noise_rate=0.15)\n",
    "if torch.cuda.is_available(): masked_lm.cuda() # Is this different from to(device)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.testing:\n",
    "    batch = nn_util.move_to_device(batch, 0 if torch.cuda.is_available() else -1)\n",
    "\n",
    "    tokens = masked_lm.masker(batch[\"input\"][\"tokens\"])\n",
    "\n",
    "    hidden_states = masked_lm.model[:2](tokens)\n",
    "\n",
    "    masked_lm.model[2](hidden_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if config.testing: print(masked_lm(**batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.training import Callback\n",
    "import gc\n",
    "\n",
    "class GCCallback(Callback):\n",
    "    \"\"\"Calls gc periodically to prevent memory errors\"\"\"\n",
    "    def __init__(self, period: int=1):\n",
    "        self._period = period\n",
    "        \n",
    "    def on_batch_end(self, data):\n",
    "        if (data[\"batches_this_epoch\"] + 1) % self._period == 0:\n",
    "            gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.training import Trainer, TrainerWithCallbacks\n",
    "\n",
    "optimizer = torch.optim.Adam(masked_lm.parameters(), lr=config.lr)\n",
    "\n",
    "trainer = TrainerWithCallbacks(\n",
    "    model=masked_lm,\n",
    "    optimizer=optimizer,\n",
    "    iterator=iterator,\n",
    "    train_dataset=train_ds,\n",
    "    validation_dataset=val_ds,\n",
    "    gradient_accumulation_steps=config.batch_size // config.computational_batch_size,\n",
    "    serialization_dir=DATA_ROOT / \"bert_ckpts_cnn_softmax\" if not config.testing else None,\n",
    "    cuda_device=0 if torch.cuda.is_available() else -1,\n",
    "    num_epochs=config.epochs,\n",
    "    callbacks=[GCCallback()],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.commands.find_learning_rate import search_learning_rate, _save_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.testing:\n",
    "    pass\n",
    "#     lrs_, losses_ = search_learning_rate(trainer, num_batches=300)\n",
    "\n",
    "#     n = -100\n",
    "#     plt.ylabel(\"loss\")\n",
    "#     plt.xlabel('learning rate (log10 scale)')\n",
    "#     plt.xscale('log')\n",
    "#     plt.plot(lrs_[:n], losses_[:n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = nn_util.move_to_device(batch, 0 if torch.cuda.is_available() else -1)\n",
    "masked_lm(**batch)[\"logits\"].argmax(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For IS loss, we need to aggregate the embeddings at the end of training to make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "vocab_sz = vocab.get_vocab_size()\n",
    "embedding_sz = bert_config.hidden_size\n",
    "\n",
    "if config.loss == \"is\":\n",
    "    bs = 16\n",
    "    output_embedding_matrix = torch.zeros(vocab_sz, embedding_sz)\n",
    "    num_batches = math.ceil(vocab_sz / bs)\n",
    "    for i in range(num_batches):\n",
    "        start,end = i*bs, min(((i+1)*bs), vocab_sz)\n",
    "        idxs = torch.arange(start=start, end=end).unsqueeze(0)\n",
    "        output_embedding_matrix[start:end, :] = loss.get_embeddings(idxs.to(device)).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manually Check Outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Implement manual checks for negative sampling loss as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_np(t): return t.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_words(arr):\n",
    "    if len(arr.shape) > 1:\n",
    "        return [to_words(a) for a in arr]\n",
    "    else:\n",
    "        arr = to_np(arr)\n",
    "        return \" \".join([vocab.get_token_from_index(i) for i in arr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preds(model, batch: TensorDict):\n",
    "    if config.loss == \"is\":\n",
    "        logits = model(**batch)[\"logits\"].cpu()\n",
    "        return (logits @ output_embedding_matrix.transpose(0, 1)).argmax(2)\n",
    "    else:\n",
    "        return model(**batch)[\"logits\"].argmax(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_lm(**batch)[\"logits\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cstm_pprint(x):\n",
    "    print(\"\\n\\n\".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cstm_pprint(to_words(batch[\"output\"][\"words\"])[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cstm_pprint(to_words(get_preds(masked_lm, batch)[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.debugging:\n",
    "    for i in range(3):\n",
    "        trainer = Trainer(\n",
    "            model=masked_lm,\n",
    "            optimizer=optimizer,\n",
    "            iterator=iterator,\n",
    "            train_dataset=train_ds,\n",
    "            validation_dataset=val_ds,\n",
    "            serialization_dir=None,\n",
    "            cuda_device=0 if torch.cuda.is_available() else -1,\n",
    "            num_epochs=1,\n",
    "        )\n",
    "        trainer.train()\n",
    "        cstm_pprint(to_words(get_preds(masked_lm, batch))[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
