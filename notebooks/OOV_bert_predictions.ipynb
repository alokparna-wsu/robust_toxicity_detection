{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from pytorch_pretrained_bert.tokenization import BertTokenizer\n",
    "from pytorch_pretrained_bert.modeling import BertForMaskedLM\n",
    "\n",
    "torch_device=torch.device('cuda')\n",
    "\n",
    "bert_model_mlm = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "bert_model_mlm.eval()\n",
    "bert_model_mlm.to(torch_device)\n",
    "\n",
    "for param in bert_model_mlm.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "bert_id2tok = dict()\n",
    "for tok, tok_id in bert_tokenizer.vocab.items():\n",
    "    bert_id2tok[tok_id] = tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters\n",
    "\n",
    "MAX_BERT_LEN=256\n",
    "MAX_COSINE_DIST=0.3\n",
    "BERT_VOCAB_QTY=30000\n",
    "\n",
    "num_threads=8\n",
    "K=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "ft_compiled_path = \"../data/jigsaw/ft_model_bert_basic_tok.npy\" # Embeddings generated from the vocabulary\n",
    "fasttext_embeds = np.load(ft_compiled_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_vocab_path = \"../data/jigsaw/data_vocab.bin\"\n",
    "#vocab=pickle.load(open(data_vocab_path,'rb'))\n",
    "\n",
    "from allennlp.data.vocabulary import Vocabulary\n",
    "vocab = Vocabulary.from_files(\"../data/jigsaw/data_ft_vocab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.data.tokenizers.word_splitter import SpacyWordSplitter\n",
    "from pytorch_pretrained_bert.tokenization import BasicTokenizer\n",
    "from allennlp.data.token_indexers import WordpieceIndexer, SingleIdTokenIndexer\n",
    "import re\n",
    "\n",
    "#_spacy_tok = SpacyWordSplitter(language='en_core_web_sm', pos_tags=False).split_words\n",
    "_bert_tok = BasicTokenizer(do_lower_case=True)\n",
    "\n",
    "spacy_tokenizer = SpacyWordSplitter(language='en_core_web_sm', pos_tags=False)\n",
    "\n",
    "from allennlp.data.token_indexers import SingleIdTokenIndexer\n",
    "token_indexer = SingleIdTokenIndexer(\n",
    "    lowercase_tokens=True,\n",
    ")\n",
    "\n",
    "from itertools import groupby\n",
    "\n",
    "def remove_url(s):\n",
    "    return re.sub(r\"http\\S+\", \"\", s)\n",
    "\n",
    "def remove_extra_chars(s, max_qty=2):\n",
    "    res = [c * min(max_qty, len(list(group_iter))) for c, group_iter in groupby(s)] \n",
    "    return ''.join(res)\n",
    "\n",
    "def tokenizer(x: str):\n",
    "    return [remove_extra_chars(w) for w in _bert_tok.tokenize(remove_url(x))]\n",
    "    #return [w.text for w in _spacy_tok(x.lower())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"n't\" in bert_tokenizer.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(bert_tokenizer.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_id2tok[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer.vocab['sh']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns arrays of arrays if there's an OOV word or an empty array instead\n",
    "# Each array element is a tuple: \n",
    "# position of OOV word (with respect to the original tokenizer), sent for BERT tokenizer\n",
    "def get_bert_masked_inputs(toks, bert_tokenizer):\n",
    "    res = []\n",
    "    \n",
    "    oov_pos = []\n",
    "    bert_vocab = bert_tokenizer.vocab\n",
    "    \n",
    "    for i in range(len(toks)):\n",
    "        if toks[i] not in bert_vocab:\n",
    "            oov_pos.append(i)\n",
    "            \n",
    "\n",
    "    for pos in oov_pos:\n",
    "        res.append( (pos, '[CLS] %s [MASK] %s [SEP]' % \n",
    "                     (' '.join(toks[0:pos]), ' '.join(toks[pos+1:])) ) )\n",
    "        \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "#from spacy.vocab import Vocab\n",
    "#from spacy.language import Language\n",
    "#nlp = Language(Vocab())\n",
    "#from spacy.lang.en import English\n",
    "#nlp = English()\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "from spacy.tokenizer import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_bert_masked_inputs(tokenizer('This is a *strangge* sentence.'), bert_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toks = bert_tokenizer.tokenize('[CLS] detete what the [MASK] are you doing here ? [SEP]')\n",
    "toks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toks = spacy_tokenizer.split_words(\"[CLS] what the [MASK] are you don't here ? [SEP]\")\n",
    "toks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[', 'CLS', ']', 'what', 'the', '[', 'MASK', ']', 'are', 'you', 'do', \"n't\", 'here', 'sh#t', 'fcuk', '?', '[', 'SEP', ']']\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"[CLS] what the [MASK] are you don't here sh#t fcuk? [SEP]\")\n",
    "print([token.text for token in doc])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"uck\" in nlp.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57852"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer(\"don't  couldn't can't you're I'm sheeeet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['don',\n",
       " \"'\",\n",
       " 't',\n",
       " 'couldn',\n",
       " \"'\",\n",
       " 't',\n",
       " 'can',\n",
       " \"'\",\n",
       " 't',\n",
       " 'you',\n",
       " \"'\",\n",
       " 're',\n",
       " 'i',\n",
       " \"'\",\n",
       " 'm',\n",
       " 'fc',\n",
       " '##uk']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_tokenizer.tokenize(\"don't  couldn't can't you're I'm fcuk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer(\"You ' re right. It ' s a miracle! You'd been deceived!\") # 've', 're', 's', 'd', 'll'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "# pos_oov is OOV index with respect to the original (not BERT) tokenizer!!!\n",
    "UtterData = namedtuple('SentData', ['batch_sent_id', 'pos_oov', 'tok_ids', 'oov_token'])\n",
    "\n",
    "def get_batch_data(torch_device, tokenizer, bert_tokenizer, sent_list, max_len=MAX_BERT_LEN):\n",
    "    \n",
    "    batch_data_raw = []\n",
    "    batch_max_seq_qty = 0\n",
    "    batch_sent_id = -1\n",
    "    for sent in sent_list:\n",
    "        batch_sent_id += 1\n",
    "        sent_toks = tokenizer(sent)\n",
    "        for sent_oov_pos, text in get_bert_masked_inputs(sent_toks, bert_tokenizer):\n",
    "            # To accurately get what is the position of [MASK] according\n",
    "            # to BERT tokenizer, we need to re-tokenize the sentence using\n",
    "            # the BERT tokenizer\n",
    "            all_bert_toks = bert_tokenizer.tokenize(text)\n",
    "            bert_toks = all_bert_toks[0:max_len] # 512 is the max. Bert seq. length\n",
    "\n",
    "            tok_ids = bert_tokenizer.convert_tokens_to_ids(bert_toks)\n",
    "            pos_oov = None\n",
    "            for i in range(len(bert_toks)):\n",
    "                if bert_toks[i] == '[MASK]':\n",
    "                    pos_oov = i\n",
    "                    break\n",
    "            assert(pos_oov is not None or len(all_bert_toks) > max_len)\n",
    "            if pos_oov is not None:\n",
    "                tok_qty = len(tok_ids)\n",
    "                batch_max_seq_qty = max(batch_max_seq_qty, tok_qty)\n",
    "                batch_data_raw.append( \n",
    "                    UtterData(batch_sent_id=batch_sent_id, \n",
    "                              pos_oov=sent_oov_pos, \n",
    "                              tok_ids=tok_ids, \n",
    "                              oov_token=sent_toks[sent_oov_pos]))\n",
    "            \n",
    "    batch_qty = len(batch_data_raw)\n",
    "    tok_ids_batch = np.zeros( (batch_qty, batch_max_seq_qty), dtype=np.int64) # zero is a padding symbol\n",
    "    for k in range(batch_qty):\n",
    "        tok_ids = batch_data_raw[k].tok_ids\n",
    "        tok_ids_batch[k, 0:len(tok_ids)] = tok_ids\n",
    "        \n",
    "                   \n",
    "    tok_ids_batch = torch.from_numpy(tok_ids_batch).to(device=torch_device) \n",
    "    \n",
    "    return batch_data_raw, tok_ids_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from collections import namedtuple\n",
    "import sys\n",
    "\n",
    "BertPredProbs = namedtuple('BertPred', ['batch_sent_id', 'pos_oov', 'logits'])\n",
    "\n",
    "def get_bert_preds_for_words_batch(torch_device, bert_model_mlm, \n",
    "                                   batch_data_raw, tok_ids_batch, # comes from get_batch_data\n",
    "                                   word_ids, # a list of IDS for which we generate logits\n",
    "                                   max_len=MAX_BERT_LEN):\n",
    "\n",
    "    seg_ids = torch.zeros_like(tok_ids_batch, device=torch_device)\n",
    "    \n",
    "    batch_qty = len(batch_data_raw)\n",
    "    \n",
    "    # Main BERT model see modeling.py in https://github.com/huggingface/pytorch-pretrained-BERT\n",
    "    bert = bert_model_mlm.bert \n",
    "    # cls is an instance of BertOnlyMLMHead (see https://github.com/huggingface/pytorch-pretrained-BERT)\n",
    "    cls = bert_model_mlm.cls\n",
    "    # predictions are of the type BertLMPredictionHead (see https://github.com/huggingface/pytorch-pretrained-BERT)\n",
    "    predictions = cls.predictions\n",
    "    transform = predictions.transform\n",
    "   \n",
    "    # We don't use the complete decoding matrix, but only selected rows\n",
    "    word_ids = torch.from_numpy(np.array(word_ids, dtype=np.int64)).to(device=torch_device)\n",
    "                                \n",
    "    weight = predictions.decoder.weight[word_ids,:]\n",
    "    bias = predictions.bias[word_ids]\n",
    "\n",
    "    # Transformations from the main BERT model\n",
    "    sequence_output, _= bert(tok_ids_batch, seg_ids, attention_mask=None, output_all_encoded_layers=False)\n",
    "    # Transformations from the BertLMPredictionHead model with the restricted last layer\n",
    "    hidden_states = transform(sequence_output)    \n",
    "    logits = torch.nn.functional.linear(hidden_states, weight) + bias                            \n",
    "                                        \n",
    "    logits=logits.detach().cpu().numpy()\n",
    "    \n",
    "    res = []\n",
    "    \n",
    "    for k in range(batch_qty):\n",
    "        \n",
    "        pos_oov = batch_data_raw[k].pos_oov         \n",
    "        res.append( BertPredProbs(batch_sent_id = batch_data_raw[k].batch_sent_id,\n",
    "                             pos_oov = pos_oov,\n",
    "                             logits = logits[k, pos_oov]\n",
    "                            ) \n",
    "                  )\n",
    "        \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer.convert_tokens_to_ids(['hell', 'fuck', 'heck', 'devil', 'shit', 'doing', \n",
    "                                      'doin', 'wearing', 'making', 'thinking', 'all', 'test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sent_list = ['What the fcuk are you doingg here?',\n",
    "             'This is a *strangge* sentence']\n",
    "\n",
    "batch_data_raw, tok_ids_batch = get_batch_data(torch_device, \n",
    "                                                tokenizer, \n",
    "                                                bert_tokenizer, \n",
    "                                                sent_list,\n",
    "                                                MAX_BERT_LEN)\n",
    "\n",
    "get_bert_preds_for_words_batch(torch_device,\n",
    "                               bert_model_mlm, \n",
    "                               batch_data_raw, tok_ids_batch,\n",
    "                               bert_tokenizer.convert_tokens_to_ids(['hell', 'fuck', 'heck', 'devil', 'shit', 'doing', \n",
    "                                                                  'doin', 'wearing', 'making', 'thinking']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bert_model_mlm.to('cpu')\n",
    "#torch.zeros(3,device=torch.device(\"cuda\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer.tokenize('б')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import *\n",
    "from overrides import overrides\n",
    "import allennlp\n",
    "from allennlp.data.vocabulary import Vocabulary\n",
    "from allennlp.data.dataset_readers import DatasetReader\n",
    "\n",
    "from allennlp.data import Instance\n",
    "from allennlp.data.token_indexers import TokenIndexer, SingleIdTokenIndexer\n",
    "from allennlp.data.tokenizers import Token\n",
    "\n",
    "from allennlp.data.tokenizers.word_splitter import SpacyWordSplitter\n",
    "from allennlp.data.token_indexers import WordpieceIndexer, SingleIdTokenIndexer\n",
    "from allennlp.data.fields import TextField, SequenceLabelField, LabelField, MetadataField, ArrayField\n",
    "class MemoryOptimizedTextField(TextField):\n",
    "    @overrides\n",
    "    def __init__(self, tokens: List[str], token_indexers: Dict[str, TokenIndexer]) -> None:\n",
    "        self.tokens = tokens\n",
    "        self._token_indexers = token_indexers\n",
    "        self._indexed_tokens: Optional[Dict[str, TokenList]] = None\n",
    "        self._indexer_name_to_indexed_token: Optional[Dict[str, List[str]]] = None\n",
    "        # skip checks for tokens\n",
    "    @overrides\n",
    "    def index(self, vocab):\n",
    "        super().index(vocab)\n",
    "        self.tokens = None # empty tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are *MAIN* vocabulary word IDs for words in the BERT vocabulary.\n",
    "bert_vocab_term_glob_ids = []\n",
    "bert_vocab_term_bert_ids = []\n",
    "\n",
    "for tok, bert_tok_id in bert_tokenizer.vocab.items():\n",
    "    glob_tok_id = vocab.get_token_index(tok)\n",
    "    if glob_tok_id > 1:\n",
    "        bert_vocab_term_glob_ids.append(glob_tok_id)\n",
    "        bert_vocab_term_bert_ids.append(bert_tok_id)\n",
    "        \n",
    "bert_vocab_term_glob_ids = np.array(bert_vocab_term_glob_ids)\n",
    "bert_vocab_term_bert_ids = np.array(bert_vocab_term_bert_ids)\n",
    "fasttext_embeds[bert_vocab_term_glob_ids].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(bert_vocab_term_bert_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nmslib, time\n",
    "\n",
    "M = 30\n",
    "efC = 200\n",
    "\n",
    "num_threads = 0\n",
    "index_time_params = {'M': M, 'indexThreadQty': num_threads, 'efConstruction': efC, 'post' : 0}\n",
    "print('Index-time parameters', index_time_params)\n",
    "\n",
    "# Space name should correspond to the space name \n",
    "# used for brute-force search\n",
    "space_name='cosinesimil'\n",
    "\n",
    "\n",
    "# Intitialize the library, specify the space, the type of the vector and add data points \n",
    "index = nmslib.init(method='hnsw', space=space_name, data_type=nmslib.DataType.DENSE_VECTOR) \n",
    "index.addDataPointBatch(fasttext_embeds[bert_vocab_term_glob_ids], bert_vocab_term_bert_ids)\n",
    "\n",
    "# Create an index\n",
    "start = time.time()\n",
    "index_time_params = {'M': M, 'indexThreadQty': num_threads, 'efConstruction': efC}\n",
    "index.createIndex(index_time_params) \n",
    "end = time.time() \n",
    "print('Index-time parameters', index_time_params)\n",
    "print('Indexing time = %f' % (end-start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting query-time parameters\n",
    "efS = 200\n",
    "K=10\n",
    "query_time_params = {'efSearch': efS}\n",
    "print('Setting query-time parameters', query_time_params)\n",
    "index.setQueryTimeParams(query_time_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pickle\n",
    "#with open(\"../data/jigsaw/val_ds.bin\", \"rb\") as f:\n",
    "#    val_ds = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import groupby\n",
    "import re\n",
    "\n",
    "def remove_extra_chars(s, max_qty=2):\n",
    "    res = [c * min(max_qty, len(list(group_iter))) for c, group_iter in groupby(s)] \n",
    "    return ''.join(res)\n",
    "\n",
    "def is_apost_token(s):\n",
    "    return re.match(r\"'[a-z]{1,3}$\", s) is not None\n",
    "\n",
    "\n",
    "def replace_by_patterns(tokenizer, s, replace_dict):\n",
    "    res = tokenizer(s)\n",
    "    for pos, repl in replace_dict.items():\n",
    "        res[pos] = repl\n",
    "    return ' '.join(res)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repl_oov_files = [ (\"../data/jigsaw/test_proced.csv\", \"../data/jigsaw/test_proced_no_oov1.csv\") , (\"../data/jigsaw/train.csv\", \"../data/jigsaw/train_no_oov1.csv\") ]\n",
    "#repl_oov_files = [ (\"../data/jigsaw/val.csv\", \"../data/jigsaw/val_no_oov1.csv\")  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, gc\n",
    "import pandas as pd\n",
    "from scipy.special import softmax\n",
    "\n",
    "\n",
    "DEBUG_PRINT=False\n",
    "\n",
    "for src_file, dst_file in repl_oov_files:\n",
    "    src_data = pd.read_csv(src_file)\n",
    "    \n",
    "    t0 = time.time()\n",
    "    preds = []\n",
    "    \n",
    "    #all_src_sents = [' '.join(t['tokens']) for t in val_ds]\n",
    "    all_src_sents = list(src_data['comment_text'])\n",
    "    all_dst_sents = []\n",
    "    \n",
    "    #print(s)\n",
    "    #preds.append(get_bert_top_preds(tokenizer, bert_tokenizer, s, 2))\n",
    "    #preds.append(get_bert_masked_inputs(tokenizer(s), bert_tokenizer, sent))\n",
    "\n",
    "\n",
    "    batch_qty_step = 20\n",
    "\n",
    "    for batch_start_sent_id in range(0, len(all_src_sents), batch_qty_step):\n",
    "        print('Batch start', batch_start_sent_id)\n",
    "\n",
    "        batch_qty = min(batch_qty_step, len(all_src_sents) - batch_start_sent_id)\n",
    "\n",
    "        batch_sents = [all_src_sents[k] for k in range(batch_start_sent_id,\n",
    "                                                   batch_start_sent_id + batch_qty)]\n",
    "\n",
    "        replace_dict = {k : dict() for k in range(0,batch_qty)} \n",
    "\n",
    "        # batch_data raw contains elements\n",
    "        # UtterData = namedtuple('SentData', ['batch_sent_id', 'pos_oov', 'tok_ids', 'oov_token')\n",
    "        # NOTE: pos_oov is OOV index with respect to the original (not BERT) tokenizer!!!\n",
    "        #\n",
    "        # tok_ids_batch is a Tensor with padded Bert-specific token IDs ready\n",
    "        # to be fed into a BERT model\n",
    "        batch_data_raw, tok_ids_batch = get_batch_data(torch_device,\n",
    "                                                     tokenizer, bert_tokenizer,\n",
    "                                                     batch_sents, \n",
    "                                                     MAX_BERT_LEN)\n",
    "\n",
    "        query_arr = []\n",
    "        query_tok_oov_id = []\n",
    "\n",
    "        for e in batch_data_raw: \n",
    "            w = e.oov_token\n",
    "            wCompr = remove_extra_chars(w)\n",
    "            wid = vocab.get_token_index(wCompr)\n",
    "            if w != wCompr:\n",
    "                if wid < 2:\n",
    "                    wid = vocab.get_token_index(w)\n",
    "\n",
    "            query_arr.append(fasttext_embeds[wid])\n",
    "            query_tok_oov_id.append(wid)\n",
    "\n",
    "        query_arr = np.array(query_arr)\n",
    "        query_matrix = np.array(query_arr)\n",
    "        query_qty = query_matrix.shape[0]\n",
    "\n",
    "        if DEBUG_PRINT: print('Query matrix shape:', query_matrix.shape)\n",
    "\n",
    "        start = time.time() \n",
    "        # nbrs is array of tuples (neighbor array, distance array)\n",
    "        # For cosine, the distance is 1 - cosine similarity\n",
    "        # k-NN search returns Bert-specific token IDs\n",
    "        nbrs = index.knnQueryBatch(query_matrix, k = K, num_threads = num_threads)\n",
    "        end = time.time() \n",
    "        if DEBUG_PRINT: \n",
    "            print('kNN time total=%f (sec), per query=%f (sec), per query adjusted for thread number=%f (sec)' % \n",
    "              (end-start, float(end-start)/query_qty, num_threads*float(end-start)/query_qty))\n",
    "\n",
    "        neighb_tok_ids=set()\n",
    "\n",
    "        for qid in range(query_qty):\n",
    "            if query_tok_oov_id[qid] > 1:\n",
    "                nbrs_ids = nbrs[qid][0]\n",
    "                nbrs_dist = nbrs[qid][1]\n",
    "\n",
    "                nqty = len(nbrs_ids)\n",
    "                for t in range(nqty):\n",
    "                    if nbrs_dist[t] < MAX_COSINE_DIST:\n",
    "                        assert(nbrs_ids[t] < BERT_VOCAB_QTY)\n",
    "                        neighb_tok_ids.add(nbrs_ids[t])\n",
    "\n",
    "\n",
    "        neighb_tok_ids = list(neighb_tok_ids)\n",
    "\n",
    "        preds = get_bert_preds_for_words_batch(torch_device,\n",
    "                                               bert_model_mlm,\n",
    "                                               batch_data_raw, tok_ids_batch,\n",
    "                                               neighb_tok_ids)\n",
    "\n",
    "        assert(len(preds) == query_qty)\n",
    "        for qid in range(query_qty):\n",
    "            e = batch_data_raw[qid]\n",
    "            glob_sent_id = batch_start_sent_id + e.batch_sent_id\n",
    "            assert(batch_sents[e.batch_sent_id] == all_src_sents[glob_sent_id])\n",
    "            if is_apost_token(e.oov_token) or e.oov_token == \"n't\":\n",
    "                # Thing's like \"I don't\" or \"You're\" are tokenized as do \"I do n't\" or \"You 're'\"\n",
    "                pass # TODO fix this\n",
    "            elif query_tok_oov_id[qid] > 1:  \n",
    "                # Let's map neighbor IDs from each queries to respective \n",
    "                # logits from the prediction set\n",
    "                logit_map = dict() # from Bert-specific token IDs to predicted logits\n",
    "                assert(len(preds[qid].logits) == len(neighb_tok_ids))\n",
    "                for i in range(len(neighb_tok_ids)):\n",
    "                    logit_map[neighb_tok_ids[i]] = preds[qid].logits[i]\n",
    "\n",
    "                e = batch_data_raw[qid]\n",
    "                if DEBUG_PRINT: \n",
    "                    print(all_src_sents[glob_sent_id])\n",
    "                    print(\"### OOV ###\", e.oov_token)\n",
    "                    print([bert_id2tok[bert_tok_id] for bert_tok_id in nbrs[qid][0]])\n",
    "\n",
    "                nbrs_sel_logits = []\n",
    "                nbrs_sel_toks = []\n",
    "                nbrs_sel_dists = []\n",
    "\n",
    "                nbrs_ids = nbrs[qid][0]\n",
    "                nbrs_dist = nbrs[qid][1]\n",
    "\n",
    "                #print('Logit map:', logit_map)\n",
    "                #print('neighb_tok_ids', neighb_tok_ids)\n",
    "\n",
    "                nqty = len(nbrs_ids)\n",
    "                for t in range(nqty):\n",
    "                    bert_tok_id = nbrs_ids[t]\n",
    "                    # nid is Bert-speicifc token ID\n",
    "                    if not bert_tok_id in neighb_tok_ids:\n",
    "                        if DEBUG_PRINT: \n",
    "                            print('Missing %s distance %g ' \n",
    "                                  % (bert_id2tok[bert_tok_id],\n",
    "                                     nbrs_dist[t]))\n",
    "                    else:\n",
    "                        if nbrs_dist[t] < MAX_COSINE_DIST:\n",
    "                            nbrs_sel_logits.append(logit_map[bert_tok_id])\n",
    "                            nbrs_sel_toks.append(bert_id2tok[bert_tok_id]) \n",
    "                            nbrs_sel_dists.append(nbrs_dist[t])\n",
    "\n",
    "                if nbrs_sel_logits:\n",
    "                    nbrs_softmax = softmax(np.array(nbrs_sel_logits))\n",
    "                    nbrs_simil = 1 - np.array(nbrs_sel_dists)\n",
    "                    nbrs_simil_adj = nbrs_softmax * nbrs_simil \n",
    "\n",
    "                    best_tok_id = np.argmax(nbrs_simil_adj)\n",
    "\n",
    "                    #print(\"batch sent id:\",e.batch_sent_id, e.pos_oov, best_tok_id)\n",
    "                    #print(replace_dict[e.batch_sent_id])\n",
    "                    assert(not e.pos_oov in replace_dict[e.batch_sent_id])\n",
    "                    replace_dict[e.batch_sent_id][e.pos_oov] = nbrs_sel_toks[best_tok_id]\n",
    "\n",
    "                    if DEBUG_PRINT: \n",
    "                        print('Selected info, best_tok:', nbrs_sel_toks[best_tok_id])\n",
    "                        for k in range(len(nbrs_sel_logits)):\n",
    "                            print(nbrs_sel_toks[k], nbrs_softmax[k], \n",
    "                                  nbrs_sel_dists[k], nbrs_simil_adj[k])\n",
    "                else:\n",
    "                    if DEBUG_PRINT: print('Nothing found!')\n",
    "\n",
    "                #if DEBUG_PRINT: print(preds[qid])\n",
    "                if DEBUG_PRINT: \n",
    "                    print(\"====================================================================\")\n",
    "\n",
    "\n",
    "\n",
    "        #gc.collect()\n",
    "        #torch.cuda.empty_cache()\n",
    "        for k in range(0, batch_qty):\n",
    "            src_sent = batch_sents[k]\n",
    "            rd = replace_dict[k]\n",
    "            #print('Replacement dict:', rd)\n",
    "            dst_sent = replace_by_patterns(tokenizer, src_sent, rd)\n",
    "            all_dst_sents.append(dst_sent)\n",
    "            if DEBUG_PRINT:\n",
    "                print(\"====================================================================\")\n",
    "                print('Replacement dict:', rd)\n",
    "                print(src_sent)\n",
    "                print('------------')\n",
    "                print(dst_sent)\n",
    "                print(\"====================================================================\")\n",
    "\n",
    "        #break\n",
    "\n",
    "    t1 = time.time()\n",
    "    print('# of src sentences:', len(all_src_sents), \n",
    "          \"# of dst sentences:\", len(all_dst_sents),\n",
    "          ' time elapsed:', t1 - t0)\n",
    "    src_data['comment_text'] = all_dst_sents\n",
    "    src_data.to_csv(dst_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#src_data[src_data[\"toxic\"]==1].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fl = pd.read_csv(src_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fl[fl[\"toxic\"]==1].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
