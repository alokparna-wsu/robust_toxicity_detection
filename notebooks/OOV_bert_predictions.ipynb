{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from pytorch_pretrained_bert.tokenization import BertTokenizer\n",
    "from pytorch_pretrained_bert.modeling import BertForMaskedLM\n",
    "\n",
    "torch_device=torch.device('cuda')\n",
    "\n",
    "bert_model_mlm = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "bert_model_mlm.eval()\n",
    "bert_model_mlm.to(torch_device)\n",
    "\n",
    "for param in bert_model_mlm.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "bert_id2tok = dict()\n",
    "for tok, tok_id in bert_tokenizer.vocab.items():\n",
    "    bert_id2tok[tok_id] = tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "ft_compiled_path = \"../data/jigsaw/ft_compiled.npy\" # Embeddings generated from the vocabulary\n",
    "data_vocab_path = \"../data/jigsaw/data_vocab.bin\"\n",
    "\n",
    "MAX_BERT_LEN=128\n",
    "MAX_COSINE_DIST=0.3\n",
    "\n",
    "num_threads=8\n",
    "K=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.data.tokenizers.word_splitter import SpacyWordSplitter\n",
    "\n",
    "_spacy_tok = SpacyWordSplitter(language='en_core_web_sm', pos_tags=False).split_words\n",
    "\n",
    "def tokenizer(s: str):\n",
    "    s = s.lower()\n",
    "    return [w.text for w in _spacy_tok(s)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'ä¸–' in bert_tokenizer.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[unused1]'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_id2tok[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3231"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_tokenizer.vocab['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns arrays of arrays if there's an OOV word or an empty array instead\n",
    "# Each array element is a tuple: \n",
    "# position of OOV word (with respect to the original tokenizer), sent for BERT tokenizer\n",
    "def get_bert_masked_inputs(toks, bert_tokenizer):\n",
    "    res = []\n",
    "    \n",
    "    oov_pos = []\n",
    "    bert_vocab = bert_tokenizer.vocab\n",
    "    \n",
    "    for i in range(len(toks)):\n",
    "        if toks[i] not in bert_vocab:\n",
    "            oov_pos.append(i)\n",
    "            \n",
    "\n",
    "    for pos in oov_pos:\n",
    "        res.append( (pos, '[CLS] %s [MASK] %s [SEP]' % \n",
    "                     (' '.join(toks[0:pos]), ' '.join(toks[pos+1:])) ) )\n",
    "        \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4, '[CLS] this is a * [MASK] * sentence . [SEP]')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_bert_masked_inputs(tokenizer('This is a *strangge* sentence.'), bert_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]', 'what', 'the', '[MASK]', 'are', 'you', 'doing', 'here', '?', '[SEP]']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toks = bert_tokenizer.tokenize('[CLS] what the [MASK] are you doing here ? [SEP]')\n",
    "toks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "# pos_bert is OOV index with respect to the original (not BERT) tokenizer!!!\n",
    "UtterData = namedtuple('SentData', ['sent_id', 'pos_bert', 'tok_ids', 'oov_token'])\n",
    "\n",
    "def get_batch_data(torch_device, tokenizer, bert_tokenizer, sent_list, max_len=MAX_BERT_LEN):\n",
    "    \n",
    "    batch_data_raw = []\n",
    "    batch_max_seq_qty = 0\n",
    "    sent_id = -1\n",
    "    for sent in sent_list:\n",
    "        sent_id += 1\n",
    "        sent_toks = tokenizer(sent)\n",
    "        for sent_oov_pos, text in get_bert_masked_inputs(sent_toks, bert_tokenizer):\n",
    "            # To accurately get what is the position of [MASK] according\n",
    "            # to BERT tokenizer, we need to re-tokenize the sentence using\n",
    "            # the BERT tokenizer\n",
    "            all_bert_toks = bert_tokenizer.tokenize(text)\n",
    "            bert_toks = all_bert_toks[0:max_len] # 512 is the max. Bert seq. length\n",
    "\n",
    "            tok_ids = bert_tokenizer.convert_tokens_to_ids(bert_toks)\n",
    "            pos_bert = None\n",
    "            for i in range(len(bert_toks)):\n",
    "                if bert_toks[i] == '[MASK]':\n",
    "                    pos_bert = i\n",
    "                    break\n",
    "            assert(pos_bert is not None or len(all_bert_toks) > max_len)\n",
    "            if pos_bert is not None:\n",
    "                tok_qty = len(tok_ids)\n",
    "                batch_max_seq_qty = max(batch_max_seq_qty, tok_qty)\n",
    "                batch_data_raw.append( \n",
    "                    UtterData(sent_id=sent_id, \n",
    "                              pos_bert=pos_bert, \n",
    "                              tok_ids=tok_ids, \n",
    "                              oov_token=sent_toks[sent_oov_pos]) )\n",
    "            \n",
    "    batch_qty = len(batch_data_raw)\n",
    "    tok_ids_batch = np.zeros( (batch_qty, batch_max_seq_qty), dtype=np.int64) # zero is a padding symbol\n",
    "    for k in range(batch_qty):\n",
    "        tok_ids = batch_data_raw[k].tok_ids\n",
    "        tok_ids_batch[k, 0:len(tok_ids)] = tok_ids\n",
    "        \n",
    "                   \n",
    "    tok_ids_batch = torch.from_numpy(tok_ids_batch).to(device=torch_device) \n",
    "    \n",
    "    return batch_data_raw, tok_ids_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from collections import namedtuple\n",
    "\n",
    "BertPred = namedtuple('BertPred', ['sent_id', 'pos_bert', 'probs', 'toks'])\n",
    "\n",
    "def get_bert_top_preds_batch(torch_device, bert_model_mlm, tokenizer, bert_tokenizer, sent_list, k, max_len=128):\n",
    "    \n",
    "    batch_data_raw, tok_ids_batch = get_batch_data(torch_device, \n",
    "                                                    tokenizer, \n",
    "                                                    bert_tokenizer, \n",
    "                                                    sent_list,\n",
    "                                                    max_len)\n",
    "    seg_ids = torch.zeros_like(tok_ids_batch, device=torch_device)\n",
    "\n",
    "    batch_qty = len(batch_data_raw)\n",
    "    \n",
    "    model_out = bert_model_mlm(tok_ids_batch, seg_ids)\n",
    "        \n",
    "    t=torch.topk(torch.nn.functional.softmax(model_out, dim=2), k=k,dim=2)\n",
    "    \n",
    "    probs=t[0].detach().cpu().numpy()\n",
    "    preds=t[1].cpu().numpy()\n",
    "    \n",
    "    res = []\n",
    "    \n",
    "    for k in range(batch_qty):\n",
    "        \n",
    "        pos_bert = batch_data_raw[k].pos_bert         \n",
    "        res.append( BertPred(sent_id = batch_data_raw[k].sent_id,\n",
    "                             pos_bert = pos_bert,\n",
    "                             probs = probs[k, pos_bert],\n",
    "                             toks = bert_tokenizer.convert_ids_to_tokens(preds[k, pos_bert]) \n",
    "                            ) \n",
    "                  )\n",
    "    torch.cuda.empty_cache()\n",
    "        \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[BertPred(sent_id=0, pos_bert=3, probs=array([9.2102903e-01, 5.3823169e-02, 1.7165799e-02, 5.3161602e-03,\n",
       "        2.7156417e-04], dtype=float32), toks=['hell', 'fuck', 'heck', 'devil', 'shit']),\n",
       " BertPred(sent_id=0, pos_bert=7, probs=array([9.9966323e-01, 1.8645465e-04, 3.9865798e-05, 1.3824779e-05,\n",
       "        1.3223614e-05], dtype=float32), toks=['doing', 'doin', 'wearing', 'making', 'thinking']),\n",
       " BertPred(sent_id=1, pos_bert=5, probs=array([0.13034092, 0.07515147, 0.05074421, 0.05045162, 0.03581695],\n",
       "       dtype=float32), toks=['a', '*', '.', '-', 'b'])]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_bert_top_preds_batch(torch_device,\n",
    "                         bert_model_mlm, tokenizer, bert_tokenizer, \n",
    "                         ['What the fcuk are you doingg here?',\n",
    "                          'This is a *strangge* sentence'], 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from collections import namedtuple\n",
    "import sys\n",
    "\n",
    "BertPredProbs = namedtuple('BertPred', ['sent_id', 'pos_bert', 'logits'])\n",
    "\n",
    "def get_bert_preds_for_words_batch(torch_device, bert_model_mlm, \n",
    "                                   batch_data_raw, tok_ids_batch, # comes from get_batch_data\n",
    "                                   word_ids, # a list of IDS for which we generate logits\n",
    "                                   max_len=MAX_BERT_LEN):\n",
    "\n",
    "    seg_ids = torch.zeros_like(tok_ids_batch, device=torch_device)\n",
    "    \n",
    "    batch_qty = len(batch_data_raw)\n",
    "    \n",
    "    # Main BERT model see modeling.py in https://github.com/huggingface/pytorch-pretrained-BERT\n",
    "    bert = bert_model_mlm.bert \n",
    "    # cls is an instance of BertOnlyMLMHead (see https://github.com/huggingface/pytorch-pretrained-BERT)\n",
    "    cls = bert_model_mlm.cls\n",
    "    # predictions are of the type BertLMPredictionHead (see https://github.com/huggingface/pytorch-pretrained-BERT)\n",
    "    predictions = cls.predictions\n",
    "    transform = predictions.transform\n",
    "   \n",
    "    # We don't use the complete decoding matrix, but only selected rows\n",
    "    word_ids = torch.from_numpy(np.array(word_ids, dtype=np.int64)).to(device=torch_device)\n",
    "                                \n",
    "    weight = predictions.decoder.weight[word_ids,:]\n",
    "    bias = predictions.bias[word_ids]\n",
    "\n",
    "    # Transformations from the main BERT model\n",
    "    sequence_output, _= bert(tok_ids_batch, seg_ids, attention_mask=None, output_all_encoded_layers=False)\n",
    "    # Transformations from the BertLMPredictionHead model with the restricted last layer\n",
    "    hidden_states = transform(sequence_output)    \n",
    "    logits = torch.nn.functional.linear(hidden_states, weight) + bias                            \n",
    "                                        \n",
    "    logits=logits.detach().cpu().numpy()\n",
    "    \n",
    "    res = []\n",
    "    \n",
    "    for k in range(batch_qty):\n",
    "        \n",
    "        pos_bert = batch_data_raw[k].pos_bert         \n",
    "        res.append( BertPredProbs(sent_id = batch_data_raw[k].sent_id,\n",
    "                             pos_bert = pos_bert,\n",
    "                             logits = logits[k, pos_bert]\n",
    "                            ) \n",
    "                  )\n",
    "                                \n",
    "    torch.cuda.empty_cache()\n",
    "        \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3109, 6616, 17752, 6548, 4485, 2725, 24341, 4147, 2437, 3241, 2035, 3231]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_tokenizer.convert_tokens_to_ids(['hell', 'fuck', 'heck', 'devil', 'shit', 'doing', \n",
    "                                      'doin', 'wearing', 'making', 'thinking', 'all', 'test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[BertPred(sent_id=0, pos_bert=3, logits=array([15.889928 , 13.05014  , 11.907355 , 10.735188 ,  7.7608795,\n",
       "         1.1303823, -0.362491 , -2.6011868, -0.1715025, -1.2756928],\n",
       "       dtype=float32)),\n",
       " BertPred(sent_id=0, pos_bert=7, logits=array([-6.5866728e+00, -3.7194698e+00, -4.6205878e+00, -7.7321987e+00,\n",
       "        -1.2921356e-03,  2.0477596e+01,  1.1890611e+01,  1.0347941e+01,\n",
       "         9.2888851e+00,  9.2444267e+00], dtype=float32)),\n",
       " BertPred(sent_id=1, pos_bert=5, logits=array([-3.2651033 , -0.14229655, -4.819803  , -4.6907854 , -0.7839051 ,\n",
       "        -1.4695988 , -3.3453448 , -5.069694  , -2.1900396 , -2.5339675 ],\n",
       "       dtype=float32))]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_list = ['What the fcuk are you doingg here?',\n",
    "             'This is a *strangge* sentence']\n",
    "\n",
    "batch_data_raw, tok_ids_batch = get_batch_data(torch_device, \n",
    "                                                tokenizer, \n",
    "                                                bert_tokenizer, \n",
    "                                                sent_list,\n",
    "                                                MAX_BERT_LEN)\n",
    "\n",
    "get_bert_preds_for_words_batch(torch_device,\n",
    "                               bert_model_mlm, \n",
    "                               batch_data_raw, tok_ids_batch,\n",
    "                               bert_tokenizer.convert_tokens_to_ids(['hell', 'fuck', 'heck', 'devil', 'shit', 'doing', \n",
    "                                                                  'doin', 'wearing', 'making', 'thinking']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bert_model_mlm.to('cpu')\n",
    "#torch.zeros(3,device=torch.device(\"cuda\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ð±']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_tokenizer.tokenize('Ð±')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import *\n",
    "from overrides import overrides\n",
    "import allennlp\n",
    "from allennlp.data.vocabulary import Vocabulary\n",
    "from allennlp.data.dataset_readers import DatasetReader\n",
    "\n",
    "from allennlp.data import Instance\n",
    "from allennlp.data.token_indexers import TokenIndexer, SingleIdTokenIndexer\n",
    "from allennlp.data.tokenizers import Token\n",
    "\n",
    "from allennlp.data.tokenizers.word_splitter import SpacyWordSplitter\n",
    "from allennlp.data.token_indexers import WordpieceIndexer, SingleIdTokenIndexer\n",
    "from allennlp.data.fields import TextField, SequenceLabelField, LabelField, MetadataField, ArrayField\n",
    "class MemoryOptimizedTextField(TextField):\n",
    "    @overrides\n",
    "    def __init__(self, tokens: List[str], token_indexers: Dict[str, TokenIndexer]) -> None:\n",
    "        self.tokens = tokens\n",
    "        self._token_indexers = token_indexers\n",
    "        self._indexed_tokens: Optional[Dict[str, TokenList]] = None\n",
    "        self._indexer_name_to_indexed_token: Optional[Dict[str, List[str]]] = None\n",
    "        # skip checks for tokens\n",
    "    @overrides\n",
    "    def index(self, vocab):\n",
    "        super().index(vocab)\n",
    "        self.tokens = None # empty tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "ft_compiled_path = \"../data/jigsaw/ft_compiled.npy\" # Embeddings generated from the vocabulary\n",
    "data_vocab_path = \"../data/jigsaw/data_vocab.bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext_embeds = np.load(ft_compiled_path)\n",
    "vocab=pickle.load(open(data_vocab_path,'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(305140, 30522)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_vocab_toks = bert_tokenizer.vocab.keys()\n",
    "vocab_toks = set( [w for idx, w in vocab.get_index_to_token_vocabulary().items() ])\n",
    "len(vocab_toks), len(bert_vocab_toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22778, 300)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# These are BERT vocabulary word IDs in the *MAIN vocabulary!*, not IDs in the BERT vocab.\n",
    "bert_vocab_ids = []\n",
    "\n",
    "for tok in bert_vocab_toks:\n",
    "    tok_id = vocab.get_token_index(tok)\n",
    "    if tok_id > 1:\n",
    "        bert_vocab_ids.append(tok_id)\n",
    "        \n",
    "bert_vocab_ids = np.array(bert_vocab_ids)\n",
    "fasttext_embeds[bert_vocab_ids].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index-time parameters {'M': 30, 'indexThreadQty': 0, 'efConstruction': 200, 'post': 0}\n",
      "Index-time parameters {'M': 30, 'indexThreadQty': 0, 'efConstruction': 200}\n",
      "Indexing time = 4.955989\n"
     ]
    }
   ],
   "source": [
    "import nmslib, time\n",
    "\n",
    "M = 30\n",
    "efC = 200\n",
    "\n",
    "num_threads = 0\n",
    "index_time_params = {'M': M, 'indexThreadQty': num_threads, 'efConstruction': efC, 'post' : 0}\n",
    "print('Index-time parameters', index_time_params)\n",
    "\n",
    "# Space name should correspond to the space name \n",
    "# used for brute-force search\n",
    "space_name='cosinesimil'\n",
    "\n",
    "\n",
    "# Intitialize the library, specify the space, the type of the vector and add data points \n",
    "index = nmslib.init(method='hnsw', space=space_name, data_type=nmslib.DataType.DENSE_VECTOR) \n",
    "index.addDataPointBatch(fasttext_embeds[bert_vocab_ids], bert_vocab_ids)\n",
    "\n",
    "# Create an index\n",
    "start = time.time()\n",
    "index_time_params = {'M': M, 'indexThreadQty': num_threads, 'efConstruction': efC}\n",
    "index.createIndex(index_time_params) \n",
    "end = time.time() \n",
    "print('Index-time parameters', index_time_params)\n",
    "print('Indexing time = %f' % (end-start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting query-time parameters {'efSearch': 200}\n"
     ]
    }
   ],
   "source": [
    "# Setting query-time parameters\n",
    "efS = 200\n",
    "K=10\n",
    "query_time_params = {'efSearch': efS}\n",
    "print('Setting query-time parameters', query_time_params)\n",
    "index.setQueryTimeParams(query_time_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"../data/jigsaw/val_ds.bin\", \"rb\") as f:\n",
    "    val_ds = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch start 0\n",
      "Query matrix shape: (86, 300)\n",
      "kNN time total=0.024678 (sec), per query=0.000287 (sec), per query adjusted for thread number=0.000000 (sec)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-170fff596e80>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     69\u001b[0m                                            \u001b[0mbert_model_mlm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m                                            \u001b[0mbatch_data_raw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtok_ids_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m                                            neighb_tok_ids)\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mquery_qty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-251be28fc676>\u001b[0m in \u001b[0;36mget_bert_preds_for_words_batch\u001b[0;34m(torch_device, bert_model_mlm, batch_data_raw, tok_ids_batch, word_ids, max_len)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;31m# Transformations from the main BERT model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0msequence_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mbert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtok_ids_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseg_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_all_encoded_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0;31m# Transformations from the BertLMPredictionHead model with the restricted last layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/neuralnlp/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/neuralnlp/lib/python3.7/site-packages/pytorch_pretrained_bert/modeling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, token_type_ids, attention_mask, output_all_encoded_layers)\u001b[0m\n\u001b[1;32m    709\u001b[0m         \u001b[0mextended_attention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m10000.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 711\u001b[0;31m         \u001b[0membedding_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    712\u001b[0m         encoded_layers = self.encoder(embedding_output,\n\u001b[1;32m    713\u001b[0m                                       \u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/neuralnlp/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/neuralnlp/lib/python3.7/site-packages/pytorch_pretrained_bert/modeling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, token_type_ids)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m         \u001b[0mwords_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m         \u001b[0mposition_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m         \u001b[0mtoken_type_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_type_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/neuralnlp/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/neuralnlp/lib/python3.7/site-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    116\u001b[0m         return F.embedding(\n\u001b[1;32m    117\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/neuralnlp/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   1452\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1453\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1454\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered"
     ]
    }
   ],
   "source": [
    "import time, gc\n",
    "t0 = time.time()\n",
    "preds = []\n",
    "sents = [' '.join(t['tokens']) for t in val_ds]\n",
    "    #print(s)\n",
    "    #preds.append(get_bert_top_preds(tokenizer, bert_tokenizer, s, 2))\n",
    "    #preds.append(get_bert_masked_inputs(tokenizer(s), bert_tokenizer, sent))\n",
    "\n",
    "\n",
    "batch_qty = 25\n",
    "\n",
    "for sent_id in range(0, len(sents), batch_qty):\n",
    "    print('Batch start', sent_id)\n",
    "    \n",
    "    sent_list = sents[sent_id:sent_id + batch_qty]\n",
    "    \n",
    "    # batch_data raw contains elements\n",
    "    # UtterData = namedtuple('SentData', ['sent_id', 'pos_bert', 'tok_ids', 'oov_token'])\n",
    "    # NOTE: pos_bert is OOV index with respect to the original (not BERT) tokenizer!!!\n",
    "    #\n",
    "    # tok_ids_batch is a Tensor with padded Bert-specific token IDs ready\n",
    "    # to be fed into a BERT model\n",
    "    batch_data_raw, tok_ids_batch = get_batch_data(torch_device,\n",
    "                                                 tokenizer, bert_tokenizer,\n",
    "                                                 sent_list, \n",
    "                                                 MAX_BERT_LEN)\n",
    "    \n",
    "    query_arr = []\n",
    "    query_tok_oov_id = []\n",
    "    \n",
    "    for e in batch_data_raw: \n",
    "        w = e.oov_token\n",
    "        wid = vocab.get_token_index(w)\n",
    "        query_arr.append(fasttext_embeds[wid])\n",
    "        query_tok_oov_id.append(wid)\n",
    "        \n",
    "    query_arr = np.array(query_arr)\n",
    "    query_matrix = np.array(query_arr)\n",
    "    query_qty = query_matrix.shape[0]\n",
    "    \n",
    "    print('Query matrix shape:', query_matrix.shape)\n",
    "    \n",
    "    start = time.time() \n",
    "    # nbrs is array of tuples (neighbor array, distance array)\n",
    "    # For cosine, the distance is 1 - cosine similarity\n",
    "    # k-NN search returns Bert-specific token IDs\n",
    "    nbrs = index.knnQueryBatch(query_matrix, k = K, num_threads = num_threads)\n",
    "    end = time.time() \n",
    "    print('kNN time total=%f (sec), per query=%f (sec), per query adjusted for thread number=%f (sec)' % \n",
    "          (end-start, float(end-start)/query_qty, num_threads*float(end-start)/query_qty))\n",
    "    \n",
    "    neighb_tok_ids=set()\n",
    "    \n",
    "    for qid in range(query_qty):\n",
    "        if query_tok_oov_id[qid] > 1:\n",
    "            nbrs_ids = nbrs[qid][0]\n",
    "            nbrs_dist = nbrs[qid][1]\n",
    "            \n",
    "            nqty = len(nbrs_ids)\n",
    "            for t in range(nqty):\n",
    "                if nbrs_dist[t] < MAX_COSINE_DIST:\n",
    "                    neighb_tok_ids.add(nbrs_ids[t])\n",
    "                \n",
    "    \n",
    "    \n",
    "    neighb_tok_ids = list(neighb_tok_ids)\n",
    "    \n",
    "    preds = get_bert_preds_for_words_batch(torch_device,\n",
    "                                           bert_model_mlm,\n",
    "                                           batch_data_raw, tok_ids_batch,\n",
    "                                           neighb_tok_ids)\n",
    "    \n",
    "    assert(len(preds) == query_qty)\n",
    "    for qid in range(query_qty):\n",
    "        if query_tok_oov_id[qid] > 1:  \n",
    "            # Let's map neighbor IDs from each queries to respective \n",
    "            # logits from the prediction set\n",
    "            logit_map = dict() # from Bert-specific token IDs to predicted logits\n",
    "            assert(len(preds[qid].logits) == len(neighb_tok_ids))\n",
    "            for i in range(len(neighb_tok_ids)):\n",
    "                logit_map[neighb_tok_ids[i]] = preds[qid].logits[i]\n",
    "\n",
    "            e = batch_data_raw[qid]\n",
    "            print(sent_list[e.sent_id])\n",
    "            print(\"### OOV ###\", e.oov_token)\n",
    "            print([vocab.get_token_from_index(bert_vocab_ids[i]) for i in nbrs[qid][0]])\n",
    "\n",
    "            nbr_logits = []\n",
    "            \n",
    "            nbrs_ids = nbrs[qid][0]\n",
    "            nbrs_dist = nbrs[qid][1]\n",
    "            \n",
    "            #print('Logit map:', logit_map)\n",
    "            #print('neighb_tok_ids', neighb_tok_ids)\n",
    "            \n",
    "            nqty = len(nbrs_ids)\n",
    "            for t in range(nqty):\n",
    "                nid = nbrs_ids[t]\n",
    "                # nid is Bert-speicifc token ID\n",
    "                if not nid in neighb_tok_ids:\n",
    "                    print('Missing %s distance %g ' \n",
    "                          % (bert_id2tok[nid],\n",
    "                            nbrs_dist[t]))\n",
    "                else:\n",
    "                    #print()\n",
    "                    nbr_logits.append(logit_map[nid])\n",
    "\n",
    "\n",
    "            #print(preds[qid])\n",
    "            print(\"====================================================================\")\n",
    "\n",
    "    \n",
    "    \n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    break\n",
    "    \n",
    "t1 = time.time()\n",
    "print('# of sentences:', len(sents), ' time elapsed:', t1 - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch_device=torch.device('cuda')\n",
    "#bert_model_mlm.to(torch_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
