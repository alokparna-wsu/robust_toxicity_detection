{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from pytorch_pretrained_bert.tokenization import BertTokenizer\n",
    "from pytorch_pretrained_bert.modeling import BertForMaskedLM\n",
    "\n",
    "torch_device=torch.device('cuda')\n",
    "\n",
    "bert_model_mlm = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "bert_model_mlm.eval()\n",
    "bert_model_mlm.to(torch_device)\n",
    "\n",
    "for param in bert_model_mlm.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "bert_id2tok = dict()\n",
    "for tok, tok_id in bert_tokenizer.vocab.items():\n",
    "    bert_id2tok[tok_id] = tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "ft_compiled_path = \"../data/jigsaw/ft_compiled.npy\" # Embeddings generated from the vocabulary\n",
    "data_vocab_path = \"../data/jigsaw/data_vocab.bin\"\n",
    "\n",
    "MAX_BERT_LEN=256\n",
    "MAX_COSINE_DIST=0.3\n",
    "BERT_VOCAB_QTY=30000\n",
    "\n",
    "num_threads=8\n",
    "K=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.data.tokenizers.word_splitter import SpacyWordSplitter\n",
    "\n",
    "_spacy_tok = SpacyWordSplitter(language='en_core_web_sm', pos_tags=False).split_words\n",
    "\n",
    "def tokenizer(s: str):\n",
    "    s = s.lower()\n",
    "    return [w.text for w in _spacy_tok(s)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"delete\" in bert_tokenizer.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[unused1]'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_id2tok[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3231"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_tokenizer.vocab['d']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns arrays of arrays if there's an OOV word or an empty array instead\n",
    "# Each array element is a tuple: \n",
    "# position of OOV word (with respect to the original tokenizer), sent for BERT tokenizer\n",
    "def get_bert_masked_inputs(toks, bert_tokenizer):\n",
    "    res = []\n",
    "    \n",
    "    oov_pos = []\n",
    "    bert_vocab = bert_tokenizer.vocab\n",
    "    \n",
    "    for i in range(len(toks)):\n",
    "        if toks[i] not in bert_vocab:\n",
    "            oov_pos.append(i)\n",
    "            \n",
    "\n",
    "    for pos in oov_pos:\n",
    "        res.append( (pos, '[CLS] %s [MASK] %s [SEP]' % \n",
    "                     (' '.join(toks[0:pos]), ' '.join(toks[pos+1:])) ) )\n",
    "        \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4, '[CLS] this is a * [MASK] * sentence . [SEP]')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_bert_masked_inputs(tokenizer('This is a *strangge* sentence.'), bert_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]', 'what', 'the', '[MASK]', 'are', 'you', 'doing', 'here', '?', '[SEP]']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toks = bert_tokenizer.tokenize('[CLS] what the [MASK] are you doing here ? [SEP]')\n",
    "toks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['do', \"n't\", 'could', \"n't\", 'ca', \"n't\", 'you', \"'re\", 'i', \"'m\"]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"don't  couldn't can't you're I'm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['don',\n",
       " \"'\",\n",
       " 't',\n",
       " 'couldn',\n",
       " \"'\",\n",
       " 't',\n",
       " 'can',\n",
       " \"'\",\n",
       " 't',\n",
       " 'you',\n",
       " \"'\",\n",
       " 're',\n",
       " 'i',\n",
       " \"'\",\n",
       " 'm']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_tokenizer.tokenize(\"don't  couldn't can't you're I'm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['insert']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_tokenizer.tokenize(\"insert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['you',\n",
       " \"'\",\n",
       " 're',\n",
       " 'right',\n",
       " '.',\n",
       " 'it',\n",
       " \"'\",\n",
       " 's',\n",
       " 'a',\n",
       " 'miracle',\n",
       " '!',\n",
       " 'you',\n",
       " \"'d\",\n",
       " 'been',\n",
       " 'deceived',\n",
       " '!']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"You ' re right. It ' s a miracle! You'd been deceived!\") # 've', 're', 's', 'd', 'll'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "# pos_oov is OOV index with respect to the original (not BERT) tokenizer!!!\n",
    "UtterData = namedtuple('SentData', ['batch_sent_id', 'pos_oov', 'tok_ids', 'oov_token'])\n",
    "\n",
    "def get_batch_data(torch_device, tokenizer, bert_tokenizer, sent_list, max_len=MAX_BERT_LEN):\n",
    "    \n",
    "    batch_data_raw = []\n",
    "    batch_max_seq_qty = 0\n",
    "    batch_sent_id = -1\n",
    "    for sent in sent_list:\n",
    "        batch_sent_id += 1\n",
    "        sent_toks = tokenizer(sent)\n",
    "        for sent_oov_pos, text in get_bert_masked_inputs(sent_toks, bert_tokenizer):\n",
    "            # To accurately get what is the position of [MASK] according\n",
    "            # to BERT tokenizer, we need to re-tokenize the sentence using\n",
    "            # the BERT tokenizer\n",
    "            all_bert_toks = bert_tokenizer.tokenize(text)\n",
    "            bert_toks = all_bert_toks[0:max_len] # 512 is the max. Bert seq. length\n",
    "\n",
    "            tok_ids = bert_tokenizer.convert_tokens_to_ids(bert_toks)\n",
    "            pos_oov = None\n",
    "            for i in range(len(bert_toks)):\n",
    "                if bert_toks[i] == '[MASK]':\n",
    "                    pos_oov = i\n",
    "                    break\n",
    "            assert(pos_oov is not None or len(all_bert_toks) > max_len)\n",
    "            if pos_oov is not None:\n",
    "                tok_qty = len(tok_ids)\n",
    "                batch_max_seq_qty = max(batch_max_seq_qty, tok_qty)\n",
    "                batch_data_raw.append( \n",
    "                    UtterData(batch_sent_id=batch_sent_id, \n",
    "                              pos_oov=sent_oov_pos, \n",
    "                              tok_ids=tok_ids, \n",
    "                              oov_token=sent_toks[sent_oov_pos]))\n",
    "            \n",
    "    batch_qty = len(batch_data_raw)\n",
    "    tok_ids_batch = np.zeros( (batch_qty, batch_max_seq_qty), dtype=np.int64) # zero is a padding symbol\n",
    "    for k in range(batch_qty):\n",
    "        tok_ids = batch_data_raw[k].tok_ids\n",
    "        tok_ids_batch[k, 0:len(tok_ids)] = tok_ids\n",
    "        \n",
    "                   \n",
    "    tok_ids_batch = torch.from_numpy(tok_ids_batch).to(device=torch_device) \n",
    "    \n",
    "    return batch_data_raw, tok_ids_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from collections import namedtuple\n",
    "import sys\n",
    "\n",
    "BertPredProbs = namedtuple('BertPred', ['batch_sent_id', 'pos_oov', 'logits'])\n",
    "\n",
    "def get_bert_preds_for_words_batch(torch_device, bert_model_mlm, \n",
    "                                   batch_data_raw, tok_ids_batch, # comes from get_batch_data\n",
    "                                   word_ids, # a list of IDS for which we generate logits\n",
    "                                   max_len=MAX_BERT_LEN):\n",
    "\n",
    "    seg_ids = torch.zeros_like(tok_ids_batch, device=torch_device)\n",
    "    \n",
    "    batch_qty = len(batch_data_raw)\n",
    "    \n",
    "    # Main BERT model see modeling.py in https://github.com/huggingface/pytorch-pretrained-BERT\n",
    "    bert = bert_model_mlm.bert \n",
    "    # cls is an instance of BertOnlyMLMHead (see https://github.com/huggingface/pytorch-pretrained-BERT)\n",
    "    cls = bert_model_mlm.cls\n",
    "    # predictions are of the type BertLMPredictionHead (see https://github.com/huggingface/pytorch-pretrained-BERT)\n",
    "    predictions = cls.predictions\n",
    "    transform = predictions.transform\n",
    "   \n",
    "    # We don't use the complete decoding matrix, but only selected rows\n",
    "    word_ids = torch.from_numpy(np.array(word_ids, dtype=np.int64)).to(device=torch_device)\n",
    "                                \n",
    "    weight = predictions.decoder.weight[word_ids,:]\n",
    "    bias = predictions.bias[word_ids]\n",
    "\n",
    "    # Transformations from the main BERT model\n",
    "    sequence_output, _= bert(tok_ids_batch, seg_ids, attention_mask=None, output_all_encoded_layers=False)\n",
    "    # Transformations from the BertLMPredictionHead model with the restricted last layer\n",
    "    hidden_states = transform(sequence_output)    \n",
    "    logits = torch.nn.functional.linear(hidden_states, weight) + bias                            \n",
    "                                        \n",
    "    logits=logits.detach().cpu().numpy()\n",
    "    \n",
    "    res = []\n",
    "    \n",
    "    for k in range(batch_qty):\n",
    "        \n",
    "        pos_oov = batch_data_raw[k].pos_oov         \n",
    "        res.append( BertPredProbs(batch_sent_id = batch_data_raw[k].batch_sent_id,\n",
    "                             pos_oov = pos_oov,\n",
    "                             logits = logits[k, pos_oov]\n",
    "                            ) \n",
    "                  )\n",
    "        \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3109, 6616, 17752, 6548, 4485, 2725, 24341, 4147, 2437, 3241, 2035, 3231]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_tokenizer.convert_tokens_to_ids(['hell', 'fuck', 'heck', 'devil', 'shit', 'doing', \n",
    "                                      'doin', 'wearing', 'making', 'thinking', 'all', 'test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[BertPred(batch_sent_id=0, pos_oov=2, logits=array([ 6.0060143,  3.6601357, -0.717967 , -2.1604285,  1.0156769,\n",
       "        -2.321704 , -7.334852 , -6.295347 , -3.6437619, -7.7260413],\n",
       "       dtype=float32)),\n",
       " BertPred(batch_sent_id=0, pos_oov=5, logits=array([ 0.8385326, -1.348005 , -2.5296175, -4.8170433, -2.2217095,\n",
       "         4.484024 , -2.3674998, -4.7202344, -3.995442 , -4.7819214],\n",
       "       dtype=float32)),\n",
       " BertPred(batch_sent_id=1, pos_oov=4, logits=array([-3.751286 , -3.4677486, -4.074821 , -5.3647747, -2.4487138,\n",
       "        -1.9118499, -4.7675657, -6.888247 , -1.6076685, -3.1759496],\n",
       "       dtype=float32))]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_list = ['What the fcuk are you doingg here?',\n",
    "             'This is a *strangge* sentence']\n",
    "\n",
    "batch_data_raw, tok_ids_batch = get_batch_data(torch_device, \n",
    "                                                tokenizer, \n",
    "                                                bert_tokenizer, \n",
    "                                                sent_list,\n",
    "                                                MAX_BERT_LEN)\n",
    "\n",
    "get_bert_preds_for_words_batch(torch_device,\n",
    "                               bert_model_mlm, \n",
    "                               batch_data_raw, tok_ids_batch,\n",
    "                               bert_tokenizer.convert_tokens_to_ids(['hell', 'fuck', 'heck', 'devil', 'shit', 'doing', \n",
    "                                                                  'doin', 'wearing', 'making', 'thinking']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bert_model_mlm.to('cpu')\n",
    "#torch.zeros(3,device=torch.device(\"cuda\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['б']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_tokenizer.tokenize('б')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import *\n",
    "from overrides import overrides\n",
    "import allennlp\n",
    "from allennlp.data.vocabulary import Vocabulary\n",
    "from allennlp.data.dataset_readers import DatasetReader\n",
    "\n",
    "from allennlp.data import Instance\n",
    "from allennlp.data.token_indexers import TokenIndexer, SingleIdTokenIndexer\n",
    "from allennlp.data.tokenizers import Token\n",
    "\n",
    "from allennlp.data.tokenizers.word_splitter import SpacyWordSplitter\n",
    "from allennlp.data.token_indexers import WordpieceIndexer, SingleIdTokenIndexer\n",
    "from allennlp.data.fields import TextField, SequenceLabelField, LabelField, MetadataField, ArrayField\n",
    "class MemoryOptimizedTextField(TextField):\n",
    "    @overrides\n",
    "    def __init__(self, tokens: List[str], token_indexers: Dict[str, TokenIndexer]) -> None:\n",
    "        self.tokens = tokens\n",
    "        self._token_indexers = token_indexers\n",
    "        self._indexed_tokens: Optional[Dict[str, TokenList]] = None\n",
    "        self._indexer_name_to_indexed_token: Optional[Dict[str, List[str]]] = None\n",
    "        # skip checks for tokens\n",
    "    @overrides\n",
    "    def index(self, vocab):\n",
    "        super().index(vocab)\n",
    "        self.tokens = None # empty tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "ft_compiled_path = \"../data/jigsaw/ft_compiled.npy\" # Embeddings generated from the vocabulary\n",
    "data_vocab_path = \"../data/jigsaw/data_vocab.bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext_embeds = np.load(ft_compiled_path)\n",
    "vocab=pickle.load(open(data_vocab_path,'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22778, 300)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# These are *MAIN* vocabulary word IDs for words in the BERT vocabulary.\n",
    "bert_vocab_term_glob_ids = []\n",
    "bert_vocab_term_bert_ids = []\n",
    "\n",
    "for tok, bert_tok_id in bert_tokenizer.vocab.items():\n",
    "    glob_tok_id = vocab.get_token_index(tok)\n",
    "    if glob_tok_id > 1:\n",
    "        bert_vocab_term_glob_ids.append(glob_tok_id)\n",
    "        bert_vocab_term_bert_ids.append(bert_tok_id)\n",
    "        \n",
    "bert_vocab_term_glob_ids = np.array(bert_vocab_term_glob_ids)\n",
    "bert_vocab_term_bert_ids = np.array(bert_vocab_term_bert_ids)\n",
    "fasttext_embeds[bert_vocab_term_glob_ids].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29611"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(bert_vocab_term_bert_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index-time parameters {'M': 30, 'indexThreadQty': 0, 'efConstruction': 200, 'post': 0}\n",
      "Index-time parameters {'M': 30, 'indexThreadQty': 0, 'efConstruction': 200}\n",
      "Indexing time = 4.952125\n"
     ]
    }
   ],
   "source": [
    "import nmslib, time\n",
    "\n",
    "M = 30\n",
    "efC = 200\n",
    "\n",
    "num_threads = 0\n",
    "index_time_params = {'M': M, 'indexThreadQty': num_threads, 'efConstruction': efC, 'post' : 0}\n",
    "print('Index-time parameters', index_time_params)\n",
    "\n",
    "# Space name should correspond to the space name \n",
    "# used for brute-force search\n",
    "space_name='cosinesimil'\n",
    "\n",
    "\n",
    "# Intitialize the library, specify the space, the type of the vector and add data points \n",
    "index = nmslib.init(method='hnsw', space=space_name, data_type=nmslib.DataType.DENSE_VECTOR) \n",
    "index.addDataPointBatch(fasttext_embeds[bert_vocab_term_glob_ids], bert_vocab_term_bert_ids)\n",
    "\n",
    "# Create an index\n",
    "start = time.time()\n",
    "index_time_params = {'M': M, 'indexThreadQty': num_threads, 'efConstruction': efC}\n",
    "index.createIndex(index_time_params) \n",
    "end = time.time() \n",
    "print('Index-time parameters', index_time_params)\n",
    "print('Indexing time = %f' % (end-start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting query-time parameters {'efSearch': 200}\n"
     ]
    }
   ],
   "source": [
    "# Setting query-time parameters\n",
    "efS = 200\n",
    "K=10\n",
    "query_time_params = {'efSearch': efS}\n",
    "print('Setting query-time parameters', query_time_params)\n",
    "index.setQueryTimeParams(query_time_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"../data/jigsaw/val_ds.bin\", \"rb\") as f:\n",
    "    val_ds = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import groupby\n",
    "import re\n",
    "\n",
    "def remove_extra_chars(s, max_qty=2):\n",
    "    res = [c * min(max_qty, len(list(group_iter))) for c, group_iter in groupby(s)] \n",
    "    return ''.join(res)\n",
    "\n",
    "def is_apost_token(s):\n",
    "    return re.match(r\"'[a-z]{1,3}$\", s) is not None\n",
    "\n",
    "\n",
    "def replace_by_patterns(tokenizer, s, replace_dict):\n",
    "    res = tokenizer(s)\n",
    "    for pos, repl in replace_dict.items():\n",
    "        res[pos] = repl\n",
    "    return ' '.join(res)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch start 0\n",
      "Batch start 20\n",
      "Batch start 40\n",
      "Batch start 60\n",
      "Batch start 80\n",
      "Batch start 100\n",
      "Batch start 120\n",
      "Batch start 140\n",
      "Batch start 160\n",
      "Batch start 180\n",
      "Batch start 200\n",
      "Batch start 220\n",
      "Batch start 240\n",
      "Batch start 260\n",
      "Batch start 280\n",
      "Batch start 300\n",
      "Batch start 320\n",
      "Batch start 340\n",
      "Batch start 360\n",
      "Batch start 380\n",
      "Batch start 400\n",
      "Batch start 420\n",
      "Batch start 440\n",
      "Batch start 460\n",
      "Batch start 480\n",
      "Batch start 500\n",
      "Batch start 520\n",
      "Batch start 540\n",
      "Batch start 560\n",
      "Batch start 580\n",
      "Batch start 600\n",
      "Batch start 620\n",
      "Batch start 640\n",
      "Batch start 660\n",
      "Batch start 680\n",
      "Batch start 700\n",
      "Batch start 720\n",
      "Batch start 740\n",
      "Batch start 760\n",
      "Batch start 780\n",
      "Batch start 800\n",
      "Batch start 820\n",
      "Batch start 840\n",
      "Batch start 860\n",
      "Batch start 880\n",
      "Batch start 900\n",
      "Batch start 920\n",
      "Batch start 940\n",
      "Batch start 960\n",
      "Batch start 980\n",
      "Batch start 1000\n",
      "Batch start 1020\n",
      "Batch start 1040\n",
      "Batch start 1060\n",
      "Batch start 1080\n",
      "Batch start 1100\n",
      "Batch start 1120\n",
      "Batch start 1140\n",
      "Batch start 1160\n",
      "Batch start 1180\n",
      "Batch start 1200\n",
      "Batch start 1220\n",
      "Batch start 1240\n",
      "Batch start 1260\n",
      "Batch start 1280\n",
      "Batch start 1300\n",
      "Batch start 1320\n",
      "Batch start 1340\n",
      "Batch start 1360\n",
      "Batch start 1380\n",
      "Batch start 1400\n",
      "Batch start 1420\n",
      "Batch start 1440\n",
      "Batch start 1460\n",
      "Batch start 1480\n",
      "Batch start 1500\n",
      "Batch start 1520\n",
      "Batch start 1540\n",
      "Batch start 1560\n",
      "Batch start 1580\n",
      "Batch start 1600\n",
      "Batch start 1620\n",
      "Batch start 1640\n",
      "Batch start 1660\n",
      "Batch start 1680\n",
      "Batch start 1700\n",
      "Batch start 1720\n",
      "Batch start 1740\n",
      "Batch start 1760\n",
      "Batch start 1780\n",
      "Batch start 1800\n",
      "Batch start 1820\n",
      "Batch start 1840\n",
      "Batch start 1860\n",
      "Batch start 1880\n",
      "Batch start 1900\n",
      "Batch start 1920\n",
      "Batch start 1940\n",
      "Batch start 1960\n",
      "Batch start 1980\n",
      "Batch start 2000\n",
      "Batch start 2020\n",
      "Batch start 2040\n",
      "Batch start 2060\n",
      "Batch start 2080\n",
      "Batch start 2100\n",
      "Batch start 2120\n",
      "Batch start 2140\n",
      "Batch start 2160\n",
      "Batch start 2180\n",
      "Batch start 2200\n",
      "Batch start 2220\n",
      "Batch start 2240\n",
      "Batch start 2260\n",
      "Batch start 2280\n",
      "Batch start 2300\n",
      "Batch start 2320\n",
      "Batch start 2340\n",
      "Batch start 2360\n",
      "Batch start 2380\n",
      "Batch start 2400\n",
      "Batch start 2420\n",
      "Batch start 2440\n",
      "Batch start 2460\n",
      "Batch start 2480\n",
      "Batch start 2500\n",
      "Batch start 2520\n",
      "Batch start 2540\n",
      "Batch start 2560\n",
      "Batch start 2580\n",
      "Batch start 2600\n",
      "Batch start 2620\n",
      "Batch start 2640\n",
      "Batch start 2660\n",
      "Batch start 2680\n",
      "Batch start 2700\n",
      "Batch start 2720\n",
      "Batch start 2740\n",
      "Batch start 2760\n",
      "Batch start 2780\n",
      "Batch start 2800\n",
      "Batch start 2820\n",
      "Batch start 2840\n",
      "Batch start 2860\n",
      "Batch start 2880\n",
      "Batch start 2900\n",
      "Batch start 2920\n",
      "Batch start 2940\n",
      "Batch start 2960\n",
      "Batch start 2980\n",
      "Batch start 3000\n",
      "Batch start 3020\n",
      "Batch start 3040\n",
      "Batch start 3060\n",
      "Batch start 3080\n",
      "Batch start 3100\n",
      "Batch start 3120\n",
      "Batch start 3140\n",
      "Batch start 3160\n",
      "Batch start 3180\n",
      "Batch start 3200\n",
      "Batch start 3220\n",
      "Batch start 3240\n",
      "Batch start 3260\n",
      "Batch start 3280\n",
      "Batch start 3300\n",
      "Batch start 3320\n",
      "Batch start 3340\n",
      "Batch start 3360\n",
      "Batch start 3380\n",
      "Batch start 3400\n",
      "Batch start 3420\n",
      "Batch start 3440\n",
      "Batch start 3460\n",
      "Batch start 3480\n",
      "Batch start 3500\n",
      "Batch start 3520\n",
      "Batch start 3540\n",
      "Batch start 3560\n",
      "Batch start 3580\n",
      "Batch start 3600\n",
      "Batch start 3620\n",
      "Batch start 3640\n",
      "Batch start 3660\n",
      "Batch start 3680\n",
      "Batch start 3700\n",
      "Batch start 3720\n",
      "Batch start 3740\n",
      "Batch start 3760\n",
      "Batch start 3780\n",
      "Batch start 3800\n",
      "Batch start 3820\n",
      "Batch start 3840\n",
      "Batch start 3860\n",
      "Batch start 3880\n",
      "Batch start 3900\n",
      "Batch start 3920\n",
      "Batch start 3940\n",
      "Batch start 3960\n",
      "Batch start 3980\n",
      "Batch start 4000\n",
      "Batch start 4020\n",
      "Batch start 4040\n",
      "Batch start 4060\n",
      "Batch start 4080\n",
      "Batch start 4100\n",
      "Batch start 4120\n",
      "Batch start 4140\n",
      "Batch start 4160\n",
      "Batch start 4180\n",
      "Batch start 4200\n",
      "Batch start 4220\n",
      "Batch start 4240\n",
      "Batch start 4260\n",
      "Batch start 4280\n",
      "Batch start 4300\n",
      "Batch start 4320\n",
      "Batch start 4340\n",
      "Batch start 4360\n",
      "Batch start 4380\n",
      "Batch start 4400\n",
      "Batch start 4420\n",
      "Batch start 4440\n",
      "Batch start 4460\n",
      "Batch start 4480\n",
      "Batch start 4500\n",
      "Batch start 4520\n",
      "Batch start 4540\n",
      "Batch start 4560\n",
      "Batch start 4580\n",
      "Batch start 4600\n",
      "Batch start 4620\n",
      "Batch start 4640\n",
      "Batch start 4660\n",
      "Batch start 4680\n",
      "Batch start 4700\n",
      "Batch start 4720\n",
      "Batch start 4740\n",
      "Batch start 4760\n",
      "Batch start 4780\n",
      "Batch start 4800\n",
      "Batch start 4820\n",
      "Batch start 4840\n",
      "Batch start 4860\n",
      "Batch start 4880\n",
      "Batch start 4900\n",
      "Batch start 4920\n",
      "Batch start 4940\n",
      "Batch start 4960\n",
      "Batch start 4980\n",
      "Batch start 5000\n",
      "Batch start 5020\n",
      "Batch start 5040\n",
      "Batch start 5060\n",
      "Batch start 5080\n",
      "Batch start 5100\n",
      "Batch start 5120\n",
      "Batch start 5140\n",
      "Batch start 5160\n",
      "Batch start 5180\n",
      "Batch start 5200\n",
      "Batch start 5220\n",
      "Batch start 5240\n",
      "Batch start 5260\n",
      "Batch start 5280\n",
      "Batch start 5300\n",
      "Batch start 5320\n",
      "Batch start 5340\n",
      "Batch start 5360\n",
      "Batch start 5380\n",
      "Batch start 5400\n",
      "Batch start 5420\n",
      "Batch start 5440\n",
      "Batch start 5460\n",
      "Batch start 5480\n",
      "Batch start 5500\n",
      "Batch start 5520\n",
      "Batch start 5540\n",
      "Batch start 5560\n",
      "Batch start 5580\n",
      "Batch start 5600\n",
      "Batch start 5620\n",
      "Batch start 5640\n",
      "Batch start 5660\n",
      "Batch start 5680\n",
      "Batch start 5700\n",
      "Batch start 5720\n",
      "Batch start 5740\n",
      "Batch start 5760\n",
      "Batch start 5780\n",
      "Batch start 5800\n",
      "Batch start 5820\n",
      "Batch start 5840\n",
      "Batch start 5860\n",
      "Batch start 5880\n",
      "Batch start 5900\n",
      "Batch start 5920\n",
      "Batch start 5940\n",
      "Batch start 5960\n",
      "Batch start 5980\n",
      "Batch start 6000\n",
      "Batch start 6020\n",
      "Batch start 6040\n",
      "Batch start 6060\n",
      "Batch start 6080\n",
      "Batch start 6100\n",
      "Batch start 6120\n",
      "Batch start 6140\n",
      "Batch start 6160\n",
      "Batch start 6180\n",
      "Batch start 6200\n",
      "Batch start 6220\n",
      "Batch start 6240\n",
      "Batch start 6260\n",
      "Batch start 6280\n",
      "Batch start 6300\n",
      "Batch start 6320\n",
      "Batch start 6340\n",
      "Batch start 6360\n",
      "Batch start 6380\n",
      "Batch start 6400\n",
      "Batch start 6420\n",
      "Batch start 6440\n",
      "Batch start 6460\n",
      "Batch start 6480\n",
      "Batch start 6500\n",
      "Batch start 6520\n",
      "Batch start 6540\n",
      "Batch start 6560\n",
      "Batch start 6580\n",
      "Batch start 6600\n",
      "Batch start 6620\n",
      "Batch start 6640\n",
      "Batch start 6660\n",
      "Batch start 6680\n",
      "Batch start 6700\n",
      "Batch start 6720\n",
      "Batch start 6740\n",
      "Batch start 6760\n",
      "Batch start 6780\n",
      "Batch start 6800\n",
      "Batch start 6820\n",
      "Batch start 6840\n",
      "Batch start 6860\n",
      "Batch start 6880\n",
      "Batch start 6900\n",
      "Batch start 6920\n",
      "Batch start 6940\n",
      "Batch start 6960\n",
      "Batch start 6980\n",
      "Batch start 7000\n",
      "Batch start 7020\n",
      "Batch start 7040\n",
      "Batch start 7060\n",
      "Batch start 7080\n",
      "Batch start 7100\n",
      "Batch start 7120\n",
      "Batch start 7140\n",
      "Batch start 7160\n",
      "Batch start 7180\n",
      "Batch start 7200\n",
      "Batch start 7220\n",
      "Batch start 7240\n",
      "Batch start 7260\n",
      "Batch start 7280\n",
      "Batch start 7300\n",
      "Batch start 7320\n",
      "Batch start 7340\n",
      "Batch start 7360\n",
      "Batch start 7380\n",
      "Batch start 7400\n",
      "Batch start 7420\n",
      "Batch start 7440\n",
      "Batch start 7460\n",
      "Batch start 7480\n",
      "Batch start 7500\n"
     ]
    }
   ],
   "source": [
    "import time, gc\n",
    "from scipy.special import softmax\n",
    "\n",
    "\n",
    "DEBUG_PRINT=False\n",
    "\n",
    "t0 = time.time()\n",
    "preds = []\n",
    "all_sents = [' '.join(t['tokens']) for t in val_ds]\n",
    "    #print(s)\n",
    "    #preds.append(get_bert_top_preds(tokenizer, bert_tokenizer, s, 2))\n",
    "    #preds.append(get_bert_masked_inputs(tokenizer(s), bert_tokenizer, sent))\n",
    "\n",
    "\n",
    "batch_qty_step = 20\n",
    "\n",
    "for batch_start_sent_id in range(0, len(all_sents), batch_qty_step):\n",
    "    print('Batch start', batch_start_sent_id)\n",
    "    \n",
    "    batch_qty = min(batch_qty_step, len(all_sents) - batch_start_sent_id)\n",
    "\n",
    "    batch_sents = [all_sents[k] for k in range(batch_start_sent_id,\n",
    "                                               batch_start_sent_id + batch_qty)]\n",
    "\n",
    "    replace_dict = {k : dict() for k in range(0,batch_qty)} \n",
    "    \n",
    "    # batch_data raw contains elements\n",
    "    # UtterData = namedtuple('SentData', ['batch_sent_id', 'pos_oov', 'tok_ids', 'oov_token')\n",
    "    # NOTE: pos_oov is OOV index with respect to the original (not BERT) tokenizer!!!\n",
    "    #\n",
    "    # tok_ids_batch is a Tensor with padded Bert-specific token IDs ready\n",
    "    # to be fed into a BERT model\n",
    "    batch_data_raw, tok_ids_batch = get_batch_data(torch_device,\n",
    "                                                 tokenizer, bert_tokenizer,\n",
    "                                                 batch_sents, \n",
    "                                                 MAX_BERT_LEN)\n",
    "    \n",
    "    query_arr = []\n",
    "    query_tok_oov_id = []\n",
    "    \n",
    "    for e in batch_data_raw: \n",
    "        w = e.oov_token\n",
    "        wCompr = remove_extra_chars(w)\n",
    "        wid = vocab.get_token_index(wCompr)\n",
    "        if w != wCompr:\n",
    "            if wid < 2:\n",
    "                wid = vocab.get_token_index(w)\n",
    "         \n",
    "        query_arr.append(fasttext_embeds[wid])\n",
    "        query_tok_oov_id.append(wid)\n",
    "        \n",
    "    query_arr = np.array(query_arr)\n",
    "    query_matrix = np.array(query_arr)\n",
    "    query_qty = query_matrix.shape[0]\n",
    "    \n",
    "    if DEBUG_PRINT: print('Query matrix shape:', query_matrix.shape)\n",
    "    \n",
    "    start = time.time() \n",
    "    # nbrs is array of tuples (neighbor array, distance array)\n",
    "    # For cosine, the distance is 1 - cosine similarity\n",
    "    # k-NN search returns Bert-specific token IDs\n",
    "    nbrs = index.knnQueryBatch(query_matrix, k = K, num_threads = num_threads)\n",
    "    end = time.time() \n",
    "    if DEBUG_PRINT: \n",
    "        print('kNN time total=%f (sec), per query=%f (sec), per query adjusted for thread number=%f (sec)' % \n",
    "          (end-start, float(end-start)/query_qty, num_threads*float(end-start)/query_qty))\n",
    "    \n",
    "    neighb_tok_ids=set()\n",
    "    \n",
    "    for qid in range(query_qty):\n",
    "        if query_tok_oov_id[qid] > 1:\n",
    "            nbrs_ids = nbrs[qid][0]\n",
    "            nbrs_dist = nbrs[qid][1]\n",
    "            \n",
    "            nqty = len(nbrs_ids)\n",
    "            for t in range(nqty):\n",
    "                if nbrs_dist[t] < MAX_COSINE_DIST:\n",
    "                    assert(nbrs_ids[t] < BERT_VOCAB_QTY)\n",
    "                    neighb_tok_ids.add(nbrs_ids[t])\n",
    "                 \n",
    "    \n",
    "    neighb_tok_ids = list(neighb_tok_ids)\n",
    "    \n",
    "    preds = get_bert_preds_for_words_batch(torch_device,\n",
    "                                           bert_model_mlm,\n",
    "                                           batch_data_raw, tok_ids_batch,\n",
    "                                           neighb_tok_ids)\n",
    "    \n",
    "    assert(len(preds) == query_qty)\n",
    "    for qid in range(query_qty):\n",
    "        e = batch_data_raw[qid]\n",
    "        glob_sent_id = batch_start_sent_id + e.batch_sent_id\n",
    "        assert(batch_sents[e.batch_sent_id] == all_sents[glob_sent_id])\n",
    "        if is_apost_token(e.oov_token) or e.oov_token == \"n't\":\n",
    "            # Thing's like \"I don't\" or \"You're\" are tokenized as do \"I do n't\" or \"You 're'\"\n",
    "            pass # TODO fix this\n",
    "        elif query_tok_oov_id[qid] > 1:  \n",
    "            # Let's map neighbor IDs from each queries to respective \n",
    "            # logits from the prediction set\n",
    "            logit_map = dict() # from Bert-specific token IDs to predicted logits\n",
    "            assert(len(preds[qid].logits) == len(neighb_tok_ids))\n",
    "            for i in range(len(neighb_tok_ids)):\n",
    "                logit_map[neighb_tok_ids[i]] = preds[qid].logits[i]\n",
    "\n",
    "            e = batch_data_raw[qid]\n",
    "            if DEBUG_PRINT: \n",
    "                print(all_sents[glob_sent_id])\n",
    "                print(\"### OOV ###\", e.oov_token)\n",
    "                print([bert_id2tok[bert_tok_id] for bert_tok_id in nbrs[qid][0]])\n",
    "\n",
    "            nbrs_sel_logits = []\n",
    "            nbrs_sel_toks = []\n",
    "            nbrs_sel_dists = []\n",
    "            \n",
    "            nbrs_ids = nbrs[qid][0]\n",
    "            nbrs_dist = nbrs[qid][1]\n",
    "            \n",
    "            #print('Logit map:', logit_map)\n",
    "            #print('neighb_tok_ids', neighb_tok_ids)\n",
    "            \n",
    "            nqty = len(nbrs_ids)\n",
    "            for t in range(nqty):\n",
    "                bert_tok_id = nbrs_ids[t]\n",
    "                # nid is Bert-speicifc token ID\n",
    "                if not bert_tok_id in neighb_tok_ids:\n",
    "                    if DEBUG_PRINT: \n",
    "                        print('Missing %s distance %g ' \n",
    "                              % (bert_id2tok[bert_tok_id],\n",
    "                                 nbrs_dist[t]))\n",
    "                else:\n",
    "                    if nbrs_dist[t] < MAX_COSINE_DIST:\n",
    "                        nbrs_sel_logits.append(logit_map[bert_tok_id])\n",
    "                        nbrs_sel_toks.append(bert_id2tok[bert_tok_id]) \n",
    "                        nbrs_sel_dists.append(nbrs_dist[t])\n",
    "            \n",
    "            if nbrs_sel_logits:\n",
    "                nbrs_softmax = softmax(np.array(nbrs_sel_logits))\n",
    "                nbrs_simil = 1 - np.array(nbrs_sel_dists)\n",
    "                nbrs_simil_adj = nbrs_softmax * nbrs_simil \n",
    "                \n",
    "                best_tok_id = np.argmax(nbrs_simil_adj)\n",
    "                \n",
    "                #print(\"batch sent id:\",e.batch_sent_id, e.pos_oov, best_tok_id)\n",
    "                #print(replace_dict[e.batch_sent_id])\n",
    "                assert(not e.pos_oov in replace_dict[e.batch_sent_id])\n",
    "                replace_dict[e.batch_sent_id][e.pos_oov] = nbrs_sel_toks[best_tok_id]\n",
    "                \n",
    "                if DEBUG_PRINT: \n",
    "                    print('Selected info, best_tok:', nbrs_sel_toks[best_tok_id])\n",
    "                    for k in range(len(nbrs_sel_logits)):\n",
    "                        print(nbrs_sel_toks[k], nbrs_softmax[k], \n",
    "                              nbrs_sel_dists[k], nbrs_simil_adj[k])\n",
    "            else:\n",
    "                if DEBUG_PRINT: print('Nothing found!')\n",
    "\n",
    "            #if DEBUG_PRINT: print(preds[qid])\n",
    "            if DEBUG_PRINT: \n",
    "                print(\"====================================================================\")\n",
    "\n",
    "    \n",
    "    \n",
    "    #gc.collect()\n",
    "    #torch.cuda.empty_cache()\n",
    "    for k in range(0, batch_qty):\n",
    "        src_sent = batch_sents[k]\n",
    "        rd = replace_dict[k]\n",
    "        #print('Replacement dict:', rd)\n",
    "        dst_sent = replace_by_patterns(tokenizer, src_sent, rd)\n",
    "        if DEBUG_PRINT:\n",
    "            print(\"====================================================================\")\n",
    "            print('Replacement dict:', rd)\n",
    "            print(src_sent)\n",
    "            print('------------')\n",
    "            print(dst_sent)\n",
    "            print(\"====================================================================\")\n",
    "    \n",
    "    #break\n",
    "    \n",
    "t1 = time.time()\n",
    "print('# of sentences:', len(all_sents), ' time elapsed:', t1 - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch_device=torch.device('cuda')\n",
    "#bert_model_mlm.to(torch_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.436952556015095"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(350/7979)*200000/60/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
