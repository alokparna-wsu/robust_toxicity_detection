{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from pytorch_pretrained_bert.tokenization import BertTokenizer\n",
    "from pytorch_pretrained_bert.modeling import BertForMaskedLM\n",
    "\n",
    "torch_device=torch.device('cuda')\n",
    "\n",
    "bert_model_mlm = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "bert_model_mlm.eval()\n",
    "bert_model_mlm.to(torch_device)\n",
    "\n",
    "for param in bert_model_mlm.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "bert_id2tok = dict()\n",
    "for tok, tok_id in bert_tokenizer.vocab.items():\n",
    "    bert_id2tok[tok_id] = tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "ft_compiled_path = \"../data/jigsaw/ft_compiled.npy\" # Embeddings generated from the vocabulary\n",
    "data_vocab_path = \"../data/jigsaw/data_vocab.bin\"\n",
    "\n",
    "MAX_BERT_LEN=256\n",
    "MAX_COSINE_DIST=0.3\n",
    "BERT_VOCAB_QTY=30000\n",
    "\n",
    "num_threads=8\n",
    "K=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.data.tokenizers.word_splitter import SpacyWordSplitter\n",
    "\n",
    "_spacy_tok = SpacyWordSplitter(language='en_core_web_sm', pos_tags=False).split_words\n",
    "\n",
    "def tokenizer(s: str):\n",
    "    s = s.lower()\n",
    "    return [w.text for w in _spacy_tok(s)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"ve\" in bert_tokenizer.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[unused1]'"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_id2tok[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3231"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_tokenizer.vocab['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns arrays of arrays if there's an OOV word or an empty array instead\n",
    "# Each array element is a tuple: \n",
    "# position of OOV word (with respect to the original tokenizer), sent for BERT tokenizer\n",
    "def get_bert_masked_inputs(toks, bert_tokenizer):\n",
    "    res = []\n",
    "    \n",
    "    oov_pos = []\n",
    "    bert_vocab = bert_tokenizer.vocab\n",
    "    \n",
    "    for i in range(len(toks)):\n",
    "        if toks[i] not in bert_vocab:\n",
    "            oov_pos.append(i)\n",
    "            \n",
    "\n",
    "    for pos in oov_pos:\n",
    "        res.append( (pos, '[CLS] %s [MASK] %s [SEP]' % \n",
    "                     (' '.join(toks[0:pos]), ' '.join(toks[pos+1:])) ) )\n",
    "        \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4, '[CLS] this is a * [MASK] * sentence . [SEP]')]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_bert_masked_inputs(tokenizer('This is a *strangge* sentence.'), bert_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]', 'what', 'the', '[MASK]', 'are', 'you', 'doing', 'here', '?', '[SEP]']"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toks = bert_tokenizer.tokenize('[CLS] what the [MASK] are you doing here ? [SEP]')\n",
    "toks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['do', \"n't\", 'could', \"n't\", 'ca', \"n't\", 'you', \"'re\", 'i', \"'m\"]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"don't  couldn't can't you're I'm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['don',\n",
       " \"'\",\n",
       " 't',\n",
       " 'couldn',\n",
       " \"'\",\n",
       " 't',\n",
       " 'can',\n",
       " \"'\",\n",
       " 't',\n",
       " 'you',\n",
       " \"'\",\n",
       " 're',\n",
       " 'i',\n",
       " \"'\",\n",
       " 'm']"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_tokenizer.tokenize(\"don't  couldn't can't you're I'm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['you',\n",
       " \"'\",\n",
       " 're',\n",
       " 'right',\n",
       " '.',\n",
       " 'it',\n",
       " \"'\",\n",
       " 's',\n",
       " 'a',\n",
       " 'miracle',\n",
       " '!',\n",
       " 'you',\n",
       " \"'d\",\n",
       " 'been',\n",
       " 'deceived',\n",
       " '!']"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"You ' re right. It ' s a miracle! You'd been deceived!\") # 've', 're', 's', 'd', 'll'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "# pos_oov is OOV index with respect to the original (not BERT) tokenizer!!!\n",
    "UtterData = namedtuple('SentData', ['batch_sent_id', 'pos_oov', 'tok_ids', 'oov_token'])\n",
    "\n",
    "def get_batch_data(torch_device, tokenizer, bert_tokenizer, sent_list, max_len=MAX_BERT_LEN):\n",
    "    \n",
    "    batch_data_raw = []\n",
    "    batch_max_seq_qty = 0\n",
    "    batch_sent_id = -1\n",
    "    for sent in sent_list:\n",
    "        batch_sent_id += 1\n",
    "        sent_toks = tokenizer(sent)\n",
    "        for sent_oov_pos, text in get_bert_masked_inputs(sent_toks, bert_tokenizer):\n",
    "            # To accurately get what is the position of [MASK] according\n",
    "            # to BERT tokenizer, we need to re-tokenize the sentence using\n",
    "            # the BERT tokenizer\n",
    "            all_bert_toks = bert_tokenizer.tokenize(text)\n",
    "            bert_toks = all_bert_toks[0:max_len] # 512 is the max. Bert seq. length\n",
    "\n",
    "            tok_ids = bert_tokenizer.convert_tokens_to_ids(bert_toks)\n",
    "            pos_oov = None\n",
    "            for i in range(len(bert_toks)):\n",
    "                if bert_toks[i] == '[MASK]':\n",
    "                    pos_oov = i\n",
    "                    break\n",
    "            assert(pos_oov is not None or len(all_bert_toks) > max_len)\n",
    "            if pos_oov is not None:\n",
    "                tok_qty = len(tok_ids)\n",
    "                batch_max_seq_qty = max(batch_max_seq_qty, tok_qty)\n",
    "                batch_data_raw.append( \n",
    "                    UtterData(batch_sent_id=batch_sent_id, \n",
    "                              pos_oov=sent_oov_pos, \n",
    "                              tok_ids=tok_ids, \n",
    "                              oov_token=sent_toks[sent_oov_pos]))\n",
    "            \n",
    "    batch_qty = len(batch_data_raw)\n",
    "    tok_ids_batch = np.zeros( (batch_qty, batch_max_seq_qty), dtype=np.int64) # zero is a padding symbol\n",
    "    for k in range(batch_qty):\n",
    "        tok_ids = batch_data_raw[k].tok_ids\n",
    "        tok_ids_batch[k, 0:len(tok_ids)] = tok_ids\n",
    "        \n",
    "                   \n",
    "    tok_ids_batch = torch.from_numpy(tok_ids_batch).to(device=torch_device) \n",
    "    \n",
    "    return batch_data_raw, tok_ids_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from collections import namedtuple\n",
    "import sys\n",
    "\n",
    "BertPredProbs = namedtuple('BertPred', ['batch_sent_id', 'pos_oov', 'logits'])\n",
    "\n",
    "def get_bert_preds_for_words_batch(torch_device, bert_model_mlm, \n",
    "                                   batch_data_raw, tok_ids_batch, # comes from get_batch_data\n",
    "                                   word_ids, # a list of IDS for which we generate logits\n",
    "                                   max_len=MAX_BERT_LEN):\n",
    "\n",
    "    seg_ids = torch.zeros_like(tok_ids_batch, device=torch_device)\n",
    "    \n",
    "    batch_qty = len(batch_data_raw)\n",
    "    \n",
    "    # Main BERT model see modeling.py in https://github.com/huggingface/pytorch-pretrained-BERT\n",
    "    bert = bert_model_mlm.bert \n",
    "    # cls is an instance of BertOnlyMLMHead (see https://github.com/huggingface/pytorch-pretrained-BERT)\n",
    "    cls = bert_model_mlm.cls\n",
    "    # predictions are of the type BertLMPredictionHead (see https://github.com/huggingface/pytorch-pretrained-BERT)\n",
    "    predictions = cls.predictions\n",
    "    transform = predictions.transform\n",
    "   \n",
    "    # We don't use the complete decoding matrix, but only selected rows\n",
    "    word_ids = torch.from_numpy(np.array(word_ids, dtype=np.int64)).to(device=torch_device)\n",
    "                                \n",
    "    weight = predictions.decoder.weight[word_ids,:]\n",
    "    bias = predictions.bias[word_ids]\n",
    "\n",
    "    # Transformations from the main BERT model\n",
    "    sequence_output, _= bert(tok_ids_batch, seg_ids, attention_mask=None, output_all_encoded_layers=False)\n",
    "    # Transformations from the BertLMPredictionHead model with the restricted last layer\n",
    "    hidden_states = transform(sequence_output)    \n",
    "    logits = torch.nn.functional.linear(hidden_states, weight) + bias                            \n",
    "                                        \n",
    "    logits=logits.detach().cpu().numpy()\n",
    "    \n",
    "    res = []\n",
    "    \n",
    "    for k in range(batch_qty):\n",
    "        \n",
    "        pos_oov = batch_data_raw[k].pos_oov         \n",
    "        res.append( BertPredProbs(batch_sent_id = batch_data_raw[k].batch_sent_id,\n",
    "                             pos_oov = pos_oov,\n",
    "                             logits = logits[k, pos_oov]\n",
    "                            ) \n",
    "                  )\n",
    "        \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3109, 6616, 17752, 6548, 4485, 2725, 24341, 4147, 2437, 3241, 2035, 3231]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_tokenizer.convert_tokens_to_ids(['hell', 'fuck', 'heck', 'devil', 'shit', 'doing', \n",
    "                                      'doin', 'wearing', 'making', 'thinking', 'all', 'test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[BertPred(batch_sent_id=0, pos_oov=2, logits=array([ 6.0060143,  3.6601357, -0.717967 , -2.1604285,  1.0156769,\n",
       "        -2.321704 , -7.334852 , -6.295347 , -3.6437619, -7.7260413],\n",
       "       dtype=float32)),\n",
       " BertPred(batch_sent_id=0, pos_oov=5, logits=array([ 0.8385326, -1.348005 , -2.5296175, -4.8170433, -2.2217095,\n",
       "         4.484024 , -2.3674998, -4.7202344, -3.995442 , -4.7819214],\n",
       "       dtype=float32)),\n",
       " BertPred(batch_sent_id=1, pos_oov=4, logits=array([-3.751286 , -3.4677486, -4.074821 , -5.3647747, -2.4487138,\n",
       "        -1.9118499, -4.7675657, -6.888247 , -1.6076685, -3.1759496],\n",
       "       dtype=float32))]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_list = ['What the fcuk are you doingg here?',\n",
    "             'This is a *strangge* sentence']\n",
    "\n",
    "batch_data_raw, tok_ids_batch = get_batch_data(torch_device, \n",
    "                                                tokenizer, \n",
    "                                                bert_tokenizer, \n",
    "                                                sent_list,\n",
    "                                                MAX_BERT_LEN)\n",
    "\n",
    "get_bert_preds_for_words_batch(torch_device,\n",
    "                               bert_model_mlm, \n",
    "                               batch_data_raw, tok_ids_batch,\n",
    "                               bert_tokenizer.convert_tokens_to_ids(['hell', 'fuck', 'heck', 'devil', 'shit', 'doing', \n",
    "                                                                  'doin', 'wearing', 'making', 'thinking']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bert_model_mlm.to('cpu')\n",
    "#torch.zeros(3,device=torch.device(\"cuda\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['б']"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_tokenizer.tokenize('б')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import *\n",
    "from overrides import overrides\n",
    "import allennlp\n",
    "from allennlp.data.vocabulary import Vocabulary\n",
    "from allennlp.data.dataset_readers import DatasetReader\n",
    "\n",
    "from allennlp.data import Instance\n",
    "from allennlp.data.token_indexers import TokenIndexer, SingleIdTokenIndexer\n",
    "from allennlp.data.tokenizers import Token\n",
    "\n",
    "from allennlp.data.tokenizers.word_splitter import SpacyWordSplitter\n",
    "from allennlp.data.token_indexers import WordpieceIndexer, SingleIdTokenIndexer\n",
    "from allennlp.data.fields import TextField, SequenceLabelField, LabelField, MetadataField, ArrayField\n",
    "class MemoryOptimizedTextField(TextField):\n",
    "    @overrides\n",
    "    def __init__(self, tokens: List[str], token_indexers: Dict[str, TokenIndexer]) -> None:\n",
    "        self.tokens = tokens\n",
    "        self._token_indexers = token_indexers\n",
    "        self._indexed_tokens: Optional[Dict[str, TokenList]] = None\n",
    "        self._indexer_name_to_indexed_token: Optional[Dict[str, List[str]]] = None\n",
    "        # skip checks for tokens\n",
    "    @overrides\n",
    "    def index(self, vocab):\n",
    "        super().index(vocab)\n",
    "        self.tokens = None # empty tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "ft_compiled_path = \"../data/jigsaw/ft_compiled.npy\" # Embeddings generated from the vocabulary\n",
    "data_vocab_path = \"../data/jigsaw/data_vocab.bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasttext_embeds = np.load(ft_compiled_path)\n",
    "vocab=pickle.load(open(data_vocab_path,'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22778, 300)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# These are *MAIN* vocabulary word IDs for words in the BERT vocabulary.\n",
    "bert_vocab_term_glob_ids = []\n",
    "bert_vocab_term_bert_ids = []\n",
    "\n",
    "for tok, bert_tok_id in bert_tokenizer.vocab.items():\n",
    "    glob_tok_id = vocab.get_token_index(tok)\n",
    "    if glob_tok_id > 1:\n",
    "        bert_vocab_term_glob_ids.append(glob_tok_id)\n",
    "        bert_vocab_term_bert_ids.append(bert_tok_id)\n",
    "        \n",
    "bert_vocab_term_glob_ids = np.array(bert_vocab_term_glob_ids)\n",
    "bert_vocab_term_bert_ids = np.array(bert_vocab_term_bert_ids)\n",
    "fasttext_embeds[bert_vocab_term_glob_ids].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29611"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(bert_vocab_term_bert_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index-time parameters {'M': 30, 'indexThreadQty': 0, 'efConstruction': 200, 'post': 0}\n",
      "Index-time parameters {'M': 30, 'indexThreadQty': 0, 'efConstruction': 200}\n",
      "Indexing time = 4.932761\n"
     ]
    }
   ],
   "source": [
    "import nmslib, time\n",
    "\n",
    "M = 30\n",
    "efC = 200\n",
    "\n",
    "num_threads = 0\n",
    "index_time_params = {'M': M, 'indexThreadQty': num_threads, 'efConstruction': efC, 'post' : 0}\n",
    "print('Index-time parameters', index_time_params)\n",
    "\n",
    "# Space name should correspond to the space name \n",
    "# used for brute-force search\n",
    "space_name='cosinesimil'\n",
    "\n",
    "\n",
    "# Intitialize the library, specify the space, the type of the vector and add data points \n",
    "index = nmslib.init(method='hnsw', space=space_name, data_type=nmslib.DataType.DENSE_VECTOR) \n",
    "index.addDataPointBatch(fasttext_embeds[bert_vocab_term_glob_ids], bert_vocab_term_bert_ids)\n",
    "\n",
    "# Create an index\n",
    "start = time.time()\n",
    "index_time_params = {'M': M, 'indexThreadQty': num_threads, 'efConstruction': efC}\n",
    "index.createIndex(index_time_params) \n",
    "end = time.time() \n",
    "print('Index-time parameters', index_time_params)\n",
    "print('Indexing time = %f' % (end-start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting query-time parameters {'efSearch': 200}\n"
     ]
    }
   ],
   "source": [
    "# Setting query-time parameters\n",
    "efS = 200\n",
    "K=10\n",
    "query_time_params = {'efSearch': efS}\n",
    "print('Setting query-time parameters', query_time_params)\n",
    "index.setQueryTimeParams(query_time_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"../data/jigsaw/val_ds.bin\", \"rb\") as f:\n",
    "    val_ds = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import groupby\n",
    "import re\n",
    "\n",
    "def remove_extra_chars(s, max_qty=2):\n",
    "    res = [c * min(max_qty, len(list(group_iter))) for c, group_iter in groupby(s)] \n",
    "    return ''.join(res)\n",
    "\n",
    "def is_apost_token(s):\n",
    "    return re.match(r\"'[a-z]{1,3}$\", s) is not None\n",
    "\n",
    "\n",
    "def replace_by_patterns(tokenizer, s, replace_dict):\n",
    "    res = tokenizer(s)\n",
    "    for pos, repl in replace_dict.items():\n",
    "        res[pos] = repl\n",
    "    return ' '.join(res)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch start 0\n",
      "Batch start 32\n",
      "Batch start 64\n",
      "Batch start 96\n",
      "Batch start 128\n",
      "Batch start 160\n",
      "Batch start 192\n",
      "Batch start 224\n",
      "Batch start 256\n",
      "Batch start 288\n",
      "Batch start 320\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-148-27a99f61ea31>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     85\u001b[0m                                            \u001b[0mbert_model_mlm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m                                            \u001b[0mbatch_data_raw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtok_ids_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m                                            neighb_tok_ids)\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mquery_qty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-128-daf758d327d5>\u001b[0m in \u001b[0;36mget_bert_preds_for_words_batch\u001b[0;34m(torch_device, bert_model_mlm, batch_data_raw, tok_ids_batch, word_ids, max_len)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0mlogits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time, gc\n",
    "from scipy.special import softmax\n",
    "\n",
    "\n",
    "DEBUG_PRINT=False\n",
    "\n",
    "t0 = time.time()\n",
    "preds = []\n",
    "all_sents = [' '.join(t['tokens']) for t in val_ds]\n",
    "    #print(s)\n",
    "    #preds.append(get_bert_top_preds(tokenizer, bert_tokenizer, s, 2))\n",
    "    #preds.append(get_bert_masked_inputs(tokenizer(s), bert_tokenizer, sent))\n",
    "\n",
    "\n",
    "batch_qty_step = 48\n",
    "\n",
    "for batch_start_sent_id in range(0, len(all_sents), batch_qty_step):\n",
    "    print('Batch start', batch_start_sent_id)\n",
    "    \n",
    "    batch_qty = min(batch_qty_step, len(all_sents) - batch_start_sent_id)\n",
    "\n",
    "    batch_sents = [all_sents[k] for k in range(batch_start_sent_id,\n",
    "                                               batch_start_sent_id + batch_qty)]\n",
    "\n",
    "    replace_dict = {k : dict() for k in range(0,batch_qty)} \n",
    "    \n",
    "    # batch_data raw contains elements\n",
    "    # UtterData = namedtuple('SentData', ['batch_sent_id', 'pos_oov', 'tok_ids', 'oov_token')\n",
    "    # NOTE: pos_oov is OOV index with respect to the original (not BERT) tokenizer!!!\n",
    "    #\n",
    "    # tok_ids_batch is a Tensor with padded Bert-specific token IDs ready\n",
    "    # to be fed into a BERT model\n",
    "    batch_data_raw, tok_ids_batch = get_batch_data(torch_device,\n",
    "                                                 tokenizer, bert_tokenizer,\n",
    "                                                 batch_sents, \n",
    "                                                 MAX_BERT_LEN)\n",
    "    \n",
    "    query_arr = []\n",
    "    query_tok_oov_id = []\n",
    "    \n",
    "    for e in batch_data_raw: \n",
    "        w = e.oov_token\n",
    "        wCompr = remove_extra_chars(w)\n",
    "        wid = vocab.get_token_index(wCompr)\n",
    "        if w != wCompr:\n",
    "            if wid < 2:\n",
    "                wid = vocab.get_token_index(w)\n",
    "         \n",
    "        query_arr.append(fasttext_embeds[wid])\n",
    "        query_tok_oov_id.append(wid)\n",
    "        \n",
    "    query_arr = np.array(query_arr)\n",
    "    query_matrix = np.array(query_arr)\n",
    "    query_qty = query_matrix.shape[0]\n",
    "    \n",
    "    if DEBUG_PRINT: print('Query matrix shape:', query_matrix.shape)\n",
    "    \n",
    "    start = time.time() \n",
    "    # nbrs is array of tuples (neighbor array, distance array)\n",
    "    # For cosine, the distance is 1 - cosine similarity\n",
    "    # k-NN search returns Bert-specific token IDs\n",
    "    nbrs = index.knnQueryBatch(query_matrix, k = K, num_threads = num_threads)\n",
    "    end = time.time() \n",
    "    if DEBUG_PRINT: \n",
    "        print('kNN time total=%f (sec), per query=%f (sec), per query adjusted for thread number=%f (sec)' % \n",
    "          (end-start, float(end-start)/query_qty, num_threads*float(end-start)/query_qty))\n",
    "    \n",
    "    neighb_tok_ids=set()\n",
    "    \n",
    "    for qid in range(query_qty):\n",
    "        if query_tok_oov_id[qid] > 1:\n",
    "            nbrs_ids = nbrs[qid][0]\n",
    "            nbrs_dist = nbrs[qid][1]\n",
    "            \n",
    "            nqty = len(nbrs_ids)\n",
    "            for t in range(nqty):\n",
    "                if nbrs_dist[t] < MAX_COSINE_DIST:\n",
    "                    assert(nbrs_ids[t] < BERT_VOCAB_QTY)\n",
    "                    neighb_tok_ids.add(nbrs_ids[t])\n",
    "                 \n",
    "    \n",
    "    neighb_tok_ids = list(neighb_tok_ids)\n",
    "    \n",
    "    preds = get_bert_preds_for_words_batch(torch_device,\n",
    "                                           bert_model_mlm,\n",
    "                                           batch_data_raw, tok_ids_batch,\n",
    "                                           neighb_tok_ids)\n",
    "    \n",
    "    assert(len(preds) == query_qty)\n",
    "    for qid in range(query_qty):\n",
    "        e = batch_data_raw[qid]\n",
    "        glob_sent_id = batch_start_sent_id + e.batch_sent_id\n",
    "        assert(batch_sents[e.batch_sent_id] == all_sents[glob_sent_id])\n",
    "        if is_apost_token(e.oov_token) or e.oov_token == \"n't\":\n",
    "            # Thing's like \"I don't\" or \"You're\" are tokenized as do \"I do n't\" or \"You 're'\"\n",
    "            pass # TODO fix this\n",
    "        elif query_tok_oov_id[qid] > 1:  \n",
    "            # Let's map neighbor IDs from each queries to respective \n",
    "            # logits from the prediction set\n",
    "            logit_map = dict() # from Bert-specific token IDs to predicted logits\n",
    "            assert(len(preds[qid].logits) == len(neighb_tok_ids))\n",
    "            for i in range(len(neighb_tok_ids)):\n",
    "                logit_map[neighb_tok_ids[i]] = preds[qid].logits[i]\n",
    "\n",
    "            e = batch_data_raw[qid]\n",
    "            if DEBUG_PRINT: \n",
    "                print(all_sents[glob_sent_id])\n",
    "                print(\"### OOV ###\", e.oov_token)\n",
    "                print([bert_id2tok[bert_tok_id] for bert_tok_id in nbrs[qid][0]])\n",
    "\n",
    "            nbrs_sel_logits = []\n",
    "            nbrs_sel_toks = []\n",
    "            nbrs_sel_dists = []\n",
    "            \n",
    "            nbrs_ids = nbrs[qid][0]\n",
    "            nbrs_dist = nbrs[qid][1]\n",
    "            \n",
    "            #print('Logit map:', logit_map)\n",
    "            #print('neighb_tok_ids', neighb_tok_ids)\n",
    "            \n",
    "            nqty = len(nbrs_ids)\n",
    "            for t in range(nqty):\n",
    "                bert_tok_id = nbrs_ids[t]\n",
    "                # nid is Bert-speicifc token ID\n",
    "                if not bert_tok_id in neighb_tok_ids:\n",
    "                    if DEBUG_PRINT: \n",
    "                        print('Missing %s distance %g ' \n",
    "                              % (bert_id2tok[bert_tok_id],\n",
    "                                 nbrs_dist[t]))\n",
    "                else:\n",
    "                    if nbrs_dist[t] < MAX_COSINE_DIST:\n",
    "                        nbrs_sel_logits.append(logit_map[bert_tok_id])\n",
    "                        nbrs_sel_toks.append(bert_id2tok[bert_tok_id]) \n",
    "                        nbrs_sel_dists.append(nbrs_dist[t])\n",
    "            \n",
    "            if nbrs_sel_logits:\n",
    "                nbrs_softmax = softmax(np.array(nbrs_sel_logits))\n",
    "                nbrs_simil = 1 - np.array(nbrs_sel_dists)\n",
    "                nbrs_simil_adj = nbrs_softmax * nbrs_simil \n",
    "                \n",
    "                best_tok_id = np.argmax(nbrs_simil_adj)\n",
    "                \n",
    "                #print(\"batch sent id:\",e.batch_sent_id, e.pos_oov, best_tok_id)\n",
    "                #print(replace_dict[e.batch_sent_id])\n",
    "                assert(not e.pos_oov in replace_dict[e.batch_sent_id])\n",
    "                replace_dict[e.batch_sent_id][e.pos_oov] = nbrs_sel_toks[best_tok]\n",
    "                \n",
    "                if DEBUG_PRINT: \n",
    "                    print('Selected info, best_tok:', nbrs_sel_toks[best_tok])\n",
    "                    for k in range(len(nbrs_sel_logits)):\n",
    "                        print(nbrs_sel_toks[k], nbrs_softmax[k], \n",
    "                              nbrs_sel_dists[k], nbrs_simil_adj[k])\n",
    "            else:\n",
    "                if DEBUG_PRINT: print('Nothing found!')\n",
    "\n",
    "            #if DEBUG_PRINT: print(preds[qid])\n",
    "            if DEBUG_PRINT: \n",
    "                print(\"====================================================================\")\n",
    "\n",
    "    \n",
    "    \n",
    "    #gc.collect()\n",
    "    #torch.cuda.empty_cache()\n",
    "    for k in range(0, batch_qty):\n",
    "        src_sent = batch_sents[k]\n",
    "        rd = replace_dict[k]\n",
    "        #print('Replacement dict:', rd)\n",
    "        dst_sent = replace_by_patterns(tokenizer, src_sent, rd)\n",
    "        if DEBUG_PRINT:\n",
    "            print(\"====================================================================\")\n",
    "            print('Replacement dict:', rd)\n",
    "            print(src_sent)\n",
    "            print('------------')\n",
    "            print(dst_sent)\n",
    "            print(\"====================================================================\")\n",
    "    \n",
    "    #break\n",
    "    \n",
    "t1 = time.time()\n",
    "print('# of sentences:', len(all_sents), ' time elapsed:', t1 - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch_device=torch.device('cuda')\n",
    "#bert_model_mlm.to(torch_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.436952556015095"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(350/7979)*200000/60/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
